[["index.html", "Data analysis and statistics for cognitive neuroscience 1 Introduction to the book", " Data analysis and statistics for cognitive neuroscience Matthew J. Crossley 2024-02-13 1 Introduction to the book First and foremost, this book is an active work in progress and is liable to change somehwat from week to week. If you find errors, or if you find any section particularly confusion, please feel free to reach out to me for help. Statistics is often taught to the aspiring cognitive neuroscientist as a series of cookbook recipes, without imparting any deep understanding of the reasoning that generated those recipes in the first place. Too often I have seen the deer in headlights look on the newly minted graduate student when a dataset they are asked to analyse does not fit the mold of the recipes they have been previously exposed to. The ambition of this book is to topple this situation on its head. We will repeatedly emphasize developing the reasoning skills necessary to understand the recipes, and to write our own recipes when called for. In the course of this book we will nevertheless encounter the usual suspects – t-tests, ANOVAs, regressions, and so – but you may find that we go deeper into fewer recipes rather than skimming the surface of many. We will use the R programming language throughout this book. We use R because it is mature; It is widely embraced in psychology and neuroscience; It is widely embraced in data science; It is relatively easy to learn. That said, it is not the only good option. Python, for example, is excellent. Matlab is also excellent – though not as widely used and not as free – but it is still widely used in academia. You might even run into more obscure languages here and there. At the end of the day, we have to pick one. R is a good place to start and it will offer you excellent return on investment. Just as R is a reasonable choice for a programming language, Rstudio is a reasonable choice for an integrated development environment (IDE). It is free, it is widely used, and it is well supported. That said, there are numerous options for programming in R. I have old-school computer science friends that edit code in a plain text file and execute it straight from a terminal. There is absolutely nothing wrong with this. Program where you like. But why bother learning to code now that chatGPT is so good at writing code for us? That is a very good question. The point I would like you to notice is how this question underscores the importance of developing our reasoning capacity both as statisticians and as programmers. The nuts and bolts of getting code to run or have some kind of statistical test or another performed on your data is only going to become easier and easier and more and more automated with increasing AI capabilities. But the reasoning behind the nuts and bolts will endure the test of time. "],["introduction-to-r.html", "2 Introduction to R 2.1 Basics 2.2 Functions 2.3 Data types 2.4 Containers 2.5 Loops 2.6 Conditional flow 2.7 Custom functions 2.8 Inspect existing objects 2.9 Summary", " 2 Introduction to R This chapter provides an introduction to R, covering basic concepts like defining variables, performing mathematical operations, and using built-in functions. It also introduces various data types (numeric, character, logical, and factor) and containers (vector, list, and data frame) in R, along with how to create and access their elements. It also covers control flow with for and while loops, conditional flow with if statements, and the creation of custom functions. Lastly, it discusses inspecting objects using functions like ls(), rm(), class(), str(), head(), tail(), and summary() to understand and manage the R environment. 2.1 Basics # Use R as a calculator 2 + 2 ## [1] 4 # Store results in variables x &lt;- 2 + 2 x ## [1] 4 # Perform mathematical operations on variables x &lt;- 2 + 2 y &lt;- 3 + 7 x + y ## [1] 14 2.2 Functions Functions are objects that take arguments as inputs, perform some operation on those inputs, and return the results. ls() is a function that reports what objects (e.g., variables you have defined) are in the current environment and therefore available for you to interact with. ls() ## [1] &quot;x&quot; &quot;y&quot; rm() is a function that can remove objects from the current environment. It takes several arguments. Type ?rm() to see help information on how to use it. We will commonly combine ls() and rm() to remove all objects from the current environment. This is a good thing to do at the beginning of every new script you write. # Remove all objects from the current R session. rm(list = ls()) 2.3 Data types 2.3.1 numeric x &lt;- 2 class(x) ## [1] &quot;numeric&quot; 2.3.2 character x &lt;- &#39;a&#39; class(x) ## [1] &quot;character&quot; 2.3.3 logical x &lt;- TRUE class(x) ## [1] &quot;logical&quot; 2.3.4 factor A factor is a categorical data type. They are most often used to code for different conditions in an experiment. x &lt;- factor(1) class(x) ## [1] &quot;factor&quot; 2.4 Containers 2.4.1 vector Create a vector with the function c(). The elements of a vector must be of the same type. Access element i of vector x with square brackets (e.g., x[i]) # Create a vector with any three numbers you like. x &lt;- c(1, 2, 3.14159) x ## [1] 1.00000 2.00000 3.14159 # Access the third element of x x3 &lt;- x[3] x3 ## [1] 3.14159 2.4.2 list Create a list with the function list() The elements of a list can be of different types. Access element i of list x with double square brackets (e.g., x[[i]]) # Create a three element list containing one numeric # item, one `character` item, and one logical item. x &lt;- list(3.14159, &#39;pi&#39;, TRUE) x ## [[1]] ## [1] 3.14159 ## ## [[2]] ## [1] &quot;pi&quot; ## ## [[3]] ## [1] TRUE # Access the third element of x x3 &lt;- x[[3]] x3 ## [1] TRUE 2.4.3 data.frame Create a data.frame with the function data.frame() -data.frame is pretty close what you might think of as an excel spreadsheet. Access column x in data.frame df with the $ operator (e.g., df$x). # Create vectors to later store in a data frame x &lt;- c(&#39;I&#39;, &#39;I&#39;, &#39;I&#39;, &#39;II&#39;, &#39;II&#39;, &#39;II&#39;, &#39;III&#39;, &#39;III&#39;, &#39;III&#39;, &#39;IV&#39;, &#39;IV&#39;, &#39;IV&#39;) y &lt;- c(&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;, &#39;d&#39;, &#39;d&#39;, &#39;e&#39;, &#39;e&#39;, &#39;f&#39;, &#39;f&#39;) z &lt;- rnorm(12) # Create the data frame df &lt;- data.frame(x, y, z) df ## x y z ## 1 I a -0.9330328 ## 2 I a -0.3860412 ## 3 I b -0.1272712 ## 4 II b -0.0750369 ## 5 II c 0.1739119 ## 6 II c 2.3674721 ## 7 III d -0.1730269 ## 8 III d -1.3843449 ## 9 III e -0.2846021 ## 10 IV e -0.5652392 ## 11 IV f 1.4165630 ## 12 IV f -0.2475928 # Access column x df$x ## [1] &quot;I&quot; &quot;I&quot; &quot;I&quot; &quot;II&quot; &quot;II&quot; &quot;II&quot; &quot;III&quot; &quot;III&quot; &quot;III&quot; &quot;IV&quot; &quot;IV&quot; &quot;IV&quot; 2.5 Loops 2.5.1 for loops for loops will run a chunk of code repeatedly for a fixed number of iterations. The general syntax of a for loop is as follows: for(x in y) { # On the first iteration, `x` will take the value `y[1]` # On the second iteration, `x` will take the value `y[2]` # On the third iteration, `x` will take the value `y[3]` # The loop will end after `x` has taken the value `y[length(y)]` # That is, the loop will end when we have iterated through # all elements in `y` } As an example suppose we want to print the numbers 1, 2, 3 # dumb monkey way to print 1, 2, 3 print(1) ## [1] 1 print(2) ## [1] 2 print(3) ## [1] 3 # smart monkey way to print 1, 2, 3 for(i in c(1, 2, 3)) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 2.5.2 while loops while loops will run a chunk of code repeatedly over and over again until some logical condition is met. You have to be careful with these, because if your code never sets up a stopping condition, then the loop will execute until your computer turns to dust. The general syntax of a for loop is as follows: condition &lt;- TRUE while(condition) { # On the first iteration, `x` will take the value `y[1]` # On the second iteration, `x` will take the value `y[2]` # On the third iteration, `x` will take the value `y[3]` # The loop will end only when `condition` is set to `FALSE` } Lets again consider the example printing the numbers 1, 2, 3 # dumb monkey way to print 1, 2, 3 print(1) ## [1] 1 print(2) ## [1] 2 print(3) ## [1] 3 # smart monkey way to print 1, 2, 3 x &lt;- 1 while(x &lt; 4) { print(x) x &lt;- x + 1 # without this line the loop would run forever } ## [1] 1 ## [1] 2 ## [1] 3 2.6 Conditional flow Very often we will want to execute a chunk of code only in some situations (e.g., for a particular experiment condition) and we will want to run some other chunk of code in other situations. The primary method for doing this is to use if statements. The general syntax of an if statement is as follows: if(condition) { # code to run if condition is TRUE } else { # code to run if condition is FALSE } For example, suppose we want to print whether or not a number is less than 5. x &lt;- 3 if(x &lt; 5) { print(&#39;x is less than 5&#39;) } else { print(&#39;x is greater than or equal to 5&#39;) } ## [1] &quot;x is less than 5&quot; 2.7 Custom functions Custom functions are very useful because they allow us to flexibly reuse the same chunk of code in different places without having to rewrite the entire chunk. The general syntax for defining functions is as follows: function_name &lt;- function(argument_1, argument_2, ...) { ## code to run when the function is called. Can use ## `argument_1`, `argument_2`, and any other argument ## passed in. ##... return(the_result) } function_name is the name of the function. argument_1, argument_2, etc. are variables that you want the code chunk inside the function to use. the_result is a variable that you will have to be careful to define in the code chunk in the function. Consider the following example: my_func &lt;- function(x, y) { z &lt;- x + y - 1 return(z) } my_func(x=2, y=3) ## [1] 4 my_func take two arguments x and y and returns x + y - 1 2.8 Inspect existing objects R has many built-in functions that are useful for inspecting what sort of thing an existing object is. We illustrate the use of some of these functions below. # define some variables (objects) to inspect var_1 &lt;- 10 var_2 &lt;- &quot;apple&quot; var_3 &lt;- TRUE var_4 &lt;- c(1, 2, 3) var_5 &lt;- list(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;) var_6 &lt;- data.frame(v4=var_4, v5=var_5) # substitute different variables into the following # functions to see how they help you inspect existing # objects. str(var_1) ## num 10 class(var_1) ## [1] &quot;numeric&quot; head(var_1) ## [1] 10 tail(var_1) ## [1] 10 summary(var_1) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 10 10 10 10 10 10 2.9 Summary Here’s a list of the R functions we saw in the above examples: ls() - Lists objects in the current environment. rm() - Removes objects from the current environment. class() - Returns the class (type) of an object. factor() - Creates a factor (categorical data type). c() - Combines values into a vector or list. list() - Creates a list. data.frame() - Creates a data frame. rnorm() - Generates normally distributed random numbers. print() - Prints its argument. str() - Displays the structure of an R object. head() - Returns the first parts of an object. tail() - Returns the last parts of an object. summary() - Provides a summary of an object’s properties. Additionally, we saw how to create and use custom functions, as well as how to use control flow constructs like for loops, while loops, and if statements. "],["introduction-to-data.html", "3 Introduction to data.table 3.1 Creating a data.table 3.2 Subset rows in i 3.3 Select columns with j 3.4 Select and operate on columns with j 3.5 Combine i and j 3.6 Group using by 3.7 Adding and modifying columns 3.8 Be careful when copying data.table objects 3.9 Getting data into a data.table 3.10 Wide vs long format", " 3 Introduction to data.table This chapter introduces the data.table library, a high-performance alternative to R’s standard data.frame. We will cover the basics of working with data.table objects, focusing on key operations such as creating data tables, subsetting rows, selecting and manipulating columns, and grouping operations. It explains the data.table syntax DT[i, j, by], where i filters rows, j selects columns or performs calculations, and by allows for group-wise operations. Practical examples demonstrate how to: Load the data.table library and create a data.table. Subset rows using integers, logical expressions, and the %in% operator. Select and operate on columns, including summarizing data with functions like mean() and sd(). Combine row selection (i) and column operations (j) for more complex data manipulation. Group data using the by argument to apply functions within groups. Add and modify columns within a data.table using the := operator. Manage data table copies and understand the difference between shallow and deep copies. Import data into a data.table using the fread function. We will also see how to deal with different data formatting, using melt and dcast functions for reshaping data tables from wide format to long or vice versa. 3.1 Creating a data.table # Load the data.table library library(data.table) # Create vectors to later store in a data frame x &lt;- c(&#39;I&#39;, &#39;I&#39;, &#39;I&#39;, &#39;II&#39;, &#39;II&#39;, &#39;II&#39;, &#39;III&#39;, &#39;III&#39;, &#39;III&#39;, &#39;IV&#39;, &#39;IV&#39;, &#39;IV&#39;) y &lt;- c(&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;, &#39;d&#39;, &#39;d&#39;, &#39;e&#39;, &#39;e&#39;, &#39;f&#39;, &#39;f&#39;) z &lt;- rnorm(12) # create a data table dt &lt;- data.table(x, y, z) dt ## x y z ## 1: I a 0.801076986 ## 2: I a 0.886827043 ## 3: I b -1.216112121 ## 4: II b -1.494334054 ## 5: II c 0.467194196 ## 6: II c -0.530958683 ## 7: III d -0.891427670 ## 8: III d 0.787302046 ## 9: III e 0.006745659 ## 10: IV e -1.836147935 ## 11: IV f 0.052353254 ## 12: IV f -1.018931991 3.2 Subset rows in i There are many ways to select rows. Set i equal to a vector of integers. Set i equal to a logical expression. It will sometimes be useful to use the %in% operator when specifying i. # select rows by passing integer indices dt[1:4] ## x y z ## 1: I a 0.801077 ## 2: I a 0.886827 ## 3: I b -1.216112 ## 4: II b -1.494334 dt[4:7] ## x y z ## 1: II b -1.4943341 ## 2: II c 0.4671942 ## 3: II c -0.5309587 ## 4: III d -0.8914277 dt[c(2, 4)] ## x y z ## 1: I a 0.886827 ## 2: II b -1.494334 dt[c(2, 5, 11)] ## x y z ## 1: I a 0.88682704 ## 2: II c 0.46719420 ## 3: IV f 0.05235325 # select rows by passing a logical expression dt[x==&#39;II&#39;] ## x y z ## 1: II b -1.4943341 ## 2: II c 0.4671942 ## 3: II c -0.5309587 dt[(x==&#39;II&#39;) &amp; (y==&#39;c&#39;)] ## x y z ## 1: II c 0.4671942 ## 2: II c -0.5309587 dt[(x==&#39;II&#39;) &amp; (y==&#39;c&#39;) &amp; (z &gt; 0)] ## x y z ## 1: II c 0.4671942 # select rows by implementing a logical expression using # `chaining` dt[x==&#39;II&#39;][y==&#39;c&#39;][z &gt; 0] ## x y z ## 1: II c 0.4671942 # select rows using the `%in%` operator dt[x %in% c(&#39;I&#39;, &#39;II&#39;)] ## x y z ## 1: I a 0.8010770 ## 2: I a 0.8868270 ## 3: I b -1.2161121 ## 4: II b -1.4943341 ## 5: II c 0.4671942 ## 6: II c -0.5309587 dt[y %in% c(&#39;b&#39;, &#39;f&#39;)] ## x y z ## 1: I b -1.21611212 ## 2: II b -1.49433405 ## 3: IV f 0.05235325 ## 4: IV f -1.01893199 # Notice that using the `%in%` operator is essentially # performing an *or* logical operation dt[(x==&#39;I&#39;) | (x==&#39;II&#39;)] ## x y z ## 1: I a 0.8010770 ## 2: I a 0.8868270 ## 3: I b -1.2161121 ## 4: II b -1.4943341 ## 5: II c 0.4671942 ## 6: II c -0.5309587 dt[(y==&#39;b&#39;) | (y==&#39;f&#39;)] ## x y z ## 1: I b -1.21611212 ## 2: II b -1.49433405 ## 3: IV f 0.05235325 ## 4: IV f -1.01893199 If you want to operate on every row of some set of columns, leave the i argument blank. This will result in the first character inside the square brackets being a comma, which looks a bit strange if you’re not used to it. Just remember to read it as select every row. data.table also provides a keyword .N that can be useful in a number of ways. One such way is to use it in selecting rows as shown in the next code chunk. dt # select every row ## x y z ## 1: I a 0.801076986 ## 2: I a 0.886827043 ## 3: I b -1.216112121 ## 4: II b -1.494334054 ## 5: II c 0.467194196 ## 6: II c -0.530958683 ## 7: III d -0.891427670 ## 8: III d 0.787302046 ## 9: III e 0.006745659 ## 10: IV e -1.836147935 ## 11: IV f 0.052353254 ## 12: IV f -1.018931991 dt[,] # select every row ## x y z ## 1: I a 0.801076986 ## 2: I a 0.886827043 ## 3: I b -1.216112121 ## 4: II b -1.494334054 ## 5: II c 0.467194196 ## 6: II c -0.530958683 ## 7: III d -0.891427670 ## 8: III d 0.787302046 ## 9: III e 0.006745659 ## 10: IV e -1.836147935 ## 11: IV f 0.052353254 ## 12: IV f -1.018931991 dt[1:.N] # select every row using the .N keyword ## x y z ## 1: I a 0.801076986 ## 2: I a 0.886827043 ## 3: I b -1.216112121 ## 4: II b -1.494334054 ## 5: II c 0.467194196 ## 6: II c -0.530958683 ## 7: III d -0.891427670 ## 8: III d 0.787302046 ## 9: III e 0.006745659 ## 10: IV e -1.836147935 ## 11: IV f 0.052353254 ## 12: IV f -1.018931991 dt[2:.N] # select all but the first row using the .N keyword ## x y z ## 1: I a 0.886827043 ## 2: I b -1.216112121 ## 3: II b -1.494334054 ## 4: II c 0.467194196 ## 5: II c -0.530958683 ## 6: III d -0.891427670 ## 7: III d 0.787302046 ## 8: III e 0.006745659 ## 9: IV e -1.836147935 ## 10: IV f 0.052353254 ## 11: IV f -1.018931991 3.3 Select columns with j To operate on specific columns, set j equal to a list of the names of the columns you want. Inside the square brackets of a data.table, list can be abbreviated with a ., which can also look strage at first. Once you get used to it, you will appreciate the brevity. 3.4 Select and operate on columns with j You can do a lot more than just return the columns specified in j. In fact, you can perform any operation you want on them. # Select every row of columns y and z the long way dt[, list(y, z)] ## y z ## 1: a 0.801076986 ## 2: a 0.886827043 ## 3: b -1.216112121 ## 4: b -1.494334054 ## 5: c 0.467194196 ## 6: c -0.530958683 ## 7: d -0.891427670 ## 8: d 0.787302046 ## 9: e 0.006745659 ## 10: e -1.836147935 ## 11: f 0.052353254 ## 12: f -1.018931991 # Select every row of columns y and z the cool way dt[, .(y, z)] ## y z ## 1: a 0.801076986 ## 2: a 0.886827043 ## 3: b -1.216112121 ## 4: b -1.494334054 ## 5: c 0.467194196 ## 6: c -0.530958683 ## 7: d -0.891427670 ## 8: d 0.787302046 ## 9: e 0.006745659 ## 10: e -1.836147935 ## 11: f 0.052353254 ## 12: f -1.018931991 # select rows 1:4 and return the z column of the result as a # vector dt[1:4, z] ## [1] 0.801077 0.886827 -1.216112 -1.494334 # select rows 1:4 and return the z column of the result as a # data.table dt[1:4, list(z)] ## z ## 1: 0.801077 ## 2: 0.886827 ## 3: -1.216112 ## 4: -1.494334 # select rows 1:4 and return the z column of the result as a # data.table, but use `.` to abbreviate `list`. This is a # thing specific to data.table. dt[1:4, .(z)] ## z ## 1: 0.801077 ## 2: 0.886827 ## 3: -1.216112 ## 4: -1.494334 # select and operate on columns # select rows 1:4 and return a summary statistic of the # resulting z column dt[1:4, mean(z)] ## [1] -0.2556355 dt[1:4, sd(z)] ## [1] 1.275245 # notice the default naming of the columns in the following # result dt[1:4, list(mean(z), sd(z))] ## V1 V2 ## 1: -0.2556355 1.275245 # control the names of the resulting columns dt[1:4, list(z_mean = mean(z), z_sd = sd(z))] ## z_mean z_sd ## 1: -0.2556355 1.275245 # select all rows and compute summary statistics of the # resulting data.table dt[, .(mean(z), sd(z))] # default column names ## V1 V2 ## 1: -0.3322011 0.9602548 dt[, list(z_mean=mean(z), z_sd=sd(z))] # custom column names ## z_mean z_sd ## 1: -0.3322011 0.9602548 3.5 Combine i and j It is straightforward to combine i and j. # Select all rows for which x==II and return columns y and z dt[x==&#39;II&#39;, .(y, z)] ## y z ## 1: b -1.4943341 ## 2: c 0.4671942 ## 3: c -0.5309587 3.6 Group using by The real magic of data.table comes in the power of the by argument. by allows you apply the operation that you specify with j to separate groups of rows defined by the unique values in the columns specified in by. Put another way, whatever column you pass to by will be split up into groups with each unique value getting its own group. Those groups will then be applied to the columns you specify in j and the operation also specified in j will be applied only to rows from the same group. Then, after all is done, everything is put back into a single data.table. # inefficient way to group by x dt[x==&quot;I&quot;, mean(z)] ## [1] 0.157264 dt[x==&quot;II&quot;, mean(z)] ## [1] -0.5193662 dt[x==&quot;III&quot;, mean(z)] ## [1] -0.03245999 dt[x==&quot;IV&quot;, mean(z)] ## [1] -0.9342422 # efficient and beautiful way to group by x dt[, mean(z), .(x)] ## x V1 ## 1: I 0.15726397 ## 2: II -0.51936618 ## 3: III -0.03245999 ## 4: IV -0.93424222 # the efficient and beautiful way of doing things extends to # grouping by multiple variables with ease dt[, mean(z), .(x, y)] ## x y V1 ## 1: I a 0.843952015 ## 2: I b -1.216112121 ## 3: II b -1.494334054 ## 4: II c -0.031882243 ## 5: III d -0.052062812 ## 6: III e 0.006745659 ## 7: IV e -1.836147935 ## 8: IV f -0.483289369 # return more than one summary statistic grouped by x dt[, .(mean(z), sd(z)), .(x)] # default naming of resulting columns ## x V1 V2 ## 1: I 0.15726397 1.1901511 ## 2: II -0.51936618 0.9808155 ## 3: III -0.03245999 0.8400513 ## 4: IV -0.93424222 0.9470947 dt[, .(z_mean=mean(z), z_sd=sd(z)), .(x)] # custom naming of resulting columns ## x z_mean z_sd ## 1: I 0.15726397 1.1901511 ## 2: II -0.51936618 0.9808155 ## 3: III -0.03245999 0.8400513 ## 4: IV -0.93424222 0.9470947 3.7 Adding and modifying columns Use the := operator inside the j argument of a data.table to add or modify a column. # if you pass a single number, data.table will fill the # entire column with that value dt[, a := 9] # otherwise, just pass a vector of the same length as the # data.table dt[, b := seq(2, 24, 2)] # notice that you get an error if you try to create a column # using a vector that isn&#39;t exactly the correct length dt[, b := seq(2, 26, 2)] ## Error in `[.data.table`(dt, , `:=`(b, seq(2, 26, 2))): Supplied 13 items to be assigned to 12 items of column &#39;b&#39;. If you wish to &#39;recycle&#39; the RHS please use rep() to make this intent clear to readers of your code. # redefine or modify an existing column the same way dt[, b := seq(4, 48, 4)] # remove a column using := and the NULL keyword dt[, b := NULL] # don&#39;t use the `&lt;-` operator and the `:=`operator at the # same time. It works but is inefficient dt &lt;- dt[, c := 9] # don&#39;t do this # it all works the same with non-numerical columns dt[, betsy := &#39;mouse&#39;] dt[, betsy := c(&#39;mouse&#39;, &#39;rat&#39;)] # notice the error ## Error in `[.data.table`(dt, , `:=`(betsy, c(&quot;mouse&quot;, &quot;rat&quot;))): Supplied 2 items to be assigned to 12 items of column &#39;betsy&#39;. If you wish to &#39;recycle&#39; the RHS please use rep() to make this intent clear to readers of your code. new_col_betsy &lt;- rep(c(&#39;mouse&#39;, &#39;rat&#39;), 6) dt[, betsy := new_col_betsy] # add betsy 3.8 Be careful when copying data.table objects dt2 = dt creates a shallow copy. This means that there is really only one data.table object in memory, but it can be referred to by both names. This means that changes to dt2 will be reflected in dt and vice-versa. You can show that this is the case by modifying dt2 and observing that dt also changes (see code chunks below). To create a deep copy use dt2 = data.table(dt). # print dt to see what it looks like before messing with it dt ## x y z a c betsy ## 1: I a 0.801076986 9 9 mouse ## 2: I a 0.886827043 9 9 rat ## 3: I b -1.216112121 9 9 mouse ## 4: II b -1.494334054 9 9 rat ## 5: II c 0.467194196 9 9 mouse ## 6: II c -0.530958683 9 9 rat ## 7: III d -0.891427670 9 9 mouse ## 8: III d 0.787302046 9 9 rat ## 9: III e 0.006745659 9 9 mouse ## 10: IV e -1.836147935 9 9 rat ## 11: IV f 0.052353254 9 9 mouse ## 12: IV f -1.018931991 9 9 rat # this is a shallow copy dt_copy = dt dt_copy[, a := NULL] # notice we also deleted the `a` column from dt, not just from # dt_copy dt ## x y z c betsy ## 1: I a 0.801076986 9 mouse ## 2: I a 0.886827043 9 rat ## 3: I b -1.216112121 9 mouse ## 4: II b -1.494334054 9 rat ## 5: II c 0.467194196 9 mouse ## 6: II c -0.530958683 9 rat ## 7: III d -0.891427670 9 mouse ## 8: III d 0.787302046 9 rat ## 9: III e 0.006745659 9 mouse ## 10: IV e -1.836147935 9 rat ## 11: IV f 0.052353254 9 mouse ## 12: IV f -1.018931991 9 rat # this is a deep copy dt_copy = data.table(dt) dt_copy[, x := NULL] # now our changes to dt_copy didn&#39;t also change dt dt ## x y z c betsy ## 1: I a 0.801076986 9 mouse ## 2: I a 0.886827043 9 rat ## 3: I b -1.216112121 9 mouse ## 4: II b -1.494334054 9 rat ## 5: II c 0.467194196 9 mouse ## 6: II c -0.530958683 9 rat ## 7: III d -0.891427670 9 mouse ## 8: III d 0.787302046 9 rat ## 9: III e 0.006745659 9 mouse ## 10: IV e -1.836147935 9 rat ## 11: IV f 0.052353254 9 mouse ## 12: IV f -1.018931991 9 rat 3.9 Getting data into a data.table The first and primary method that will use to get data into a data.table is the fread function, which is part of the data.table library. The following code chunk reads a .csv file into data.table. # Location of the csv file # can be on the internet or on your local machine f &lt;- &#39;https://crossley.github.io/book_stats/data/maze/maze.csv&#39; # use the fread function d &lt;-fread(f) d ## rat maze run time ## 1: 1 1 1 182.5532 ## 2: 1 1 2 180.5880 ## 3: 1 1 3 161.5351 ## 4: 1 1 4 198.2432 ## 5: 1 1 5 155.6673 ## --- ## 6012: 47 8 12 323.0179 ## 6013: 47 8 13 205.0180 ## 6014: 47 8 14 228.6531 ## 6015: 47 8 15 234.1131 ## 6016: 47 8 16 305.8473 A second method that we will sometimes use to get data into a data.table is as follows: s1 &lt;- &quot;family_id age_mother dob_child1 dob_child2 dob_child3 1 30 1998-11-26 2000-01-29 NA 2 27 1996-06-22 NA NA 3 26 2002-07-11 2004-04-05 2007-09-02 4 32 2004-10-10 2009-08-27 2012-07-21 5 29 2000-12-05 2005-02-28 NA&quot; DT &lt;- fread(s1) DT ## family_id age_mother dob_child1 dob_child2 dob_child3 ## 1: 1 30 1998-11-26 2000-01-29 &lt;NA&gt; ## 2: 2 27 1996-06-22 &lt;NA&gt; &lt;NA&gt; ## 3: 3 26 2002-07-11 2004-04-05 2007-09-02 ## 4: 4 32 2004-10-10 2009-08-27 2012-07-21 ## 5: 5 29 2000-12-05 2005-02-28 &lt;NA&gt; This can be useful in a few odd circumstances. For example, you may wish to copy some text in another program and paste it over into your R session to turn it into a data.table. 3.10 Wide vs long format The following data.table is in long format: ## run rat maze_time ## 1: 1 1 88.417468 ## 2: 1 2 18.503129 ## 3: 1 3 124.226348 ## 4: 1 4 -42.509839 ## 5: 1 5 136.594112 ## 6: 2 1 124.841265 ## 7: 2 2 106.528818 ## 8: 2 3 101.915639 ## 9: 2 4 125.733838 ## 10: 2 5 35.098992 ## 11: 3 1 88.083124 ## 12: 3 2 166.413570 ## 13: 3 3 210.096910 ## 14: 3 4 114.377148 ## 15: 3 5 88.224640 ## 16: 4 1 8.793163 ## 17: 4 2 -43.758624 ## 18: 4 3 20.291047 ## 19: 4 4 225.408311 ## 20: 4 5 177.214219 ## 21: 5 1 78.048437 ## 22: 5 2 57.518972 ## 23: 5 3 58.101990 ## 24: 5 4 199.698686 ## 25: 5 5 72.422197 ## run rat maze_time The following data.table is in wide format: ## maze_time_rat_1 maze_time_rat_2 maze_time_rat_3 maze_time_rat_4 ## 1: 223.83041 16.795670 183.20471 20.46609 ## 2: 72.06537 -16.657055 77.26713 94.51225 ## 3: 275.79031 -6.559058 126.61374 125.01413 ## 4: 156.07461 -56.378205 62.32973 161.82433 ## 5: 54.72160 215.653700 344.13646 82.73765 ## maze_time_rat_5 run ## 1: -122.390027 1 ## 2: -26.361438 2 ## 3: 135.872890 3 ## 4: 98.895452 4 ## 5: 5.935084 5 Each format has its pros and cons, and you may find yourself needing to switch between them even within the same dataset. To go from wide to long format you should use the melt function from the data.table library. To go from long to wide format you should use the dcast function from the data.table library. Using these functions is just a bit beyond the scope of this lecture, but we will return later in the unit to learn about them. If you are feeling confident in your use of data.table then you may enjoy getting a little ahead of the game by reading the following vignette: reshaping data.table vignette "],["introduction-to-ggplot.html", "4 Introduction to ggplot 4.1 geom_histogram 4.2 geom_point 4.3 geom_boxplot 4.4 geom_violin 4.5 geom_hist 4.6 geom_bar 4.7 geom_line", " 4 Introduction to ggplot This chapter introduces the ggplot library in R. We will cover some basic data visualization techniques essential for effectively communicating descriptive statistics. For example, we will see how to make scatter plots, bar plots, box plots, histograms, and violin plots, all of which are common and useful when analyzing and presenting data. Data visualisation is a core component of modern descriptive statistics and it is essential that we become proficient at collecting data, assembling it into csv-style formats, piping it into R, and inspecting it using plots. There are numerous methods to generate plots using R, but in this unit, we will focus exclusively on first representing our data as a data.table and then using ggplot to make plots. Note that many of the visualisations we will demonstrate below look best when we have lots of data, so lets make a big toy example now. # define example data with more observations x1 &lt;- runif(100) x2 &lt;- runif(100) x3 &lt;- runif(100) # create a wide format data.table d_wide &lt;- data.table(x1, x2, x3) # convert to a long format data.table d_long &lt;- melt(d_wide, measure.vars=c(&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;)) 4.1 geom_histogram 4.2 geom_point Perhaps the most straightforward approach to visualising data is simply to plot points for each observation in your sample. ggplot(data=d_long, aes(x=variable, y=value)) + geom_point() 4.3 geom_boxplot Box plots give a summary of how your data is distributed by visually marking out the median value as well as the 25th to the 75th percentile. The idea is to concisely illustrate where the majority of the data fall. Whiskers will typically extend from the ends of the box to indicate the more extreme end of your data, and very extreme data points will often be plotted individually. This aspect of box plots doesn’t have as strong a convention as the rest, so it’s important to read the documentation to be sure you are plotting what you think you are plotting. I often find that it can be quite nice to use both geom_point and geom_box in conjunction as follows. ggplot(data=d_long, aes(x=variable, y=value)) + geom_boxplot() + geom_point() 4.4 geom_violin Violin plots are similar to box plots, except rather than represent your data in terms of percentiles, it attempts to give you a continuous estimate of how much our your data fall along the range of possible values. As with box plots, I often like to overlay individual points. ggplot(data=d_long, aes(x=variable, y=value)) + geom_violin() + geom_point() 4.5 geom_hist A histogram attempts to illustrate how data is distributed by grouping data points into a set number of bins, and plotting a bar with height equal to the number of points in each bin. If you have a lot of data, this method can work really well and convey lots of great information. With a smaller data set — like the toy rat example that we are currently using — it only works okay. In any case, working with histograms can involve a bit of tweaking to get things to look nice. For example, you can control how many bins are used and how big or small each bin is by using the bins and breaks argument. ggplot(data=d_long, aes(x=value, fill=variable)) + geom_histogram(bins = 10) + facet_wrap(~variable) 4.6 geom_bar Bar plots are among the most common plots you will encounter as you navigate pretty much any scientific field. They throw away all information about how your data is distributed, and instead report only on the average values (unless error bars are included). There are many ways to use ggplot and geom_bar to make a bar plot. A good way to start is to first create a data.table that contains only the average values that you want to be represented by the bar heights: d_mean = d_long[, .(var_mean = mean(value)), .(variable)] ggplot(data=d_mean, aes(x=variable, y=var_mean)) + geom_bar(stat=&#39;identity&#39;) 4.7 geom_line Of course, in certain circumstance, we might want to make a simple line plot. This is also easy to achieve with ggplot, but to demonstrate its use, we will need a different example data set. x &lt;- seq(-10, 10, 0.1) y &lt;- sin(x) d &lt;- data.table(x, y) ggplot(data=d, aes(x=x, y=y))+ geom_line() + ylab(&#39;sin(x)&#39;) "],["basic-descriptive-statistics.html", "5 Basic descriptive statistics 5.1 Central tendency of a sample 5.2 Spread of a sample", " 5 Basic descriptive statistics 5.1 Central tendency of a sample Given a sample, a measure of central tendency is supposed to tell us where most values tend to be clustered. One very common measure of sample central tendency is called the sample mean. The sample mean is denoted by \\(\\overline{\\boldsymbol{x}}\\), and is defined by the following equation: \\[ \\overline{\\boldsymbol{x}} = \\frac{x_1 + x_2 + x_3 + ... + x_n}{n} \\] We can write this concisely as: \\[ \\overline{\\boldsymbol{x}} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} \\] Another common measure of sample central tendency is called the sample median. We will denote it by \\(\\widetilde{\\boldsymbol{x}}\\), and it is defined simply as the value that splits the observations in half. Finally, sample mode is the element that occurs most often in the sample. 5.1.1 Central tendency by hand Suppose you have the following observations: \\[ \\boldsymbol{x} = (55, 35, 23, 44, 31) \\] To compute the mean, we simply plug these numbers into the equation. \\[ \\overline{\\boldsymbol{x}} = \\frac{55 + 35 + 23 + 44 + 31}{5} = \\frac{188}{5} = 37.6 \\] To compute the median, first sort the data from smallest to largest: \\[ \\boldsymbol{x}_{sorted} = (23, 31, 35, 44, 55) \\] Then, pick the value that ends up in the middle: \\[ \\widetilde{\\boldsymbol{x}} = 35 \\] Since we have an odd number of observations, finding the median is pretty intuitive, but what if we had an even number of observations? In this case, we will take the mean of the middle two numbers. \\[ \\boldsymbol{x} = (55, 35, 23, 44) \\] \\[ \\boldsymbol{x}_{sorted} = (23, 35, 44, 55) \\] \\[ \\widetilde{\\boldsymbol{x}} = \\frac{35 + 44}{2} = 39.5 \\] 5.1.2 Central tendency using R In general, things are easier and we are happier and more productive human beings if we use R. We just store our sample observations in a variable x, and use built-in R functions mean() and median() to compute the sample mean and sample median. # mean and median of sample 1 above x &lt;- c(55, 35, 23, 44, 31) mean(x) ## [1] 37.6 median(x) ## [1] 35 # mean and median of sample 2 above x &lt;- c(55, 35, 23, 44) mean(x) ## [1] 39.25 median(x) ## [1] 39.5 5.1.3 Central tendency and outliers Sometimes a sample contains a few observations that are very different from the majority of the others. Theses observations are called outliers. How will outliers influence our measures of central tendency? To answer this question, consider the rat maze running example from above. # define vectors that contain the maze times from the # example given at the top of the lecture. x1 &lt;- c(52.38, 55.41, 70.88, 43.30, 50.15, 41.99, 36.82, 34.05, 52.70, 72.25) x2 &lt;- c(62.36, 53.89, 53.95, 33.81, 61.12, 61.48, 36.89, 49.45, 52.50, 50.95) x3 &lt;- c(52.04, 48.28, 48.12, 58.89, 51.76, 42.88, 49.04, 60.41, 53.99, 70.06) # combine all observations into a single vector x &lt;- c(x1, x2, x3) # compute the mean and median of x mean(x) ## [1] 52.06 median(x) ## [1] 52.21 # add an outlier to x x &lt;- c(x, 300) # compute the mean and median of x with the outlier in the data mean(x) ## [1] 60.05806 median(x) ## [1] 52.38 Here, you can see that the mean, but much less so the median, is sensitive to outliers. So, which is a better measure of central tendency? The answer to this question depends entirely on what you think is an outlier and how much you care about them. Saying much more than that is beyond the scope of this lecture, but we should leave with at least a simple lesson: it is always a good idea to identify and investigate outliers in our data. 5.2 Spread of a sample Measures of spread of a sample are supposed to tell us how widely the sample observations are distributed. One very common measure of spread is called sample variance. It is denoted by \\(\\boldsymbol{s}^2\\) and it is defined as: \\[ \\boldsymbol{s}^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} ( x_{i} - \\overline{\\boldsymbol{x}} )^2 \\] An additional measure of spread is called the sample standard deviation. It is denoted by, \\(\\boldsymbol{s}\\), and it is defined simply as the square root of the sample variance. \\[ \\boldsymbol{s} = \\sqrt{\\boldsymbol{s}^2} \\] A third measure of spread that we will consider is called the sample range, and it is defined as the difference between the most extreme observed values. 5.2.1 Spread by hand Consider the following sample: \\[ \\boldsymbol{x} = (55, 35, 23, 44) \\] If for some reason you needed to compute sample variance, and every computer near you was broken, then you could compute the sample variance of this sample by hand as follows: \\[ \\boldsymbol{s}^2 = \\frac{ (55-39.25)^2 + (35 -39.25)^2 + (23-39.25)^2 + (44-39.25)^2 }{4-1} \\] \\[ \\boldsymbol{s}^2 = \\frac{ (15.75)^2 + (-4.25)^2 + (-16.25)^2 + (4.75)^2 }{4-1} \\] \\[ \\boldsymbol{s}^2 = \\frac{ 248.0625 + 18.0625 + 264.0625 + 22.5625 }{4-1} \\] \\[ \\boldsymbol{s}^2 = \\frac{ 552.75 }{4-1} = 184.25 \\] Well, that sucked, and in the next section we will see that R will do this for us with grace and ease. 5.2.2 Spread by R We now use R to quickly compute the all measures of spread just covered. var(x) ## [1] 2080.861 sd(x) ## [1] 45.61645 diff(range(x)) ## [1] 266.19 Notice that that sample variance is larger than sample standard deviation. This will always be true. "],["random-variables.html", "6 Random variables 6.1 Defining traits of random variables 6.2 Continuous vs discrete 6.3 Finite vs Infinite 6.4 Sampling from random variables 6.5 Example: Sampling from a simple discrete random variable 6.6 Example: Sampling from a simple continuous random variable 6.7 Summary 6.8 Using samples to estimate probability distributions 6.9 Discrete distributions", " 6 Random variables My undergraduate degree is in physics. Often, when this comes up in conversation others have commented what a large jump it must have been to go from physics to cognitive neuroscience. I can definitely understand where this feeling must come from, but my experience of the transition has been only a very minor gaps. Ultimately we are all scientists, and as such, our primary objective is to uncover and understand the underlying processes that govern the physical universe. Physicists are often equipped with one or many very good mathemtical descriptions of the universe. In fact, the physicist is very lucky becuase these mathematical descriptions don’t just describe the observations that have already been obtained about our universe, but they go even farther by specifying a process that could have generated those observations. By specifiying a process, these descriptions generate predictions about the outcomes of future experiments. So you can see that these descriptions are actually theories of the laws of physics. We cognitive neuroscientists are in the same boat but the physisict is far luckier than us in at least one respect. The systems they study are in many cases far simpler to describe then the behaviour of the human nervous system. Where the phenomena they seek to describe are amenable to fine print descirptions of process, we cognitive neuroscientists are often left much more in the dark. This doesn’t mean that we give up. It just means that we have to rely more heavily on statistical thinking than our physicist counterparts. The key idea I would like you to take away from these ramblings is that every time you perform a statistical test (e.g., t-test, ANOVA, regression, etc.) you are in principle building and testing a potential model of the universe. It’s not as deep as the models physicists get to play with becuase a statistical model doesn’t specifiy the process through which observations are generated. Instead, a statistical model simply specifies the outcomes that you are likely to observe if a particular model is true. Or conversely, it can tell you how likely a particular set of observations was to come from a particular model. The idea is that by doing very careful statistical thinking, we will be able to narrow down our understanding of the sort of outcomes that characterise our little sub-area of cognitive neuroscience, and this will eventually enable us to build models that do specify process as our physicist colleagues enjoy. At the heart a statistical model is the concept of a random variable. You need to think a little bit abstractly to really understand what a random variable is. It is tempting to think of a random variable as a single number than is somehow randomly chosen. Or perhaps your intuition tells that a random variable is somehow something that you can hold in your hands or know directly. Unfortunately, these intuitions are not quite right. Rather, a random variable is a process that generates data. It is therefore conceptually tied to the laws that govern the physical universe themselves and also to the speciifc details of the experiment that the data was collected from. because all data that we might ever observe will be subject to these laws. The connectino to the specific experiment comes from the fact it is the details of our experiment that dictates what exactly we measure. 6.1 Defining traits of random variables Population and Sample space: The entire set of things under study is a population . Here, a “thing” can be just about anything. It can refer to people with a specific demographic, neurons in a particular part of the primate brain, or even the possible outcomes of a partciular experiment. A population can be finite or infinite, and it can be discrete or continuous (more on these terms soon). Generally, when we conduct a study, the population is what we aim to learn something about. A sample space is the set of all possible outcomes of an experiment. For instance, the sample space of a coin toss is \\({Heads, Tails}\\), the sample space for rolling a six-sided die is \\(\\{1, 2, 3, 4, 5, 6\\}\\), and the sample space of both tossing a coin and rolling a die is \\(\\{H1, H2, H3, H4, H5, H6, T1, T2, T3, T4, T5, T6\\}\\). But like populations, sample spaces can be finite or infinite, and continuous or discrete. Populations and sample spaces are clearly quite similar but there are some graspable distinctions. In particular, a popoulation isn’t tied very strongly to a particular experiment whereas a sample space is. Probability distribution: A function that assigns a probability to each possible outcome in the sample space. Every random variable is in essence defined by its probability distribution. This makes probability distributions one of most important concepts in statistics. Since we use random variables as our models of the brain universe, and every random variable is defined by its probability distribution, it follows that probability distributions are the things that allow us to make predictions about the outcomes of future experiments. Sometimes we won’t be in a very good position to know or observe the entire probability distribution of a random variable so we might instead attempt to work with only pieces of the distribution. For example, we might focus on the central tendency of the distribution (i.e. ,what observations does it produce the most of), or the spread of the distribution (i.e., how much do the observations vary). Both of the defining traits I’ve emphasised above require further explanation to be fully understood. We will need to build a solid understanding of what it means for a population or a sample space to be infinite vs finite; what it means for it to be continuous vs discrete; and how to go about specifying a probability distribution. These topcis will lead us down the road of probability theory and experiment design. 6.2 Continuous vs discrete The data that random variables produce can be either discrete or continuous. In a rather straight-forward use of terms, a random variable that produces discrete data is called a discrete random variable and a random variable that produces continuous data is called a continuous random variable. For example, consider the random variable \\(X\\) defined as a process that returns the body lengths in cm of rats drawn from a population defined as all rats in the universe. The sample space for this random variable is all real numbers greater than zero. A sample from this random variable might look as follows: \\[ \\boldsymbol{x} = (2.877970, 7.497241, 5.455286, 4.903578, 7.806955) \\] Here, \\(X\\) is a continuous random variable because it seems that any value greater than zero can be obtained when sampling from it. As another example, consider the random variable \\(X\\) defined as a process that returns the number of neurons in a rat cerebral cortex drawn from the same population as in the previous example. A sample from this random variable might look as follows: \\[ \\boldsymbol{x} = (21e6, 21e6, 18e6, 20e6, 22e6) \\] Here, \\(X\\) is a discrete random variable because that only integer values can be obtained when sampling from it. As a third example, we might perform an experiment where we observe a rat navigate a maze several times, and measure the time to completion for each maze run. Suppose that we observed 10 runs with the following times in seconds: \\[ \\boldsymbol{x} = (52.38, 55.41, 70.88, 43.30, 50.15, 41.99, 36.82, 34.05, 52.70, 72.25) \\] Here, \\(X\\) is a again a continuous random variable because time can take any real number value. However, if instead of measuring time, we measured the number of turns the rat took to complete the maze, then our sample might look like this: \\[ \\boldsymbol{x} = (8, 2, 10, 7, 3, 1, 6, 9, 5, 4) \\] Here, \\(X\\) is a discrete random variable. In summary, continuous random variables produce continuous data and discrete random variables produce discrete data. Continuous data can take any real number value, while discrete data can only take a discrete set of values. 6.3 Finite vs Infinite Populations and sample spaces can be composed of either finite or infinite sets. In another rather staight-forward use of terms, they are finite if they contain a finite number of things and they are called infinite if they contain an infinite number of things. For example, the population of all neurons in a particular human brain is finite because – while there are certainly a lot of them – their number is finite. As a more tradtiional example, the sample space of rolling a six-sided die is finite, consisting of just six possible outcomes: {1, 2, 3, 4, 5, 6}. In contrast, the population of times a specific neuron generates an action potential after stimulus onset is infinite. To see this consider simply consider the number of possible times that exist between 1.0 and 1.1 seconds. It could be 1.01, 1.001, 1.0001, and so on. No matter how small you make an increment, you can always make another increment smaller than the last. This leads to infinity. 6.4 Sampling from random variables When we run an experiment and gather data we say that we are sampling or that we are drawing samples from the random variable that generates data in our experiment. Suppose we have a two electrodes implanted in a brain – one in region A and another in region B – and we want to know if region A causes firing in region B. Our approach is to inject some excitatory current into region A and measure the effect on the firing rate of region B. In principle, if we observe more spike in region B when we inject current in region A, then we have evidence that region A drives region B. However, there’s a bit of a problem. Region B has some baseline firing rate that is not zero. So, if we observe spikes in region B, we can’t be sure if they were caused by the current we injected into region A or if they were just spontaneous spikes. We need to somehow determine if the spikes in region B are more than we would expect by chance. Our approach is to use a random variable \\(X\\) to model the number spikes would expect to see in region B during region A stimulation. That is, \\(X\\) is the process that generates the number of spikes we observe in region B during region A stimulation. In this case, the sample space of \\(X\\) is composed of all possible number of spikes we might observe in our experiment, \\({0, 1, 2, 3, ... }\\). That we can figure out just by thinking about it. But what we really want to know is the probabily distribution of \\(X\\). In eseence, this would allow us to say whether or not \\(X\\) is more likely to generate a spike (or gennerate more spikes) during the stimulation of region A. This probability distribution is set by the universe and we unfortunately cannot know it just by thinking about it. The best we can do is run our experiment and try to use the data that we get to make a best guess about what we think this probability distribution is. This last sentence is very important. We run our experiemnt and get some data. That is, we obtain samples from our random variable. We then must use these samples to make a good guess about the probability distributino of random variable that gave us the data in the first place. In essence, we use samples to estimate properties of random variables. 6.5 Example: Sampling from a simple discrete random variable Consider a random variable \\(X\\) defined as the number of correct responses a participant gives in a short memory recall test. Suppose that in this test, participants are shown a list of five words for a brief period and then asked to recall as many words as possible after a short delay. The sample space of \\(X\\) is \\({0, 1, 2, 3, 4, 5}\\). We can see that the sample space of \\(X\\) is both discrete and finite. We cannot know with 100% certainty the true probability distribition for \\(X\\). However, as an eaxmple, lets suppose that some higher power has given us this information and it looks as follows: The above plot is called a probability distribution. It shows possible outcomes of the random variable \\(X\\) on the x-axis and the probability of each outcome on the y-axis. There are a few subtle details about this plot that we will cover in a later section. For now, lets focus on the most important aspect: The taller the bar,, the more likely the corresponding outcome is to occur. If this was the true probability distribution for \\(X\\), and we performed out experiment a few times, we might get: ## [1] 4 0 2 2 1 Note that every time we ran our experiment we get different results. Recall that this is the essence of what it means for \\(X\\) to be a random variable. Also note that it is difficult to get a good sense of the overall pattern of the results by just looking at the first few. Rather, we need to look at the results of many. If we performed our experiment many times – many more times than would be conventient to print the results in line (say \\(n=1000\\)) – then we need a better way to visualize the results. With discrete random variables a good way to do this is to use a simple bar plot. We can see that the most common outcomes are 2 and 3, which is what we would expect given that the peak of the probability distribution occurs at this value, but we also get many other outcomes. In general, the count of occurrences of each outcomee is about proportional to the height of the bar in the probability distribution. This is the essence of the concept of a probability distribution. 6.6 Example: Sampling from a simple continuous random variable Consider a random variable \\(X\\) defined as the reaction time (in seconds) of participants in an experiment where they are asked to press a button as soon as they see a light flash on a screen. Here, \\(X\\) is a continuous random variable because it can take any value that is greater than zero. Of course, as an experimenter, you probably don’t want to wait around for infinity seconds for your participant to finish, so there are some practical cutoffs. Those can in principle be baked into to the analysis for now lets keep our lives simple and ignore them. Again, lets further suppose that some higher power has given us knowledge of the true probability distribution for \\(X\\). There are again several important and subtle aspects of this probability distribution that we will wait until later to address. For now, lets focus on the big picture. We see that very short (close to zero) and very long reaction times are both very unlikely. We also see that the spread of possible reaction times is bunched up between zero and the peak of the disribution and that it is stretched out from the peak to the extreme long reaction times. If this was the true distribution for \\(X\\), and we performed our experiment many times, we might obtain the following: We can again see that the most common outcome is near the peak of the probability distribution and that the relative frequency of each outcome is about proportional to the height of the probability distribution at the corresponding outcome. 6.7 Summary Random variables are data generating processes defined by a probability distribution. The probability distribution specifies how likely each possible outcome is to occur. When we sample from a random variable – e.g., by performing an experiment – we obtain a set of outcomes. The relative frequency of each outcome will generally be about proportional to the height of the probability distribution at the corresponding outcome. 6.8 Using samples to estimate probability distributions A random variable is a data generating process. You can think of it like an infinitely deep bucket full of experiment outcomes. Whenever you perform an experiment, you reach into the bucket, and pull out one outcome at random. All possible experiment outcomes contained by the bucket define the population under study, and the set of probabilities corresponding to each possible outcome is the probability distribution. Any outcome you pull out of the bucket is a sample from the random variable. In general, we use samples to estimate full probabilitiy distributions or aspects of probability distributions (e.g., their central tendancy and dispersion). 6.9 Discrete distributions Consider an experiment where participants are shown a list of 10 words for a brief period and then asked to recall as many words as possible after a short delay. Lets define a random variable \\(X\\) as the number of words correctly recalled. With this definition, \\(X\\) can only take on integer values from 0 to 10. Suppose we have conducted the experiment with 100 participants and obtain the following sample. The height of each bar represents the count of participants who recalled that many words. We have seen in previous sections that the count of samples at each possible outcome is about proportional to the height of the probability mass function at that outcome. This means that the bar graph we’ve just plotted can be thought of as an estimate of the probability mass function of the random variable \\(X\\). The main problem with this estimate is that the sum of the height of all the bars does not equal 1. To fix this, we need only to normalize the height of each bar by the total number of samples (i.e., divide each bar height by the total number of samples). After normalisation we have the following: By using proportion instead of count we now have a valid estimate of the probability mass function of the random variable \\(X\\). This is a very simple example of how we can use samples to estimate probability distributions. In general, we can use the same approach to estimate any discrete probability distribution. 6.9.1 Continuous distributions Consider an experiment where the reaction time of participants is measured in seconds as they perform a task requiring quick responses to visual stimuli. Define a random variable \\(X\\) as the reaction time. Here, \\(X\\) can take any real value between zero and infinity. Suppose we conducted the experiment with 100 participants and obtained the following sample of reaction times. To estimate the probability distribution of \\(X\\) – the random variable that generated these reaction times – we use a histogram. We will also overlay a density plot to provide a smoother estimate of the distribution. ## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(density)` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. The histoogram makes intuitive sense to use in this context – becuase at least given good choices for bin width and large enough sample size it ends up looking like the distributions we sample from – but it is actually a stranger fit that if first appears. This is because a histogram uses discrete bins to estimate a continuous probability distribution. This should bring you back to your introduction to calculus. In calculus, you learned that you can estimate the area under a curve by dividing the area into small rectangles and then summing the area of the rectangles. The samller the rectangles the better the estimate. A very similar mechanism is at play with a histogram. "],["probability-fundamentals.html", "7 Probability fundamentals 7.1 Fundamental Concepts 7.2 Probability Rules 7.3 Probability distributions 7.4 Computing probabilities from probability distributions", " 7 Probability fundamentals The foundation of statistics is built on the modeling of real-world phenomena through random variables, which in turn are characterized by sample spaces and probability distributions. Here we dive a bit deeper into the formal aspects of probability theory so that we may better use and interpret statistical models. 7.1 Fundamental Concepts The sample space of an experiment is the set of all possible outcomes of that experiment. An outcome is a possible result of an experiment. An event is a set of outcomes of an experiment. An elementary event is an event which contains only a single outcome in the sample space. 7.1.1 Example: Coin Flip Experiment Consider flipping a coin twice, leading to these possible outcomes: Outcome # 1st Flip 2nd Flip 1 H H 2 H T 3 T H 4 T T Here, the sample space is \\(\\{(H,H), (H,T), (T,H), (T,T)\\}\\). Each individual row is a possible outcome. An example event could be \\(\\{(H,H), (T,T)\\}\\), representing getting the same side in both flips. Notice that it is composed of two individual outcomes. An elementary event, such as \\(\\{(H,H)\\}\\), specifies a single outcome from the sample space. 7.1.2 Probability Defined Probability is simply the likelihood of an event occurring. if the sample space is discrete, and the likelihood of each outcome is equal, then the probability of an event is simply the proportion of outcomes contained by that event relative to all possible outcomes. A similar definition for probability holds if the space of possible outcomes is continuous, but there are a few interesting and not immediately intuitive little kinks that we will need to deal with. For instance, if the sample space is continuous, then the probability of any any single event is actually zero (more on this in a later lecture). 7.2 Probability Rules Probabilities range between \\(0\\) and \\(1\\) (\\(0 \\leq P(e) \\leq 1\\) for any event \\(e\\)). The sum of probabilities for all outcomes in the sample space equals \\(1\\). The probability of an event not happening is \\(P(\\neg e) = 1 - P(e)\\). For mutually exclusive events (i.e., events that share no outcomes), the probability of any event occurring is the sum of their individual probabilities: \\[ P(E) = P(e_1 \\cup e_2 \\ldots \\cup e_n) = P(e_1) + P(e_2) + \\ldots + P(e_n) \\] 7.2.1 Independence, Intersection, and Union Events \\(e_1\\) and \\(e_2\\) are independent if the occurrence of one does not affect the likelihood of the other, defined as \\(P(e_1 \\cap e_2) = P(e_1)P(e_2)\\). The notation \\(P(e_1 \\cap e_2)\\) denotes the probability of \\(e_1\\) and \\(e_2\\) happening, highlighting the intersection of events. The notation \\(P(e_1 \\cup e_2)\\) denotes the probability of either \\(e_1\\) or \\(e_2\\) happening, highlighting the union of events. 7.2.2 Validating Probability Sets Let \\(A\\), \\(B\\), \\(C\\) share no outcomes and be the possible events of an experiment with the following probabilities of occurrence: \\[ P(A) = 0.4 \\\\ p(B) = 0.3 \\\\ P(C) = 0.3 \\\\ \\] This is a valid set of probabilities because they obey: \\(0 &lt; P &lt; 1\\) \\(P(A) + P(B) + P(C) = 1\\) Now suppose that we are told: \\[ P(A \\cup B) = 0.6 \\\\ P(A \\cup C) = 0.7 \\\\ P(B \\cup C) = 0.6 \\\\ \\] This does not obey the rules of probability because we are told that they do not share any outcomes and yet all union probabilities are not equal to the sum of their parts (e.g., \\(P(A \\cup B) \\neq P(A) + P(B)\\)). 7.3 Probability distributions If \\(X\\) is a random variable, then the probability distribution of \\(X\\) is a function that gives the probability that \\(X\\) will yield on a particular value when it is sampled. It is very imoprtant to emphasise that a probability distribution specifies the probability of every possible outcome the random variable can take. 7.3.1 Example: Discrete random variable Lets return to our coin flip experiment in which we flip a coin twice. The sample space for the experiment is given by the following table: Outcome # 1st Flip 2nd Flip 1 H H 2 H T 3 T H 4 T T Now, let \\(X\\) be the random variable that counts the number of heads in the experiment. The probability distribution of \\(X\\) is given by the following table: \\(x\\) \\(P(X=x)\\) 0 0.25 1 0.5 2 0.25 Note that here we have assumed that coin is fair and that the P(H) = P(T) = 0.5. With this assumption we are able to compute the probability of each outcome in the sample space by adding the number of heads in each row of the sample space table and divding by the total number of rows. Also, note that \\(X\\) cannot yield fewer than zero heads or more than two heads, so the probability distribution of \\(X\\) is fully specified by the above table. Lets plot the probability distribution of \\(X\\) and also draw a few samples from \\(X\\). As in previous examples we see that the majority of the samples are clustered around the outcomes associated with the highest probability in the probability distribution. 7.3.2 Example: Continuous random variable Lets return to our reaction time experiment in which we have a random variable \\(X\\) that measures the time it takes for a person to react to a stimulus. The sample space of this experiment is the set of all real numbers greater than zero which is both continuous and infinite. This means that we cannot specify the probability of every possible outcome using a table as we did for the discrete random variable. We can howeber visualise it using a plot. Lets assume that the probability distribution of \\(X\\) is given by the following plot: We again see that the majority of the samples are clustered around the outcomes associated with the highest probability. However, note that there is an important twist here. Notice that the y-axis of the plot of the probability distribution is labelled probability density f(x) and not simply P(X=x). This is because when \\(X\\) is continuous \\(P(X=x)=0\\) for all \\(x\\). The \\(f(x)\\) label refers to probability density. This is a very cool and counterintuitive idea that has important implications for how we interact with probability distributions. We turnt to this next. 7.4 Computing probabilities from probability distributions With discrete probability distributions, we were able to compute the probability of certain events by counting the total number of ways for an outcome to occur and dividing by the total number of possible outcomes. However, for continuous probability distributions, this method doesn’t work. To see this, consider the continuous distribution associated with the simple reaction time experiment in the previous section. Suppose we are interested in \\(p(X=1)\\). We first need to count the number of ways for an outcome to occur. For \\(p(X=1)\\) – a single outcome – there is just one way. Next – assuming equal probability of all outcomes (not a good assumption in this case but this is the least of our troubles at the moment) – we need to divde by the total number of possible outcomes. However, since \\(X\\) is continuous, there are an infinite number of possible outcomes. So we are left dividing one by infinity. Any real number divided by infinity is zero. So here lies our problem. The probability of any single outcome in a continuous distribution is zero. Perhaps the situation improves if instead of asking about the probbaility of single outcomes, we ask about the probability of ranges of outcomes. For example, what is the probability that \\(X\\) is less than \\(1\\)? How many ways are there for \\(x\\) to be less than \\(1\\)? Since \\(x\\) is continuous, there are infinite ways for this occur! What about the total number of possible outcomes? Right. Infinity again. So we are left dividing one infinity by another infinity. Quite a confusing and ill-defined situation. 7.4.1 Discrete distributions With discrete probability distributions – also called probability mass functions (pmfs) – finite and well-defined probabilities exist for every single possible outcome. This means that you can plot the distribution, and read probabilities right off the y-axis of the plot. 7.4.2 Continuous distributions With continuous probability distributions – aslo called probability density functions (pdfs) – the probability of any single outcome is zero. The distribution represents probability density rather than probability. In this case, probability is computed by taking the area under the curve. "],["moments-of-a-random-variable.html", "8 Moments of a random variable 8.1 The expected value of a random variable 8.2 The variance of a random variable 8.3 Using sample statistics to estimate random variable moments", " 8 Moments of a random variable Sometimes specififying the entire probability distribution is cumbersome or unnecessary. In these cases, we sometimes seek to characterize the shape of the distribution using the moments of the random variable. The moments of a random variable are simple scalar values that are computed from knowledge of the probability distribution. Moments provide insiight into a probability distribition’s central tendency, dispersion, skewness, kurtosis, etc. 8.1 The expected value of a random variable It’s cool to know that moments are a thing but a full treatment of moments is beyond the scope of this book. It will be sufficient to know that the first moment of a random variable is the expected value of the random variable and the second moment is the variance of the random variable. The expected value of a random variable is a measure of the central tendency of the random variable. It is also called the population mean. The expected value of a random variable is a statement of where most of your data is likely to fall if you sample the random variable many times. It is defined as the weighted average of the possible outcomes of the random variable, where the weights are given by the probability of each outcome. For a discrete random variable, the expected value is given by the following equation: \\[\\begin{align} \\mathbb{E}\\big[X\\big] &amp;= \\mu \\\\ &amp;= x_1 p(x_1) + x_2 p(x_2) + \\ldots + x_n p(x_n) \\\\ &amp;= \\sum_{i}^{n} x_i p(x_i) \\end{align}\\] When dealing with continuous random variables, this equation becomes the following: \\[\\begin{align} \\mathbb{E}\\big[X\\big] &amp;= \\mu \\\\ &amp;= \\int_{a}^{b} f(x) dx \\end{align}\\] where the possible outcomes of the random variable \\(X\\) are continuous in the interval \\([a, b]\\), and \\(f(x)\\) is the probability density function of \\(X\\). 8.2 The variance of a random variable We saw that for discrete random variables, population variance is defined as the expected squared deviation from the mean, as given by the following equation: The variance of a random variable – called the population variance is a measure of the spread of the random variable. It is a statement of how much the data is likely to deviate from the expected value if you sample the random variable many times. It is defined as the expected value of the squared deviation of the random variable from its mean. For discrete random variables, the variance is given by the following equation: \\[\\begin{align} \\mathbb{Var}\\big[X\\big] &amp;= \\sigma^2 \\\\ &amp;= E((X - \\mu)^2) \\\\ &amp;= \\sum_i x_{i}^2 p(X=x_{i}) - \\mu^2 \\end{align}\\] For continuous random variables, this equation becomes the following: \\[\\begin{align} \\mathbb{Var}\\big[X\\big] &amp;= \\sigma^2 \\\\ &amp;= E((X - \\mu)^2) \\\\ &amp;= \\int_{a}^{b} (x - \\mu)^2 f(x) dx \\end{align}\\] Computing the expected value and the variance of a continuous random variable requires the evauluation of integrals. This book will not cover the details of how to compute these integrals. Instead, it will be sufficient to know that the integral of a function over a given interval is the area under the curve of the function over that interval. That will be more than enough for our purposes. 8.3 Using sample statistics to estimate random variable moments 8.3.1 Population mean for discrete \\(X\\) Let \\(X\\) be a discrete random variable. Let \\(\\boldsymbol{x} = \\{x_1, \\ldots, x_n\\}\\) be a sample from \\(X\\). The central tendency of the sample \\(\\boldsymbol{x}\\) is given by the sample mean \\(\\bar{\\boldsymbol{x}}\\): \\[ \\begin{align} \\bar{\\boldsymbol{x}} &amp;= \\frac{1}{n} \\sum_{i=1}^{n} x_{i} \\end{align} \\] The true central tendency of \\(X\\) is given by the population mean which is denoted by \\(\\mu\\) and is defined by an operation called the expected value of \\(X\\) denoted \\(\\mathbb{E}\\big[X\\big]\\): \\[ \\begin{align} \\mathbb{E}\\big[\\boldsymbol{X}\\big] &amp;= \\mu \\\\ &amp;= x_1 p(x_1) + x_2 p(x_2) + \\ldots + x_n p(x_n) \\\\ &amp;= \\sum_{i}^{n} x_i p(x_i)\\\\ \\end{align} \\] If we do not know the true value of the population mean \\(\\mu\\) then we can estimate it using the sample mean \\(\\bar{\\boldsymbol{x}}\\). \\[ \\begin{align} \\hat{\\mu} &amp;= \\bar{\\boldsymbol{x}} \\end{align} \\] This is called a point estimate of \\(\\mu\\), because we are specifying a single number (i.e., a single point) that is our best guess for its true value. Later, we will learn about interval estimates of population parameters, which provide a range of best guess (e.g., we might try to say that we are \\(95\\%\\) percent sure that the true value of some population parameter is between some lower value and some upper value). We will see how to generate interval estimates in a later lecture. For now, it is sufficient to understand their conceptual relationship to point estimates. 8.3.2 Population variance for discrete \\(X\\) Similarly, a common measure of the spread of a sample is given by the sample variance \\(\\boldsymbol{s}^2\\): \\[ \\begin{align} \\boldsymbol{s}^2 &amp;= \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{\\boldsymbol{x}})^2 \\end{align} \\] The true variance of \\(X\\) is given by the population variance which is denoted by \\(\\sigma^2\\) and is defined as follows: \\[ \\begin{align} \\mathbb{Var}\\big[\\boldsymbol{X}\\big] &amp;= \\sigma^2 \\\\ &amp;= \\mathbb{E}\\big[(X - \\mu)^2\\big] \\\\ &amp;= \\sum(x^2 - 2x\\mu + \\mu^2) p(x) \\\\ &amp;= \\sum x^2 p(x) - \\sum 2 x \\mu p(x) + \\sum \\mu^2 p(x) \\\\ &amp;= \\sum x^2 p(x) - 2 \\mu \\sum x p(x) + \\mu^2 \\sum p(x) \\\\ &amp;= \\sum x^2 p(x) - 2 \\mu^2 + \\mu^2 \\\\ &amp;= \\left(\\sum_i x_{i}^2 p(x_{i})\\right) - \\mu^2 \\end{align} \\] If we do not know the true value of the population variance \\(\\sigma^2\\) then we can estimate it using the sample variance \\(\\bar{\\boldsymbol{s}^2}\\). \\[ \\begin{align} \\hat{\\sigma^2} &amp;= \\boldsymbol{s}^2 \\\\ \\end{align} \\] 8.3.3 Discrete \\(X\\) example As a concrete example, consider the following discrete probability distribution corresponding to the random variable \\(X\\). Note that this is a probability distribution. It defines how likely outcomes from a random variable is. It is not itself a sample from a random variable Since we are explicitly given the probability distribution — i.e., we are told exactly what the probability of each event is — so we can calculate the population mean \\(\\mu\\) as follows: \\[ \\begin{align} \\mu &amp;= \\mathbb{E}\\big[\\boldsymbol{X}\\big] \\\\ &amp;= \\sum_{i}^{n} x_i p(x_i) \\\\ &amp;= (1 \\times 0.1) + (2.0 \\times 0.4) + (3.0 \\times 0.5) \\\\ &amp;= 2.4 \\end{align} \\] Notice here that we have calculated the true value of a population parameter, yet we have not collected a single observation from an experiment. This is only possible because we were given the true probability distribution. When we are given exact probability distributions, then we can calculate population parameters exactly. When we are not given exact probability distributions, then we must estimate population parameters using sample statistics obtained by performing an experiment in which we sample from the random variable under consideration. Now suppose that we draw a the following sample of \\(n=10\\) from this distribution: ## [1] &quot;x:&quot; ## [1] 2 2 3 3 3 2 3 3 3 1 ## [1] &quot;sample mean:&quot; ## [1] 2.5 The sample mean of this sample is \\(\\bar{\\boldsymbol{x}} =\\) 2.5. Note that our sample mean is not equal to the population mean (it’s just a fluke if it is). In fact, every time we run this experiment, we will likely get a different sample mean. Lets run it 5 more times and check each one. ## [1] &quot;x:&quot; ## [1] 3 2 3 3 3 2 3 1 3 3 ## [1] &quot;sample mean:&quot; ## [1] 2.6 ## [1] &quot;x:&quot; ## [1] 2 2 2 3 1 3 3 2 1 2 ## [1] &quot;sample mean:&quot; ## [1] 2.1 ## [1] &quot;x:&quot; ## [1] 2 3 3 2 3 1 3 3 3 2 ## [1] &quot;sample mean:&quot; ## [1] 2.5 ## [1] &quot;x:&quot; ## [1] 2 2 3 1 1 3 3 3 2 3 ## [1] &quot;sample mean:&quot; ## [1] 2.3 ## [1] &quot;x:&quot; ## [1] 1 3 2 2 3 1 3 2 3 1 ## [1] &quot;sample mean:&quot; ## [1] 2.1 This indicates that the sample mean is different every time we run the experiment, and therefore, the sample mean is a random variable itself! This idea is very important to how we go about performing statistical inference, and we will treat it more formally in later lectures. For now, make sure you see clearly why and how we know it is a random variable 8.3.4 Population mean and variance for continuous \\(X\\) Let \\(X\\) be a continuous random variable. Let \\(\\boldsymbol{x} = \\{x_1, \\ldots, x_n\\}\\) be a sample from \\(X\\). Sample statistics are computed in exacrtly the same way regardless of whether \\(X\\) is continuous or discrete. Population parameters are again computed in terms of the expected value operator, but the way this operator works is a bit different depending on whether \\(X\\) is continuous or discrete. In particular, if \\(X\\) is continuous, then we will replace discrete sums (i.e., \\(\\sum x\\)) with continuous integrals (i.e., \\(\\int x dx\\)). 8.3.4.1 Continuous \\(X\\) mean \\[ \\begin{align} \\bar{\\boldsymbol{x}} &amp;= \\frac{1}{n} \\sum_{i=1}^{n} x_{i} \\\\ \\mathbb{E}\\big[\\boldsymbol{X}\\big] &amp;= \\mu \\\\ &amp;= \\int x f(x) dx \\\\ \\\\ \\hat{\\mu} &amp;= \\bar{\\boldsymbol{x}} \\end{align} \\] 8.3.4.2 Continuous \\(X\\) variance \\[ \\begin{align} \\boldsymbol{s}^2 &amp;= \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{\\boldsymbol{x}})^2 \\\\ \\mathbb{Var}\\big[\\boldsymbol{X}\\big] &amp;= \\sigma^2 \\\\ &amp;= \\mathbb{E}\\big[(X - \\mu)^2\\big] \\\\ &amp;= \\int(x^2 - 2x\\mu + \\mu^2) f(x) dx \\\\ &amp;= \\int x^2 f(x) dx - \\int 2 x \\mu f(x) dx + \\int \\mu^2 f(x) dx \\\\ &amp;= \\int x^2 f(x) dx - 2 \\mu \\int x f(x) dx + \\mu^2 \\int f(x) dx \\\\ &amp;= \\int x^2 f(x) dx - 2 \\mu^2 + \\mu^2 \\\\ &amp;= \\left(\\int x^2 fx \\right) - \\mu^2 \\\\ \\hat{\\sigma^2} &amp;= \\boldsymbol{s}^2 \\\\ \\end{align} \\] 8.3.5 Continuous \\(X\\) example As a concrete example, consider the following continuous probability distribution corresponding to the random variable \\(X\\). Notice that the y-axis is not labelled with \\(P(X=x)\\). This is because when \\(X\\) is continuous \\(P(X=x)=0\\) for all \\(x\\). The \\(f(x)\\) label refers to probability density. We will discuss this more formally in a later lecture. As for our discrete \\(X\\) example, we are explicitly given the probability distribution — i.e., we are told exactly what the probability density of each event is — so we can calculate the population mean \\(\\mu\\) as follows: \\[ \\begin{align} \\mu &amp;= E(\\boldsymbol{X}) \\\\ &amp;= \\int x p(x) dx \\\\ &amp;= 0 \\\\ \\end{align} \\] Note that you will not be expacted to evaluate difficult integrals in this unit. You will, however, need to understand the trick I used to evaluate this integral. The trick is see that the distribution is symmetric, and so the expected value is the peak value. Now suppose that we draw a the following sample of \\(n=10\\) from this distribution: ## [1] &quot;x:&quot; ## [1] 0.2885703 -1.6685417 0.8510622 0.2157761 -1.8147071 -0.1734513 ## [7] 0.9537678 0.7037876 -0.6312850 0.9075918 ## [1] &quot;sample mean:&quot; ## [1] -0.03674294 The sample mean of this sample is \\(\\bar{\\boldsymbol{x}} =\\) -0.0367429. As for our discrete \\(X\\) exmaple, we see that our sample mean is not equal to the population mean. Lets run it 5 more times to demonstrate again that the sample mean is indeed a ranomd variable. ## [1] &quot;x:&quot; ## [1] -0.3969633 0.3819590 0.9285327 0.3905761 -0.6527025 0.8199824 ## [7] 1.4774301 -0.9291964 -0.1821943 1.0036585 ## [1] &quot;sample mean:&quot; ## [1] 0.2841082 ## [1] &quot;x:&quot; ## [1] -0.85199189 -1.71937580 -0.49469904 1.06648978 -1.36184763 -0.24830383 ## [7] 0.96193008 0.04692745 0.56691905 1.62398016 ## [1] &quot;sample mean:&quot; ## [1] -0.04099717 ## [1] &quot;x:&quot; ## [1] 0.90077121 0.59119065 0.49098641 -1.15841660 0.83919689 0.54210662 ## [7] 0.40362073 0.05720034 -1.27104788 0.15359375 ## [1] &quot;sample mean:&quot; ## [1] 0.1549202 ## [1] &quot;x:&quot; ## [1] 1.25495610 -0.61171296 -1.80084925 1.83802787 -0.56427794 1.14035660 ## [7] 0.62405347 1.65773610 -0.04209058 -0.30044453 ## [1] &quot;sample mean:&quot; ## [1] 0.3195755 ## [1] &quot;x:&quot; ## [1] 1.7322580 2.1565298 0.3185189 0.1698470 0.7423495 -0.6725367 ## [7] 0.4978262 -0.9896120 -0.2974796 0.7948835 ## [1] &quot;sample mean:&quot; ## [1] 0.4452585 "],["bernoulli-random-variables.html", "9 Bernoulli random variables 9.1 Learning objectives 9.2 Bernoulli random variable", " 9 Bernoulli random variables 9.1 Learning objectives Define and understand Bernoulli random variables. 9.2 Bernoulli random variable 9.2.1 Definition A single sample from a any random variable that produces a dichotomous outcome is formally defined as a Bernoulli trial. To be a Bernoulli trial, a few conditions must be met: Each trial yields one of the two outcomes usually called success (\\(S\\)) and failure (\\(F\\)). For each trial, the probability of success \\(P(S)\\) is the same and is denoted by \\(p = P(S)\\). The probability of failure is then \\(q = P(F) = 1 - P(S)\\) for each trial. Trials are independent. The probability of success in a trial does not change given any amount of information about the outcomes of other trials. Bernoulli random variables are completely defined by the single parameter \\(p = P(S)\\). If \\(X\\) is a Bernoulli random variable then we would write: \\(X \\sim Bernoulli(p)\\) 9.2.2 Probability distribution The probability distribution of a Bernoulli random variable is very simple. We simply need to indicate two probabilities as follows. "],["binomial-random-variables.html", "10 Binomial random variables 10.1 Learning objectives 10.2 Binomial random variable", " 10 Binomial random variables 10.1 Learning objectives Define and understand Binomial random variables. 10.2 Binomial random variable 10.2.1 Definition Consider a fixed number \\(n\\) of Bernoulli trials conducted with success probability \\(P(S)=p\\) and failure probability \\(P(F)=q=1-p\\) in each trial. Define the random variable \\(X\\) as follows: \\(X =\\) the sum of the number of Successes from \\(n\\) of the above Bernoulli trials. Then \\(X\\) is called a binomial distribution with \\(n\\) trials and success probability \\(p\\). The binomial distribution is completely defined by two parameters, \\(n\\) and \\(p\\), and we can write: \\(X \\sim Binomial(n,p)\\) 10.2.2 Probability distribution 10.2.2.1 Population parameters: n=2, p=0.5 Define the random variable \\(X\\) as follows: \\(X\\) = the number of heads in \\(2\\) flips of a fair coin. Then \\(X \\sim Binomial(n=2, p=0.5)\\) To obtain the probability distribution, we must compute the probability of every possible outcome that \\(X\\) can produce. With \\(n=2\\) flips, it is easy to see that there are 3 possible outcomes (0, 1, or 2 heads) which are obtained with the following possible events and corresponding probabilities: \\[\\begin{align} P(X=0) &amp;= P(TT) \\\\ &amp;= P(T)P(T) \\\\ &amp;= 0.5 \\times 0.5 \\\\ &amp;= 0.25 \\\\\\\\ P(X=1) &amp;= P(TH) + P(HT) \\\\ &amp;= P(T)P(H) + P(H)P(T) \\\\ &amp;= 0.5 \\times 0.5 + 0.5 \\times 0.5 \\\\ &amp;= 0.25 + 0.25 \\\\ &amp;= 0.5 \\\\\\\\ P(X=2) &amp;= P(HH) \\\\ &amp;= P(H)P(H) \\\\ &amp;= 0.5 \\times 0.5 \\\\ &amp;= 0.25 \\end{align}\\] Above, we were able to write \\(P(TT)=P(T)P(T)\\) because coin flips are statistically independent. In simple cases like this it’s no big deal to compute the probabilities or all possible events by hand, but this approach quickly becomes intractable in real world situations. Luckily, R provides functions for essentially all the probability distributions you will likely ever find yourself using. In the case of a binomial distribution, we will use the dbinom() function as illustrated below. 10.2.2.2 Population parameters: function of n and p Now lets run through this exercise with different \\(n\\) and \\(p\\) values. Below, notice that when \\(p=0.1\\), the distribution is skewed to the left. This makes sense because if the probability of success is very small, then the probability of achieving zero or only few successes is high. Similarly, notice that when \\(p=0.9\\), the distribution is skewed to the right. The same logic applied in the previous sentence applies here. If the probability of success is very high, then we should expect many successes, and most of the probability will correspond to larger outcomes. "],["normal-distribution.html", "11 Normal Distribution 11.1 Learning objectives 11.2 Normal distribution", " 11 Normal Distribution 11.1 Learning objectives Define and understand Normal random variables. 11.2 Normal distribution 11.2.1 Definition The normal distribution is defined by two parameters: \\(\\mu\\): population mean \\(\\sigma\\): population standard deviation It is just a quirk of the normal distribution that the parameters that define it correspond to its mean and standard deviation. This will not generally be true for most other distributions that we will deal with. 11.2.2 Probability distribution The normal distribution is continuous, and the continuous probability distributions are called probability density functions (pdfs). The following equation defines the pdf of a normal distribution: \\[ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}{\\frac{(x - \\mu)^2}{\\sigma}}} \\] 11.2.2.1 Normal distribution: function of \\(\\mu\\) and \\(\\sigma\\) Lets have a look at how the normal distribution changes with different mean and variance. 11.2.2.2 The standard normal (Z) The standard normal distribution is a normal distribution with zero mean and unit variance (\\(\\mu=0\\), \\(\\sigma^2=1\\)). It is also called the \\(Z\\) distribution. It looks like this: "],["using-r-to-compute-probabilities.html", "12 Using R to compute probabilities", " 12 Using R to compute probabilities For most probability distributions, R has 4 built-in functions that tell you almost everything you will ever want to know about them. For the Binomial distribution, these functions are the following: dbinom(x): Probability mass function pbinom(x): Cumulative distribution function qbinom(p): quantile function rbinom(n): function for random samples For the Normal distributions, these functions are the following: dnorm(x): Probability density function pnorm(x): Cumulative distribution function qnorm(p): quantile function rnorm(n): function for random samples You can see the naming convention adopted by R right away. The d functions are mass or density functions. For discrete distributions, these functions return \\(P(X=x)\\). E.g., dbinom(x, n, p) returns the probability that a binomial random variable with parameters \\(n\\) and \\(p\\) will yield a value of \\(x\\). For continuous distributions, these functions return a probability density. To get probability, we must consider a range of outcomes \\([a, b]\\) and compute the area under the curve. Computing the exact area under the curve requires evaluating an integral, which is too hard for us and awkward to do using a d function. It will be best to use a p function in this case (see below). The p functions are cumulative probability functions. By default these functions return \\(P(X \\leq x)\\). If you specify lower.tail=FALSE then these functions return \\(P(X\\&gt;x)\\). Be careful when using these functions to appreciate that for continuous distributions \\(P(X \\leq x)=1-P(X \\geq x)\\) but for discrete distributions \\(P(X \\leq x)=1-P(X \\geq x+1)\\). All that is basically just to say be careful when considering whether to use greater than or greather than and equal to etc. The q functions are quantile functions. They are the inverse of the cumulative probability functions. Here, you specify a cumulative probability \\(q\\), and the function returns the value of \\(x\\) such that \\(P(X\\&lt;x)=q\\). The r functions generate random samples. pmf: probabilities are given by values on the y-axis. cdf: cumulative probabilities \\((X\\&lt;x)\\) are given by reading values on the y-axis. qf: use this function to specify a probability (x-axis) and get the value that satisfies this probability from the y-axis. pdf: probabilities are given by the area under the curve. cdf: cumulative probabilities \\((X\\&lt;x)\\) are given by reading values on the y-axis. qf: use this function to specify a probability (x-axis) and get the value that satisfies this probability from the y-axis. In general, I think it is important for you to be able to read each of the types of plots above, so please really try to encode these and think about them deeply. 12.0.0.1 Binomial example: \\(P(X \\&lt; 8)\\) n &lt;- 15 p &lt;- 0.5 ## Using pmf (exact) px &lt;- sum(dbinom(0:7, n, p)) px ## [1] 0.5 ## using cdf (exact) px &lt;- pbinom(7, n, p) px ## [1] 0.5 12.0.0.2 Normal example: \\(P(X \\&lt; 1)\\) mu &lt;- 0 sig &lt;- 1 ## Using pdf (not exact) px &lt;- sum(dnorm(seq(-5, 1, .1), mu, sig)*.1) px ## [1] 0.8532414 ## using cdf (exact) px &lt;- pnorm(1, mu, sig) px ## [1] 0.8413447 12.0.0.3 Binomial example: \\(P(X \\&gt; 9)\\) n &lt;- 15 p &lt;- 0.5 ## Using pmf (exact) px &lt;- sum(dbinom(10:n, n, p)) px ## [1] 0.1508789 ## using cdf (exact) px &lt;- pbinom(9, n, p, lower.tail=F) px ## [1] 0.1508789 12.0.0.4 Normal example: \\(P(X \\&gt; 1.8)\\) mu &lt;- 0 sig &lt;- 1 ## Using pdf (not exact) p &lt;- sum(dnorm(seq(1.8, 5, .1), mu, sig)*.1) p ## [1] 0.03999603 ## using cdf (exact) p &lt;- pnorm(1.8, mu, sig, lower.tail=F) p ## [1] 0.03593032 "],["distribution-of-sample-means.html", "13 Distribution of sample means 13.1 Learning objectives 13.2 Introduction", " 13 Distribution of sample means 13.1 Learning objectives Understand the concept of a sample mean as a random variable. Understand the concept of the distribution of sample means. 13.2 Introduction Every time we draw a sample from any random variable, we can compute the sample mean of that sample. In general, if we repeat that procedure many times, we will see that every time we draw a sample, we get different outcomes and therefore different sample means. This means that the sample mean is itself a random variable (i.e., because every time you measure it you get a different number). Lets examine this by considering samples from the Binomial random variable with distribution shown in the left column of the following, and samples from the Normal random variable with distribution shown on the right. More precisely, consider the following: \\(X \\sim \\mathcal{Binomial}(n=10, p=0.5)\\) ## sample_number sample_outcome sample_mean ## 1: sample_1 4 5 ## 2: sample_1 3 5 ## 3: sample_1 6 5 ## 4: sample_1 3 5 ## 5: sample_1 6 5 ## 6: sample_1 5 5 ## 7: sample_1 7 5 ## 8: sample_1 2 5 ## 9: sample_1 8 5 ## 10: sample_1 6 5 ## 11: sample_2 4 5 ## 12: sample_2 4 5 ## 13: sample_2 6 5 ## 14: sample_2 3 5 ## 15: sample_2 7 5 ## 16: sample_2 4 5 ## 17: sample_2 5 5 ## 18: sample_2 6 5 ## 19: sample_2 4 5 ## 20: sample_2 7 5 ## 21: sample_3 3 6 ## 22: sample_3 7 6 ## 23: sample_3 7 6 ## 24: sample_3 6 6 ## 25: sample_3 9 6 ## 26: sample_3 6 6 ## 27: sample_3 3 6 ## 28: sample_3 6 6 ## 29: sample_3 5 6 ## 30: sample_3 8 6 ## sample_number sample_outcome sample_mean \\(Y \\sim \\mathcal{N}({\\mu=5, \\sigma=2.5})\\) ## sample_number sample_outcome sample_mean ## 1: sample_1 1.07982714 5.128237 ## 2: sample_1 5.17417802 5.128237 ## 3: sample_1 4.23205034 5.128237 ## 4: sample_1 4.96986678 5.128237 ## 5: sample_1 12.23713598 5.128237 ## 6: sample_1 1.50329880 5.128237 ## 7: sample_1 6.08028350 5.128237 ## 8: sample_1 9.58164429 5.128237 ## 9: sample_1 3.47244364 5.128237 ## 10: sample_1 2.95164322 5.128237 ## 11: sample_2 5.12077365 3.982125 ## 12: sample_2 8.25137842 3.982125 ## 13: sample_2 4.14218790 3.982125 ## 14: sample_2 2.43552182 3.982125 ## 15: sample_2 5.17637135 3.982125 ## 16: sample_2 -0.04454817 3.982125 ## 17: sample_2 1.31136220 3.982125 ## 18: sample_2 7.71615700 3.982125 ## 19: sample_2 6.14703894 3.982125 ## 20: sample_2 -0.43499107 3.982125 ## 21: sample_3 6.54404064 4.965397 ## 22: sample_3 -0.76198838 4.965397 ## 23: sample_3 3.88257822 4.965397 ## 24: sample_3 5.74872669 4.965397 ## 25: sample_3 1.42881353 4.965397 ## 26: sample_3 8.16874371 4.965397 ## 27: sample_3 8.03626447 4.965397 ## 28: sample_3 3.31286017 4.965397 ## 29: sample_3 7.80255477 4.965397 ## 30: sample_3 5.49137444 4.965397 ## sample_number sample_outcome sample_mean We see that every time we sample from either random variable: We get a sample mean \\(\\bar{\\boldsymbol{x}}\\) that is close to the population mean \\(\\mu\\), but doesn’t match it exactly unless by dumb luck. Every sample from \\(X\\) leads to a different value for \\(\\bar{\\boldsymbol{x}}\\). Thus, we have verified that \\(\\bar{\\boldsymbol{x}}\\) must itself be a random variable. Moving forward, we will denote the random variable corresponding to the distribution of sample means with the symbol \\(\\bar{X}\\), and continue to use \\(\\bar{\\boldsymbol{x}}\\) to refer to a particular sample mean. \\[ \\begin{align} X &amp; \\rightarrow \\{x_{1}, \\ldots, x_{n}\\} \\\\ \\\\ \\bar{X} &amp; \\rightarrow \\frac{1}{n} \\{ x_{1} + \\ldots + x_{n} \\} \\end{align} \\] Notice that \\(n&gt;1\\) samples from \\(X\\) are needed to generate a single \\(n=1\\) sample from \\(\\bar{X}\\). This means that in order to estimate the distribution of sample means, we need to draw \\(n&gt;1\\) samples from \\(X\\) many times. If we perform an experiment in which we draw \\(n=10\\) samples from \\(X\\) or from \\(Y\\), compute the sample means for each (\\(\\bar{x}\\) and \\(\\bar{y}\\)), and repeat 500 times, then we get the following estimate for the distribution of sample means: Note that even though \\(X\\) is discrete, \\(\\bar{X}\\) is continuous. We also note that both \\(\\bar{X}\\) and \\(\\bar{Y}\\) look bell-shaped. This is because of something called the central limit theorem. Before we say more about this very important theorem, lets investigate how the original distributions (\\(X\\) and \\(Y\\)) compare to their corresponding distributions of sample means (\\(\\bar{X}\\) and \\(\\bar{Y}\\)) in terms of central tendancy and spread. We can see a few important things from these plots: The central tendancy (i.e., mean) of the distribution of sample means \\(\\bar{X}\\) looks to be about the same as that for the original distribution \\(X\\). The spread of the distribution of sample means \\(\\bar{X}\\) looks to be smaller than that for the original distribution \\(X\\). The central limit theorem helps us formalise both of these observations. "],["central-limit-theorem.html", "14 Central limit theorem 14.1 Learning objectives 14.2 Central limit theorem", " 14 Central limit theorem 14.1 Learning objectives Understand the central limit theorem and the distribution of sample means and how it depends on sample size. 14.2 Central limit theorem The central limit theorem tells us that the sum of independent and identically distributed random variables approximates a Normal distribution. Let \\(\\boldsymbol{Y} = \\boldsymbol{X}_1 + \\boldsymbol{X}_2 + \\dots + \\boldsymbol{X}_n\\) and \\(\\boldsymbol{X}_i \\sim \\boldsymbol{D}\\) where \\(\\boldsymbol{D}\\) can have any distribution whatsoever. Then, \\(\\boldsymbol{Y} \\sim \\mathcal{N}(\\mu_{Y}, \\sigma_{y}^2)\\) 14.2.1 Central limit theorem mean and variance One way to see where the mean and variance parameters come from in the central limit theorem is to use the following rule: Let \\(Y \\sim a X + b\\) Then: \\(\\mathbb{E}\\big[Y\\big] = a \\mathbb{E}\\big[X\\big] + E(b)\\) \\(\\mathbb{Var}\\big[Y\\big] = (a^2) \\mathbb{Var}\\big[X\\big]\\) 14.2.2 Applied to the distribution of sample sums \\[\\begin{align} \\boldsymbol{Y} &amp;= \\boldsymbol{X}_1 + \\boldsymbol{X}_2 + \\dots + \\boldsymbol{X}_n \\\\ \\\\ E(\\boldsymbol{Y}) &amp;= E(\\boldsymbol{X}_1 + \\boldsymbol{X}_2 + \\dots + \\boldsymbol{X}_n) \\\\ &amp;= E(\\boldsymbol{X}_1) + E(\\boldsymbol{X}_2) + \\dots + E(\\boldsymbol{X}_n) \\\\ &amp;= \\mu_x + \\mu_x + \\ldots + \\mu_x \\\\ &amp;= n \\mu_x \\\\ \\\\ Var(\\boldsymbol{Y}) &amp;= Var(\\boldsymbol{X}_1 + \\boldsymbol{X}_2 + \\dots + \\boldsymbol{X}_n) \\\\ &amp;= Var(\\boldsymbol{X}_1) + Var(\\boldsymbol{X}_2) + \\dots + Var(\\boldsymbol{X}_n) \\\\ &amp;= \\sigma_x^2 + \\sigma_x^2 + \\dots + \\sigma_x^2 \\\\ &amp;= n \\sigma_x^2 \\\\ \\end{align}\\] \\[\\begin{align} \\mu_{Y} &amp;= n \\mu_{X} \\\\ \\sigma_Y^2 &amp;= n \\sigma_X^2 \\\\ \\end{align}\\] 14.2.3 Applied to the distribution of sample means \\[\\begin{align} \\boldsymbol{Y} &amp;= \\frac{1}{n} (\\boldsymbol{X}_1 + \\boldsymbol{X}_2 + \\dots + \\boldsymbol{X}_n) \\\\ \\\\ E(\\boldsymbol{Y}) &amp;= E(\\frac{1}{n} (\\boldsymbol{X}_1 + \\boldsymbol{X}_2 + \\dots + \\boldsymbol{X}_n)) \\\\ &amp;= \\frac{1}{n} (E(\\boldsymbol{X}_1) + E(\\boldsymbol{X}_2) + \\dots + E(\\boldsymbol{X}_n)) \\\\ &amp;= \\frac{1}{n} (\\mu_x + \\mu_x + \\ldots + \\mu_x) \\\\ &amp;= \\mu_x \\\\ \\\\ Var(\\boldsymbol{Y}) &amp;= Var(\\frac{1}{n} (\\boldsymbol{X}_1 + \\boldsymbol{X}_2 + \\dots + \\boldsymbol{X}_n)) \\\\ &amp;= Var(\\frac{1}{n^2} (Var(\\boldsymbol{X}_1) + Var(\\boldsymbol{X}_2) + \\dots + Var(\\boldsymbol{X}_n)) \\\\ &amp;= \\frac{1}{n^2} ( \\sigma_x^2 + \\sigma_x^2 + \\dots + \\sigma_x^2 ) \\\\ &amp;= \\frac{1}{n} \\sigma_x^2 \\\\ \\end{align}\\] \\[\\begin{align} \\mu_{Y} &amp;= \\mu_{X} \\\\ \\sigma_Y^2 &amp;= \\frac{1}{n} \\sigma_X^2 \\\\ \\end{align}\\] 14.2.4 Standard Error When applying the central limit theorem to the distribution of sample means, we get: \\[\\begin{align} \\mu_{Y} &amp;= \\mu_{X} \\\\ \\sigma_Y^2 &amp;= \\frac{1}{n} \\sigma_X^2 \\\\ \\end{align}\\] When expressed in terms of standard deviation: \\[\\begin{align} \\mu_{Y} &amp;= \\mu_{X} \\\\ \\sigma_y &amp;= \\frac{1}{\\sqrt{n}} \\sigma_X \\\\ \\end{align}\\] Here, \\(\\sigma_Y\\) is called the standard error of the mean (SEM) and it is very commonly used to draw error bars on various plots. "],["hypothesis-testing.html", "15 Hypothesis Testing 15.1 Learning objectives 15.2 Null Hypothesis Significance Testing 15.3 Two-tailed tests introduction 15.4 Summary", " 15 Hypothesis Testing 15.1 Learning objectives Define and understand null hypothesis significance testing. Define and understand p-value. Define and understand more extreme outcomes in the context of hypothesis testing. Define and understand critical value. Define and understand rejection region. Define and understand \\(1-\\alpha\\%\\) confidence interval. Be able to perform hypothesis binomial tests and t-tests manually (i.e., run through the 5 steps). Understand when and how to use binom.test(). Understand when and how to use t.test(). 15.2 Null Hypothesis Significance Testing We will unpack each of these 5 steps in the examples that follow. They are listed here for reference. Specify the null and alternative hypotheses (\\(H_0\\) and \\(H_1\\)) in terms of a population parameter \\(\\theta\\). Specify the type I error rate – denoted by the symbol \\(\\alpha\\) – you are willing to tolerate. Specify the sample statistic \\(\\widehat{\\theta}\\) that you will use to estimate the population parameter \\(\\theta\\) in step 1 and state how it is distributed under the assumption that \\(H_0\\) is true. Obtain a random sample and use it to compute the sample statistic from step 3. Call this value \\(\\widehat{\\theta}_{\\text{obs}}\\). If \\(\\widehat{\\theta}_{\\text{obs}}\\) or a **more extreme outcome is very unlikely to occur under the assumption that \\(H_0\\) is true, then reject \\(H_0\\). Otherwise, do not reject \\(H_0\\). 15.3 Two-tailed tests introduction Last lecture, we learned how to perform one-tailed hypothesis tests. The hypothesis framing in those tests was as follows: \\[ H_0: \\theta = c \\\\ H_1: \\theta &lt; c \\\\ \\text{or} \\\\ H_0: \\theta = c \\\\ H_1: \\theta &gt; c \\] The hypotheses of a two-tailed test takes the following form: \\[ H_0: \\theta = c \\\\ H_1: \\theta \\neq c \\\\ H_1: \\theta &lt; c \\text{ or } \\theta &gt; c \\] As we will see in the following examples, the main difference between one-tailed and two-tailed tests is that the p-value gets contributions from both the lower and the upper tail of the sampling distribution. 15.4 Summary The p-value is the probability of the outcome or a more extreme outcome occurring under the assumption that \\(H_0\\) is true. What counts as a more extreme outcome is determined by \\(H_1\\). The critical value is the observed value for which more extreme outcomes would lead us to reject \\(H_0\\). The rejection region is the set of all outcomes that would lead us to reject \\(H_0\\). binom.test() can be used to efficiently perform a binomial test without manually labouring through the 5 steps. Must do everything long-form if dealing with a Normal sampling distribution (known variance). If the standard deviation of the original sampling distribution is not known, then it must be estimated, and the appropriate test to use is a t-test. A \\(t\\) distribution has higher tails than a \\(Z\\) distribution but is otherwise very similar. t.test() can be used to efficiently perform a t-test without manually labouring through the 5 steps. "],["inferential-error-and-statistical-power.html", "16 Inferential error and statistical power 16.1 Type I versus Type II errors 16.2 power", " 16 Inferential error and statistical power 16.1 Type I versus Type II errors 16.1.1 Definitions Hypothesis testing is about using noisy data to make decisions about what we think the true state of the universe is. Sometimes, our procedure for making these decisions will lead us to make the correct decision, but sometimes we will make the wrong decision. In the context of hypothesis testing, there are two ways to make a correct decision and two ways to make an incorrect decision. These are summarised in the following table. H0 True H1 True Reject H0 Type I error (\\(\\alpha\\)) Power (\\(1-\\beta\\)) Fail to reject H0 Confidence (\\(1-\\alpha\\)) Type II error (\\(\\beta\\)) Here, we have introduced the concept of power. Power is the probability of correctly rejecting \\(H_0\\). Power can only be computed with respect to some fully specified \\(H_1\\). This is best seen with a visualisation. Type I error is given by \\(\\alpha\\) The probability of a type I error is given by the area under the \\(H_0\\) curve in the rejection region. A type I error is the probability of incorrectly rejecting \\(H_0\\) Type I error is completely determined by \\(H_0\\) Type I error does not depend on \\(H_1\\) Power is given by \\(1 - \\beta\\) Power is given by the area under the \\(H_1\\) curve in the rejection region. Power can depends on both \\(H_0\\) and \\(H_1\\) Confidence is given by \\(1 - \\alpha\\) Confidence is given by the area under the \\(H_0\\) curve outside the rejection region. Confidence depends only on \\(H_0\\) Type II error is given by \\(\\beta\\) The probability of a type II error is given by the area under the \\(H_1\\) curve outside the rejection region. A type II error is the probability of incorrectly failing to reject \\(H_0\\) Type II error depends on both \\(H_0\\) and \\(H1\\) 16.2 power In general, we want as much power as possible. This is because, if \\(H_0\\) isn’t true, then we’d really like to reject it. How can we increase power? 16.2.1 Increase the distance between \\(H_0\\) and \\(H_1\\) The closer together the \\(H_0\\) and \\(H_1\\) distributions, the less power we have. 16.2.2 Decrease the variance of \\(H_0\\) and \\(H_1\\) The smaller the variance of \\(H_0\\) and \\(H_1\\), the greater power we have 16.2.3 Power as a function of sample size The key point here is that, when dealing with the distribution of sample means, variance is inversely proportional to sample size. \\(\\sigma_\\bar{X} = \\frac{\\sigma_X}{\\sqrt{n}}\\) This means that power increases with increased sample size. "],["confidence-intervals.html", "17 Confidence intervals 17.1 Confidence intervals for two-tailed tests 17.2 Confidence intervals for one-tailed tests", " 17 Confidence intervals 17.1 Confidence intervals for two-tailed tests Way back in the beginning of the class we covered descriptive statistics, and more recently we have used some of these descriptive statistics as estimates of population parameters. For example, we have used the sample mean \\(\\bar{x}\\) as an estimate of the population mean \\(\\mu_X\\). This is an example of a point estimate of a population parameter, and it represents our single best guess about the true value of the parameter in question. An alternative approach to point estimation is to instead try to estimate a range of values in which the true value of the parameter is likely to reside. This is called interval estimation and it is accomplished by constructing a confidence interval. It is simplest to conceive of confidence intervals in the context of two-tailed tests, so what follows is for that situation. To construct a confidence interval for the population mean \\(\\mu_X\\) (assuming the population variance \\(\\sigma_X^2\\) is known), first compute the sample mean \\(\\bar{x}\\). Next, the width of the CI is the same as the width of the distribution of sample means required to cover \\(1-\\alpha\\) percent of it’s probability. Call this width \\(w\\). Then the confidence interval estimate of \\(\\mu_X\\) is given by \\(CI = \\left[ \\bar{x}-\\frac{w}{2} \\text{, } \\bar{x}+\\frac{w}{2} \\right]\\) In the above plot, red vertical lines mark the confidence interval and black vertical lines mark the critical values. CI is always centred around the observed value. Critical values are always centred around the mean of the null sampling distribution. This can all be written concisely as follows: \\[ CI_{1-\\alpha} = \\left[ \\bar{x} - \\Phi_{\\bar{X}}^{-1}(1-\\frac{\\alpha}{2}), \\text{ } \\bar{x} + \\Phi_{\\bar{X}}^{-1}(1-\\frac{\\alpha}{2}) \\right] \\] For example, the 95% confidence interval for the population mean of \\(X \\sim N(\\mu_X, \\sigma_X)\\) is given by: \\[\\begin{align} CI_{1-0.05} &amp;= \\left[ \\bar{x} - \\Phi_{X}^{-1}(1-\\frac{0.05}{2}), \\text{ } \\bar{x} + \\Phi_{X}^{-1}(1-\\frac{0.05}{2}) \\right] \\\\ \\\\ CI_{.95} &amp;= \\left[ \\bar{x} - \\Phi_{X}^{-1}(1-0.025), \\text{ } \\bar{x} + \\Phi_{X}^{-1}(1-0.025) \\right] \\\\ \\\\ &amp;= \\left[ \\bar{x} - \\Phi_{X}^{-1}(0.975), \\text{ } \\bar{x} + \\Phi_{X}^{-1}(0.975) \\right] \\end{align}\\] Notice that just like the point estimate \\(\\bar{x}\\), a confidence interval is also a random variable, and so changes with every experiment performed. We can demonstrate this by repeating the procedure and observing that the obtained confidence intervals are different with each repetition. Common misconceptions about confidence intervals: There is a 95% chance that the true population mean falls within the confidence interval. The mean will fall within the confidence interval 95% of the time. Neither of these statements is true. Rather, a 95% level of confidence means that 95% of the confidence intervals calculated from a set of random samples will contain the true population mean. For example, if you repeated your study 100 times, we would expect that 95 out of 100 of these studies to produce confidence intervals that contain the true population mean. 17.2 Confidence intervals for one-tailed tests In the context of a one-tailed test, confidence intervals always take the form \\((a, \\infty)\\) for a greater than test direction or \\((\\infty, a)\\) for a less than test direction. This is like saying that with a greater than test we really only care about a lower bound on the interval beyond which we would reject the null. A similar logic holds for less than tests. To compute \\(a\\) we follow much the same procedure as with two-tailed tests, but since it’s a one-tailed test, we use all of \\(\\alpha\\) when computing the finite boundary. CI for greater than: \\[ CI_{1-\\alpha} = \\left[ \\bar{x} - \\Phi_{\\bar{X}}^{-1}(1-\\alpha), \\text{ } \\infty \\right] \\] CI for less than: \\[ CI_{1-\\alpha} = \\left[ \\infty, \\text{ } \\bar{x} - \\Phi_{\\bar{X}}^{-1}(1-\\alpha) \\right] \\] "],["normal-test.html", "18 Normal test 18.1 Two-tailed Normal test", " 18 Normal test Consider an experiment in which a rat is placed into a maze and given the chance to search for a bit of cheese hidden somewhere in the maze. After much training, the researchers are interested in assessing whether or not the animal has learned where the cheese is hidden. The researchers also know that rats without any training whatsoever find the cheese on average in 90 seconds with a standard deviation of 20 seconds. They perform 15 trials and measure the time to cheese on each trial. The data are as follows: 1. Specify the null and alternative hypotheses (\\(H_0\\) and \\(H_1\\)) in terms of a distribution and population parameter. If the rat has learned something about where to find the cheese, then we expect it’s time to be less than that of naive rats, which we are told is 90 seconds. This leads to the following hypotheses. \\[ H_0: \\mu = 90 \\\\ H_1: \\mu &lt; 90 \\] 2. Specify the type I error rate – denoted by the symbol \\(\\alpha\\) – you are willing to tolerate. \\[ \\alpha = 0.05 \\] 3. Specify the sample statistic that you will use to estimate the population parameter in step 1 and state how it is distributed under the assumption that \\(H_0\\) is true. \\[ \\widehat{\\mu} = \\bar{x} \\\\ \\bar{x} \\sim \\mathcal{N}(\\mu_{\\bar{x}}, \\sigma_{\\bar{x}}) \\] Since we are the sample mean \\(\\bar{x}\\) to estimate \\(\\mu\\), the sampling distribution of our test statistic is the distribution of sample means. This is great news because we know from previous lectures how the mean and variance of the distribution of sample means \\(\\bar{x}\\) relates to the mean and variance of our origin distribution \\(x\\). In particular, we know: \\[ \\mu_{\\bar{x}} = \\mu_{x} \\\\ \\sigma_{\\bar{x}} = \\frac{\\sigma_{x}}{\\sqrt{n}} \\] We can now inspect the sampling distribution under the assumption that \\(H_0\\) is true, and inspect how likely the observed \\(\\bar{x}\\) is to be sampled from this distribution. 4. Obtain a random sample and use it to compute the sample statistic from step 3. Call this value \\(\\widehat{\\theta}_{\\text{obs}}\\) In this example, \\(\\widehat{\\theta}_{\\text{obs}}\\) corresponds to the observed sample mean. The data, sampled from the random variable \\(X\\) is given by the following times: x_obs ## [1] 105.25909 73.47533 106.59599 105.44859 88.29283 49.20100 61.42866 ## [8] 74.10559 79.88466 128.09307 95.27187 64.01982 57.04686 74.21077 ## [15] 74.01570 Thus, the observed sample mean \\(\\bar{x}_{\\text{obs}}\\), sampled from the random variable \\(\\bar{X}\\), is obtained as follows: x_bar_obs &lt;- mean(x_obs) Therefore, \\(\\bar{x}_{\\text{obs}}=\\) 82.4233202 5. If \\(\\widehat{\\theta}_{\\text{obs}}\\) is very unlikely to occur under the assumption that \\(H_0\\) is true, then reject \\(H_0\\). Otherwise, do not reject \\(H_0\\). When computing the p-value, we will turn to pnorm(). From the plot above, and from reasoning about the alternative hypothesis, we see that we need lower.tail=TRUE. mu_x &lt;- 90 sig_x &lt;- 20 mu_x_bar &lt;- mu_x sig_x_bar &lt;- sig_x / sqrt(n) ## p-value pval &lt;- pnorm(x_bar_obs, mu_x_bar, sig_x_bar, lower.tail=TRUE) pval ## [1] 0.07115842 ## critical value x_bar_crit &lt;- qnorm(0.05, mu_x_bar, sig_x_bar, lower.tail=TRUE) x_bar_crit ## [1] 81.50601 It is easy to decide whether or not to reject \\(H_0\\) based on the p-value or the critical region, but it sure would be nice if R gave us a one liner like binom.test(). Unfortunately, in the case of a normal sampling distribution, no such R function exists. The reason for this is that to have a normal \\(\\bar{x}\\) sampling distribution, you have to know both the mean and the variance of the \\(H_0\\) distribution. The mean is specified by \\(H_0\\) so is no issue, but we rarely are in a situation to know the population variance of X, and we therefore have to estimate it. This leads us to the famous t-test. 18.1 Two-tailed Normal test Last lecture we considered an experiment in which a rat is placed into a maze and given the chance to search for a bit of cheese hidden somewhere in the maze. The researchers know that rats without any training whatsoever find the cheese on average in 90 seconds with a standard deviation of 20 seconds. After much training, the researchers are interested in assessing whether or not the animal has learned where the cheese is hidden or on the contrary, if the animal has become frustrated and is taking longer than baseline. 1. Specify the null and alternative hypotheses (\\(H_0\\) and \\(H_1\\)) in terms of a distribution and population parameter. \\[ H_0: \\mu = 90 \\\\ H_1: \\mu \\neq 90 \\] 2. Specify the type I error rate – denoted by the symbol \\(\\alpha\\) – you are willing to tolerate. \\[ \\alpha = 0.05 \\] 3. Specify the sample statistic that you will use to estimate the population parameter in step 1 and state how it is distributed under the assumption that \\(H_0\\) is true. \\[ \\widehat{\\mu} = \\bar{x} \\\\ \\bar{x} \\sim \\mathcal{N}(\\mu_{\\bar{x}}, \\sigma_{\\bar{x}}) \\\\ \\mu_{\\bar{x}} = \\mu_{x} \\\\ \\sigma_{\\bar{x}} = \\frac{\\sigma_{x}}{\\sqrt{n}} \\] 4. Obtain a random sample and use it to compute the sample statistic from step 3. Call this value \\(\\widehat{\\theta}_{\\text{obs}}\\) The researchers perform 15 trials and measure the time to cheese on each trial. The data are as follows: xobs &lt;- c(105.25909, 73.47533, 106.59599, 105.44859, 88.29283, 49.20100, 61.42866, 74.10559, 79.88466, 128.09307, 95.27187 ,64.01982 ,57.04686 ,74.21077, 74.01570) xbarobs &lt;- mean(xobs) 5. If \\(\\widehat{\\theta}_{\\text{obs}}\\) is very unlikely to occur under the assumption that \\(H_0\\) is true, then reject \\(H_0\\). Otherwise, do not reject \\(H_0\\). n &lt;- 15 mux &lt;-90 sigx &lt;- 20 muxbar &lt;- 90 sigxbar &lt;- sigx / sqrt(n) xbarobs_upper &lt;- -(xbarobs - mux) + mux xbarobs_lower &lt;- xbarobs xbar_crit_upper &lt;- qnorm(0.05/2, muxbar, sigxbar, lower.tail=FALSE) xbar_crit_lower &lt;- qnorm(0.05/2, muxbar, sigxbar, lower.tail=TRUE) # compute p-value by hand pval_upper &lt;- pnorm(xbarobs_upper, muxbar, sigxbar, lower.tail=FALSE) pval_lower &lt;- pnorm(xbarobs_lower, muxbar, sigxbar, lower.tail=TRUE) pval &lt;- pval_upper + pval_lower pval ## [1] 0.1423169 "],["z-test.html", "19 Z-test", " 19 Z-test Why is the \\(Z\\) distribution so important? To be candid, for the privileged R users of the world, it really isn’t that essential to our day to day statistical adventures. This is because, if we want to know what \\(P(X &lt; x)\\) for \\(X \\sim N(\\mu_X, \\sigma_x)\\) then we just type pnorm(x, mu_x, sigma_x, lower.tail=TRUE) and let R do all the hard work. But what is R doing under the hood? Recall that \\(P(X &lt; x)\\) for \\(X \\sim N(\\mu_X, \\sigma_x)\\) corresponds to the area under the probability density function in the interval \\([-\\infty, x]\\). Further recall that the equation that describes the PDF of a normal distribution is given by: \\[ f(x) = \\frac{1}{\\sigma_X \\sqrt(2 \\pi)} e^{ -\\frac{1}{2} (\\frac{x - \\mu_X}{\\sigma_X})^2 } \\] This means that \\(P(X &lt; x)\\) is given by: \\[ P(X &lt; a) = \\int_{\\infty}^{a} \\frac{1}{\\sigma_X \\sqrt(2 \\pi)} e^{ -\\frac{1}{2} (\\frac{x - \\mu_X}{\\sigma_X})^2 } \\] It turns out that without awesome computer programs like R, evaluating this integral is challenging. The old school solution to this challenge was to evaluate this integral for the standard normal (\\(Z\\)) and put that solution in the back of standard statistics text books in the form of big giant tables. The \\(Z\\) distribution has other uses in statistics and machine learning, but most of them are beyond the scope of this unit. We will see that the overarching idea of the z-transform – i.e., standardising data to mean zero and variance one – comes up when we get to the t-test (later this lecture!). mu_x &lt;- 90 mu_x_bar &lt;- mu_x sig_x &lt;- 10 n &lt;- 100 x_obs &lt;- rnorm(n, mu_x, sig_x) # Using Normal distribution x_bar_obs &lt;- mean(x_obs) sig_x_bar &lt;- sig_x / sqrt(n) px &lt;- pnorm(x_bar_obs, mu_x_bar, sig_x_bar, lower.tail=TRUE) ## Using Z distribution z_obs &lt;- (x_bar_obs - 90) / sig_x_bar pz &lt;- pnorm(z_obs, 0, 1, lower.tail=TRUE) ## Compare p-values from different methods px ## [1] 0.4145534 pz ## [1] 0.4145534 How are the p-values the same? The plot below is the X distribution on the left and the Z distribution on the right. - Notice that the Z is just a scaled version of X, and the that scaling also applied to the observed value. This means that all probabilities (area under the curve) are the same. "],["t-test.html", "20 t-test 20.1 Two-tailed t-test", " 20 t-test So far, everything that we have done we have been lucky enough to know both the mean and the variance of the sampling distribution in our hypothesis tests. The mean is specified in \\(H_0\\) and \\(H_1\\) and the variance has either fallen out luckily (e.g., as with the Binomial test), or I have just given you a number and told you pretend that we just know it to be true (e.g., previous cheese maze example). Of course, in most real world scenarios, we will not know the variance of the sampling distribution, and this means that the approaches we have developed so far aren’t quite appropriate. Here is what we do instead: Let \\(X_1, X_2, \\ldots, X_n\\) be independent and identically distributed as \\[ X_i \\sim N(\\mu_X, \\sigma_X) \\] and define two random variables \\(\\bar{X}\\) and \\(S^2\\) as \\[ \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i \\] \\[ S^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2 \\] then the random variable \\[ \\frac{\\bar{X} - \\mu_X}{\\frac{\\sigma_X}{\\sqrt{n}}} \\sim N(0, 1) = Z \\] and \\[ \\frac{\\bar{X} - \\mu_X}{\\frac{S}{\\sqrt{n}}} \\sim t(n-1) \\] where \\(t\\) is a t-distribution, which is completely defined by one parameter called the degrees of freedom given by \\(n-1\\). This all means that the mathematical formulation for how our sampling distribution is defined is different depending on whether or not we know \\(\\sigma_X\\). Lets examine how this pans out using our previous cheese example, but without assuming known variance. 1. Specify the null and alternative hypotheses (\\(H_0\\) and \\(H_1\\)) in terms of a distribution and population parameter. \\[ H_0: \\mu = 90 \\\\ H_1: \\mu &lt; 90 \\] 2. Specify the type I error rate – denoted by the symbol \\(\\alpha\\) – you are willing to tolerate. \\[ \\alpha = 0.05 \\] 3. Specify the sample statistic that you will use to estimate the population parameter in step 1 and state how it is distributed under the assumption that \\(H_0\\) is true. In this example we do not know \\(\\sigma_{x}\\), and so we must estimate it. This means that we do not want to reason using the observed \\(\\bar{x}\\) value and corresponding sampling distribution, but instead want to reason using an observed \\(t\\) value and corresponding t-distribution. \\[ t_{obs} = \\frac{\\bar{x} - \\mu_x}{\\frac{s_x}{\\sqrt{n}}} \\sim t(n-1) \\] 4. Obtain a random sample and use it to compute the sample statistic from step 3. Call this value \\(\\widehat{\\theta}_{\\text{obs}}\\) For our data, the following is true: \\(n=15\\), \\(\\bar{x} =\\) 90.1349684, \\(s_x =\\) 9.5456853 \\(t_{obs} =\\) 0.141392. The above plot shows the \\(t(n-1)\\) sampling distribution in colour, and the \\(Z\\sim\\mathcal{N}(0,1)\\) in black. The \\(t\\) has higher tails than the \\(Z\\). This is because the t-value is the result of two random variables (sample mean and sample variance), while the z-value is only a product of only one random variable (the sample mean). However, it is easy to see that the difference between \\(t\\) and \\(Z\\) is reduces as \\(n\\) increases. 5. If \\(\\widehat{\\theta}_{\\text{obs}}\\) is very unlikely to occur under the assumption that \\(H_0\\) is true, then reject \\(H_0\\). Otherwise, do not reject \\(H_0\\). When computing the p-value, we will turn to pt(). From the plot above, and from reasoning about the alternative hypothesis, we see that we need lower.tail=TRUE. ## p-value p_val &lt;- pt(t_obs, n-1, lower.tail=TRUE) p_val ## [1] 0.5560762 ## critical value t_crit &lt;- qt(0.05, n-1, lower.tail=TRUE) t_crit ## [1] -1.660391 Finally, there is a built in function called t.test() that will do all of this for you. t.test(x_obs, mu=90, alternative=&#39;less&#39;) ## ## One Sample t-test ## ## data: x_obs ## t = 0.14139, df = 99, p-value = 0.5561 ## alternative hypothesis: true mean is less than 90 ## 95 percent confidence interval: ## -Inf 91.71993 ## sample estimates: ## mean of x ## 90.13497 20.1 Two-tailed t-test t-test arises from the Normal test scenarios in which the sample variance \\(\\sigma_X\\) is unknown. 1. Specify the null and alternative hypotheses (\\(H_0\\) and \\(H_1\\)) in terms of a distribution and population parameter. \\[ H_0: \\mu = 90 \\\\ H_1: \\mu \\neq 90 \\] 2. Specify the type I error rate – denoted by the symbol \\(\\alpha\\) – you are willing to tolerate. \\[ \\alpha = 0.05 \\] 3. Specify the sample statistic that you will use to estimate the population parameter in step 1 and state how it is distributed under the assumption that \\(H_0\\) is true. \\[ \\widehat{\\mu} = \\bar{x} \\\\ \\bar{x} \\sim \\mathcal{N}(\\mu_{\\bar{x}}, \\sigma_{\\bar{x}}) \\\\ \\mu_{\\bar{x}} = \\mu_{x} \\\\ \\sigma_{\\bar{x}} = \\frac{\\sigma_{x}}{\\sqrt{n}} \\rightarrow \\widehat{\\sigma}_{\\bar{x}} = \\frac{s_{x}}{\\sqrt{n}} \\\\ t = \\frac{\\bar{x} - \\mu_x}{\\frac{s_x}{\\sqrt{n}}} \\sim t(n-1) \\] 4. Obtain a random sample and use it to compute the sample statistic from step 3. Call this value \\(\\widehat{\\theta}_{\\text{obs}}\\) The researchers perform 15 trials and measure the time to cheese on each trial. The data are as follows: xobs &lt;- c(105.25909, 73.47533, 106.59599, 105.44859, 88.29283, 49.20100, 61.42866, 74.10559, 79.88466, 128.09307, 95.27187 ,64.01982 ,57.04686 ,74.21077, 74.01570) n &lt;- length(xobs) xbarobs &lt;- mean(xobs) sigxbarobs &lt;- sd(xobs) / sqrt(n) mux &lt;-90 muxbar &lt;- 90 tobs &lt;- (xbarobs - muxbar) / sigxbarobs 5. If \\(\\widehat{\\theta}_{\\text{obs}}\\) is very unlikely to occur under the assumption that \\(H_0\\) is true, then reject \\(H_0\\). Otherwise, do not reject \\(H_0\\). tobs_upper &lt;- -tobs tobs_lower &lt;- tobs t_crit_upper &lt;- qt(0.05/2, n-1, lower.tail=FALSE) t_crit_lower &lt;- qt(0.05/2, n-1, lower.tail=TRUE) # compute p-value by hand pval_upper &lt;- pt(tobs_upper, n-1, lower.tail=FALSE) pval_lower &lt;- pt(tobs_lower, n-1, lower.tail=TRUE) pval &lt;- pval_upper + pval_lower pval ## [1] 0.2024876 # check with t.test t.test(xobs, mu=mux, alternative=&#39;two.sided&#39;) ## ## One Sample t-test ## ## data: xobs ## t = -1.3372, df = 14, p-value = 0.2025 ## alternative hypothesis: true mean is not equal to 90 ## 95 percent confidence interval: ## 70.27055 94.57609 ## sample estimates: ## mean of x ## 82.42332 "],["independent-samples-t-test.html", "21 Independent samples t-test", " 21 Independent samples t-test Thus far we have learned how to test hypotheses that ask if a sample of data were likely generated by a random variable with a mean of some given value. For example, given sample \\(x_1 \\ldots x_n\\) from a random variable \\(X\\), do you believe that \\(\\mu_X &gt; c\\) where \\(c\\) is some number. This situation will occur in your research career numerous times, so learning how to do this will easily prove to be a wise investment. However, in many circumstances, we will want to test hypotheses that ask if two samples were likely obtained by different random variables or not. That is, we will want to compare two treatments. The good news is that the fundamental logic of NHST that we have already established still works perfectly in this scenario. As an example, lets return to the criterion learning data we have discussed numerous times in lecture and homework. library(data.table) library(ggplot2) rm(list = ls()) d &lt;- fread( &#39;https://crossley.github.io/book_stats/data/criterion_learning/crit_learn.csv&#39;) d &lt;- d[cnd %in% c(&#39;Delay&#39;, &#39;Short ITI&#39;, &#39;Long ITI&#39;)] dd &lt;- d[order(cnd, sub), mean(t2c), .(cnd, sub)] ggplot(dd, aes(cnd, V1)) + geom_violin() + ylab(&#39;Mean Trials to Criterion&#39;) Consider two random variables \\(X \\sim \\mathcal{N}(\\mu_X, \\sigma_X)\\) and \\(Y \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y)\\). Let \\(X\\) generate data for the Delay condition and \\(Y\\) generate data for the Long ITI condition. Furthermore, lets assume that \\(X\\) and \\(Y\\) are independent. This means that given a sample from \\(X\\), you don’t change your beliefs about \\(Y\\) in any way, and vice versa. Is it realistic to assume that \\(X\\) and \\(Y\\) are independent? Since different subjects are in each condition (i.e., this is a between-subjects design), we can assume independence. We’d need to rethink our assumptions if the same subjects were involved in sampling from both \\(X\\) and \\(Y\\). This particular experiment was designed to answer a simple question: Does a feedback delay cause the mean trials to criterion to increase relative to a control condition where feedback is given immediately and a delay is put in the inter-trial interval. 1. Specify the null and alternative hypotheses (\\(H_0\\) and \\(H_1\\)) in terms of a distribution and population parameter. \\[ H_0: \\mu_X = \\mu_Y \\\\ H_1: \\mu_X &gt; \\mu_Y \\] 2. Specify the type I error rate – denoted by the symbol \\(\\alpha\\) – you are willing to tolerate. \\[\\alpha = 0.05\\] 3. Specify the sample statistic that you will use to estimate the population parameter in step 1 and state how it is distributed under the assumption that \\(H_0\\) is true. Hmm. Here things seem to get a bit tricky. How do we estimate \\(\\mu_X = \\mu_Y\\)? The first step is to rewrite our hypotheses such that all parameters are on the same side of the equation. \\[ H_0: \\mu_X - \\mu_Y = 0 \\\\ H_1: \\mu_X - \\mu_Y &gt; 0 \\] The next step is define a third random variable \\(W = X - Y\\). With this definition, we know: \\[ W \\sim \\mathcal{N}(\\mu_W, \\sigma_W) \\\\ \\mu_W = \\mu_X - \\mu_Y \\\\ \\sigma^2_W = \\sigma^2_X + \\sigma^2_Y \\] Now we can rewrite our hypotheses as: \\[ H_0: \\mu_W = 0 \\\\ H_1: \\mu_W &gt; 0 \\] Finally, we can actually get to business as usual for step 3: \\[ \\begin{align} \\widehat{\\mu_W} &amp;= \\overline{W} \\sim \\mathcal{N}(\\mu_\\overline{W}, \\sigma_\\overline{W}) \\\\\\\\ \\mu_\\overline{W} &amp;= \\mu_\\overline{X} - \\mu_\\overline{Y} \\\\ &amp;= \\mu_X - \\mu_Y \\\\\\\\ \\sigma^2_\\overline{W} &amp;= \\sigma^2_\\overline{X} + \\sigma^2_\\overline{Y} \\\\ &amp;= \\frac{\\sigma^2_X}{n} + \\frac{\\sigma^2_Y}{n} \\end{align} \\] We know that \\(\\sigma^2_W = \\sigma^2_X + \\sigma^2_Y\\), so if we somehow now \\(\\sigma^2_X\\) and \\(\\sigma^2_Y\\), then we can just plug those in a proceed with Normal distributions. Much more commonly, we will not know \\(\\sigma^2_X\\) and \\(\\sigma^2_Y\\), so we will have to estimate them, and because of this, \\(\\hat{\\mu_W}\\) will have a \\(t\\) distribution, not a Normal distribution. To figure out exactly how this t-distribution is configured, we need to figure out (1) how to estimate \\(\\sigma_\\overline{W}\\), and (2) the degrees of freedom of the resulting \\(t\\) distribution. 3. Specify the sample statistic that you will use to estimate the population parameter in step 1 and state how it is distributed under the assumption that \\(H_0\\) is true. 21.0.1 Equal sample size and equal variance \\[ \\begin{align} s^2_\\overline{W} &amp;= s^2_\\overline{X} + s^2_\\overline{Y} \\\\ &amp;= \\frac{s^2_X}{n_X} + \\frac{s^2_Y}{n_Y} \\\\ &amp;= \\frac{s^2_X}{n} + \\frac{s^2_Y}{n} \\\\ &amp;= \\frac{s^2_X + s^2_Y}{n} \\\\\\\\ s_\\overline{W} &amp;= \\sqrt{\\frac{s^2_X + s^2_Y}{n}} \\end{align} \\] Here, it is common to define a variable called the pooled variance denoted by \\(s_p\\) and defined by: \\[ s^2_p = \\frac{S^2_X + S^2_Y}{2} \\] That is, in this case, the pooled variance is just the average variance over each component. In any case, we can define our test statistic and sampling distribution as follows: \\[ \\begin{align} t_{obs} &amp;= \\frac{\\overline{W} - \\mu_\\overline{W}}{s_\\overline{W}} \\\\ &amp;= \\frac{\\overline{X} - \\overline{Y}}{\\sqrt{\\frac{s^2_X + s^2_Y}{n}}} \\\\\\\\ t_{obs} &amp;\\sim t(df) \\\\\\\\ df &amp;= (n_X-1) + (n_Y-1) \\\\ &amp;= 2n - 2 \\end{align} \\] Alternatively, you can write this using the pooled variance as follows: \\[ \\begin{align} t_{obs} &amp;= \\frac{\\overline{W} - \\mu_\\overline{W}}{s_\\overline{W}} \\\\ &amp;= \\frac{\\overline{X} - \\overline{Y}}{s_p \\sqrt{\\frac{2}{n}}} \\\\\\\\ t_{obs} &amp;\\sim t(df) \\\\\\\\ df &amp;= (n_X-1) + (n_Y-1) \\\\ &amp;= 2n - 2 \\end{align} \\] 21.0.2 Unequal sample sizes and equal variances With unequal sample sizes, the pooled variance is weighted more towards the sample with the larger size: \\[ s^2_p = \\frac{(n_X-1)s^2_X + (n_Y-1)s^2_Y}{n_X + n_Y - 2} \\] The rest works out almost the same as above: \\[ \\begin{align} t_{obs} &amp;= \\frac{\\overline{W} - \\mu_\\overline{W}}{s_\\overline{W}} \\\\ &amp;= \\frac{\\overline{X} - \\overline{Y}}{s_p \\sqrt{\\frac{1}{n_X} + \\frac{1}{n_Y}}} \\\\\\\\ t_{obs} &amp;\\sim t(df) \\\\\\\\ df &amp;= (n_X-1) + (n_Y-1) \\\\ &amp;= n_X + n_Y - 2 \\end{align} \\] 21.0.3 Unequal sample size and unequal variance NOTE: This works for equal sample size as well. It turns out that the expression for our observed t-value and estimated variance is quite simple, but the degrees of freedom for the test looks a bit more complicated. \\[ \\begin{align} t_{obs} &amp;= \\frac{\\overline{X} - \\overline{Y}}{\\sqrt{\\frac{s^2_X}{n_X} + \\frac{s^2_Y}{n_Y}}}\\\\\\\\ df &amp;= \\frac{\\left( \\frac{s^2_X}{n_X} + \\frac{s^2_Y}{n_Y} \\right)^2} {\\frac{\\left(\\frac{s^2_X}{n_X} \\right)^2}{n_X-1} + \\frac{\\left( \\frac{s^2_Y}{n_Y} \\right)^2}{n_Y-1}} \\end{align} \\] This expression for \\(df\\) is known as the Welch–Satterthwaite equation. 4. Obtain a random sample and use it to compute the sample statistic from step 3. Call this value \\(\\widehat{\\theta}_{\\text{obs}}\\) x &lt;- dd[cnd==&#39;Delay&#39;, V1] y &lt;- dd[cnd==&#39;Long ITI&#39;, V1] nx &lt;- length(x) ny &lt;- length(y) xbar &lt;- mean(x) ybar &lt;- mean(y) varx &lt;- var(x) vary &lt;- var(y) ## we have almost equal n, and lets assume equal variance s &lt;- sqrt((varx + vary)/nx) tobs &lt;- (xbar - ybar) / s df &lt;- nx + ny - 2 5. If \\(\\widehat{\\theta}_{\\text{obs}}\\) is very unlikely to occur under the assumption that \\(H_0\\) is true, then reject \\(H_0\\). Otherwise, do not reject \\(H_0\\). pval &lt;- pt(tobs, df, lower.tail=F) if(pval &lt; 0.05) { print(&#39;reject the null&#39;) print(tobs) print(df) print(pval) } else { print(&#39;fail to reject the null&#39;) print(tobs) print(df) print(pval) } ## [1] &quot;fail to reject the null&quot; ## [1] 0.876303 ## [1] 39 ## [1] 0.1931156 ## Check our work using built in t.test() t.test(x=x, y=y, alternative=&#39;greater&#39;, mu=0, paired=FALSE, var.equal=TRUE, conf.leve=0.95) ## ## Two Sample t-test ## ## data: x and y ## t = 0.88703, df = 39, p-value = 0.1902 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -30.40335 Inf ## sample estimates: ## mean of x mean of y ## 140.9593 107.1572 Our results are very close to the results from t.test() but not exact. ## Don&#39;t assume equal sample sizes, but still assume equal variance s &lt;- sqrt( (((nx-1)*varx + (ny-1)*vary) / (nx + ny - 2)) * (1/nx + 1/ny) ) tobs &lt;- (xbar - ybar) / s df &lt;- nx + ny - 2 pval &lt;- pt(tobs, df, lower.tail=F) if(pval &lt; 0.05) { print(&#39;reject the null&#39;) print(tobs) print(df) print(pval) } else { print(&#39;fail to reject the null&#39;) print(tobs) print(df) print(pval) } ## [1] &quot;fail to reject the null&quot; ## [1] 0.8870323 ## [1] 39 ## [1] 0.1902496 t.test(x=x, y=y, alternative=&#39;greater&#39;, mu=0, paired=FALSE, var.equal=TRUE, conf.leve=0.95) ## ## Two Sample t-test ## ## data: x and y ## t = 0.88703, df = 39, p-value = 0.1902 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -30.40335 Inf ## sample estimates: ## mean of x mean of y ## 140.9593 107.1572 # df &lt;- (varx/nx +vary/ny)^2 / ( (varx/nx)^2/(nx-1) + (vary/ny)^2/(ny-1)) Hooray! Now everything matches exactly! "],["paired-samples-t-test.html", "22 Paired samples t-test", " 22 Paired samples t-test Suppose we ask the following question about our criterion learning data: Do people reach criterion faster on the second problem than they do on the first problem? To address this question we proceed as we did in the previous example. Let \\(X \\sim \\mathcal{N}(\\mu_X, \\sigma_X)\\) and \\(Y \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y)\\). Let \\(X\\) generate data for problem 1 condition and \\(Y\\) generate data for the problem 2 condition. In the previous example, \\(X\\) and \\(Y\\) were independent. This was a reasonable assumption because our design is between-subjects with respect to condition. Here, however, we are comparing data from the same subjects on different problems. That is, our design is within-subject with respect to problem number. This means that we cannot assume independence, and our procedure will look a bit different. As we will see below, the trick will be to compute difference scores for each subject, and then simply proceed as we would with a one-sample t-test using these difference scores as our sample. 1. Specify the null and alternative hypotheses (\\(H_0\\) and \\(H_1\\)) in terms of a distribution and population parameter. Let \\(D\\) be the random variable that generates paired difference scores between \\(X\\) and \\(Y\\). \\[ H_0: \\mu_D = 0 \\\\ H_1: \\mu_D &gt; 0 \\] 2. Specify the type I error rate – denoted by the symbol \\(\\alpha\\) – you are willing to tolerate. \\[\\alpha = 0.05\\] 3. Specify the sample statistic that you will use to estimate the population parameter in step 1 and state how it is distributed under the assumption that \\(H_0\\) is true. \\[ \\begin{align} \\hat{\\mu_D} = \\overline{D} &amp;= \\frac{1}{n} \\sum_{i=1}^n x_i - y_i \\\\ \\overline{D} &amp;\\sim \\mathcal{N}(\\mu_\\overline{D}, \\sigma_\\overline{D}) \\\\ \\mu_\\overline{D} &amp;= \\mu_D \\\\ \\sigma_\\overline{D} &amp;= \\frac{\\sigma_D}{\\sqrt{n}} \\end{align} \\] As usual, we will typically not know \\(\\sigma_D\\) and so we will estimate it with the sample standard deviation, and the resulting test statistic will be \\(t\\). \\[ t_{obs} = \\frac{\\overline{D} - \\mu_{\\overline{D}}}{\\sigma_\\overline{D}} \\sim t(n - 1) \\] where the degrees of freedom is \\(n-1\\). 4. Obtain a random sample and use it to compute the sample statistic from step 3. Call this value \\(\\widehat{\\theta}_{\\text{obs}}\\) library(data.table) library(ggplot2) rm(list = ls()) d &lt;- fread( &#39;https://crossley.github.io/book_stats/data/criterion_learning/crit_learn.csv&#39;) d &lt;- d[cnd %in% c(&#39;Delay&#39;, &#39;Short ITI&#39;, &#39;Long ITI&#39;)] dd &lt;- d[order(cnd, sub), mean(t2c), .(cnd, sub)] ## Add a column to indicate the number of problems solved. ## This is important because we cant compute a diff score ## unless they solved at least two problems. d[, nps := max(prob_num), .(cnd, sub)] x &lt;- d[nps &gt; 1 &amp; cnd %in% c(&#39;Delay&#39;, &#39;Long ITI&#39;) &amp; prob_num==1, unique(t2c)] y &lt;- d[nps &gt; 1 &amp; cnd %in% c(&#39;Delay&#39;, &#39;Long ITI&#39;) &amp; prob_num==2, unique(t2c)] D &lt;- x - y n &lt;- length(D) Dbar &lt;- mean(D) Dbarsig &lt;- sd(D) / sqrt(n) tobs &lt;- Dbar / Dbarsig 5. If \\(\\widehat{\\theta}_{\\text{obs}}\\) is very unlikely to occur under the assumption that \\(H_0\\) is true, then reject \\(H_0\\). Otherwise, do not reject \\(H_0\\). df &lt;- n - 1 pval &lt;- pt(tobs, df, lower.tail=F) if(pval &lt; 0.05) { print(&#39;reject the null&#39;) print(tobs) print(df) print(pval) } else { print(&#39;fail to reject the null&#39;) print(tobs) print(df) print(pval) } ## [1] &quot;fail to reject the null&quot; ## [1] -0.3542251 ## [1] 30 ## [1] 0.6371761 # pass the difference scores and set paired to FALSE t.test(x=D, alternative=&#39;greater&#39;, mu=0, paired=FALSE, var.equal=TRUE, conf.leve=0.95) ## ## One Sample t-test ## ## data: D ## t = -0.35423, df = 30, p-value = 0.6372 ## alternative hypothesis: true mean is greater than 0 ## 95 percent confidence interval: ## -59.2225 Inf ## sample estimates: ## mean of x ## -10.22581 # pass both x and y and set paried to TRUE t.test(x=x, y=y, alternative=&#39;greater&#39;, mu=0, paired=TRUE, var.equal=TRUE, conf.leve=0.95) ## ## Paired t-test ## ## data: x and y ## t = -0.35423, df = 30, p-value = 0.6372 ## alternative hypothesis: true mean difference is greater than 0 ## 95 percent confidence interval: ## -59.2225 Inf ## sample estimates: ## mean difference ## -10.22581 "],["binomial-test.html", "23 Binomial Test", " 23 Binomial Test Consider an experiment in which researchers are trying to determine if a particular rat has learned to press one of two levers whenever they are placed inside of an experimental apparatus. Intuitively, answering this questions would be as simple as just watching the rat and taking note of whether it pressed the correct lever or not. The trouble is that rat behaviour is somewhat random: it sometimes pressed the correct lever and sometimes presses the incorrect lever. Suppose that the experiment contained \\(n=100\\) trials and the number of trials in which the rat pressed the lever was \\(n_{\\text{pressed}}=61\\). Did the rat learn or not? We turn to Null Hypothesis Significance Testing (NHST) to answer this question. 1. Specify the null and alternative hypotheses (\\(H_0\\) and \\(H_1\\)) in terms of a distribution and population parameter. First, decide on a statistical model for the measurements you are trying to reason about. That is, think of your data as being samples from a random variable. What distribution does that random variable have? We have a rat that either presses the correct lever or doesn’t. This is a dichotomous outcome, so we know that Binomial distribution is a good model. Recall that the binomial has two parameters, (1) the number of trials \\(n\\) and (2) the probability of success \\(p\\). The number of trials is fixed for us by our experiment (\\(n=100\\)), and \\(p\\) is the very thing we are trying to reason about. If \\(p=0.5\\) then the rat is just guessing at the levers. This corresponds to a null result, and therefore to the null hypothesis \\(H_0\\). If \\(p&gt;0.5\\) then the rat is doing better than guessing and we would say that it has learned. Therefore, this corresponds to the alternative hypothesis \\(H_1\\). Let \\(X\\) be the binomial random variable that generates the results in the above experiment. Then, \\[ X \\sim binomial(n=100, p) \\\\ H_0: p = 0.5 \\\\ H_1: p &gt; 0.5 \\] 2. Specify the type I error rate – denoted by the symbol \\(\\alpha\\) – you are willing to tolerate. It is traditional in psychology and neuroscience to tolerate a \\(5\\%\\) type I error rate in our inference procedure. This means that 5 out of every 100 times we think the rat has learned, we will have made an incorrect conclusion. \\[ \\alpha=0.05 \\] 3. Specify the sample statistic that you will use to estimate the population parameter in step 1 and state how it is distributed under the assumption that \\(H_0\\) is true. We are trying to make an inference about the population parameter \\(p\\). An intuitive estimate of this parameter is the proportion of successful trials relative to all trials performed. Let \\(x\\) be the be the count of successful trials. Then, \\[ \\widehat{p}=\\frac{x}{n} \\] We know that \\(X\\) is Binomial, but what about \\(\\widehat{p}\\)? With some careful thinking, we can see that \\(\\widehat{p}\\) is also Binomial. The possible outcomes of \\(X\\) just need to be normalised by the total number of observations \\(n\\). The probabilities associated with each outcome are the same as those that corresponded to the \\(X\\) distribution. Another approach (not taken here) is to rephrase the hypotheses to be in terms of counts instead of proportions (e.g., \\(H_0: np = 50; H_0: np &gt; 50\\)). The sampling distribution of \\(\\widehat{p}\\) under the null is as follows: \\[ \\widehat{p} = \\frac{X}{n} \\sim binomial(n=100, p=0.5), x \\rightarrow \\frac{x}{n} \\] 4. Obtain a random sample and use it to compute the sample statistic from step 3. Call this value \\(\\widehat{\\theta}_{\\text{obs}}\\) This was given to us in the formulation of the example. The key is to understand that in this example, \\(\\widehat{\\theta}_{\\text{obs}} = \\widehat{p}_{\\text{obs}}\\). \\[ \\widehat{p}_{\\text{obs}}=\\frac{61}{100} \\] 5. If \\(\\widehat{\\theta}_{\\text{obs}}\\) is very unlikely to occur under the assumption that \\(H_0\\) is true, then reject \\(H_0\\). Otherwise, do not reject \\(H_0\\). Compute the probability of \\(\\widehat{\\theta}_{\\text{obs}}\\) or a more extreme outcome occurring under the assumption that \\(H_0\\) is true. This value is called the p-value in NHST. If the p-value is very small (less than ) then reject \\(H_0\\). Otherwise, fail to reject \\(H_0\\). Formally, we write our decision rule as follows: \\[ \\text{if } P(\\widehat{\\theta} \\geq \\widehat{\\theta}_{\\text{obs}} | H_0) \\leq \\alpha \\rightarrow \\text{Reject } H_0\\\\ \\text{otherwise fail to reject } H_0 \\] Notice that our logic only allows to reject or fail to reject the null hypothesis. We can’t make any inference about the alternative hypothesis (e.g., we can’t accept the alternative). We are only assessing the evidence for the null. The purpose of including an alternative hypothesis is to give meaning to what sorts of outcomes correspond to more extreme than what we observed. To proceed, note again that in this example \\(\\widehat{\\theta}_{\\text{obs}} = \\widehat{p}_{\\text{obs}}\\). We therefore need to compute \\[ P(\\widehat{p} \\geq \\widehat{p}_{\\text{obs}} | H_0) \\] When actually computing the p-value, we will turn to pbinom(). From the plot above, and from reasoning about the alternative hypothesis, we see that we need lower.tail=FALSE. n &lt;- 100 xobs &lt;- 61 # observed count phatobs &lt;- xobs / n # observed proporion ## `xobs-1` because `lower.tail=FALSE` give P(X &gt; x) ## but we want P(X &gt;= x) pval &lt;- pbinom(xobs-1, n, p, lower.tail=FALSE) pval ## [1] 0.0176001 ## Notice that in the above we used `xobs` and not `phatobs`. ## This is becuase `pbinom()` needs counts, not proportions ## to functino properly. The probability it returns for `xobs` ## will perfectly correspond to the probabiliy of `phatobs` ## because of the notes discussed above in step 3 The decision rule above can also be expressed in terms of a critical value. The critical value \\(\\widehat{\\theta}_{\\text{crit}}\\) in NHST is the outcome such that \\(P(\\widehat{\\theta} \\geq \\widehat{\\theta}_{\\text{crit}} | H_0) \\leq \\alpha\\). As such, any observed outcome equal to the critical value or more extreme than the critical value will lead to the rejection of the null. For this reason, values more extreme than the critical value are said reside in the rejection region. To obtain critical values, we need to get an outcome that corresponds to a specific probability. This is given to us by the qbinom() function, again with lower.tail=FALSE. n &lt;- 100 p &lt;- 0.5 xcrit &lt;- qbinom(0.05, n, p, lower.tail=FALSE) ## Have to divide by n because we are working with ## proportions not counts phatcrit &lt;- xcrit / n xcrit ## [1] 58 phatcrit ## [1] 0.58 The five steps outlined above are the core of null-hypothesis significance testing, and working through each step in the longhand format that we just did is important to be able to do. This is because these five steps are completely general and will perfectly apply to any situation you might find yourself in. That said, in many situations, R has built-in functions to handle all five steps in one line of code. When making inferences from a binomial distribution, you can use the binom.test() function. xobs &lt;- 61 ## number of observed successes n &lt;- 100 ## n parameter of H0 p &lt;- 0.5 ## p parameter of H0 alpha &lt;- 0.05 ## error rate tolerance alt &lt;- &#39;greater&#39; ## test direction ## Use built in `binom.test()` binom.test(xobs, n, p, alternative = alt, conf.level = 1 - alpha ) ## ## Exact binomial test ## ## data: xobs and n ## number of successes = 61, number of trials = 100, p-value = 0.0176 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.5230939 1.0000000 ## sample estimates: ## probability of success ## 0.61 "],["one-way-anova.html", "24 One-way ANOVA 24.1 Learning objectives 24.2 Introduction 24.3 Intuition 24.4 Formal treatment 24.5 Example 1: Made up data 24.6 Example 2: Criterion learning data", " 24 One-way ANOVA 24.1 Learning objectives Understand the basic logic of how a 1-way ANOVA uses variances to test for equality of means. Understand why mean squares are used in the test statistic of a 1-way ANOVA rather than sum of squares. Be able to calculate all terms in an ANOVA table by hand. Understand how to use the ez library to perform 1-way ANOVAs. 24.2 Introduction Suppose you have \\(k\\) different treatment groups. A one-way ANOVA asks if there are any differences in the effects of treatment between any of the groups. This type of test is called an omnibus test. In raw form, the H’s look like this: \\(\\begin{align} &amp; H_0: \\mu1 = \\mu2 = \\mu3 \\ldots \\\\ &amp; H_1: \\lnot H_0 \\\\ \\end{align}\\) Usually, we have spent time in steps 1 and 3 of our hypothesis testing procedure to rewrite our hypotheses such that we can come up with a single statistic that is a good estimate of a single all-encompassing parameter. This case seems harder than most, we are interested in so many darn means! It turns out that the primary method used in this situation actually rests on variances. This can certainly seem a bit strange given that the test is concerned with means. Lets see how this all pans out. 24.3 Intuition The logic of an ANOVA can be understood intuitively as follows: between-group variation — how different are group means from each other? within-group variation — how noisy is your data in general? If the means are all the same (i.e., \\(H_0\\) is true), then between-group and within-group variation will be very similar. If \\(H_0\\) is not true, then between-group variation will be larger than within-group variation. Another good blurb for intuition can be read here: https://stats.stackexchange.com/questions/40549/how-can-i-explain-the-intuition-behind-anova ANOVA is statistical technique used to determine whether a particular classification of the data is useful in understanding the variation of an outcome. Think about dividing people into buckets or classes based on some criteria, like suburban and urban residence. The total variation in the dependent variable (the outcome you care about, like responsiveness to an advertising campaign) can be decomposed into the variation between classes and the variation within classes. When the within-class variation is small relative to the between-class variation, your classification scheme is in some sense meaningful or useful for understanding the world. Members of each cluster behave similarly to one another, but people from different clusters behave distinctively. This decomposition is used to create a formal F test of this hypothesis. Replacing the word variance in the above with the word similarity might be helpful / more intuitive. Armed with this intuition, we come up with the following test statistic form. \\(\\begin{align} &amp; H_0: \\frac{\\sigma^2_{between-group}}{\\sigma^2_{within-group}} = 1 \\\\ &amp; H_1: \\frac{\\sigma^2_{between-group}}{\\sigma^2_{within-group}} &gt; 1 \\\\ \\end{align}\\) First, notice that an ANOVA always uses an alternative hypothesis that is one-sided. This is because we only reject the null if the between-group variability is significantly greater than the within-group variability, and this always corresponds to a greater than in \\(H_1\\). Second, what the heck is \\(\\sigma^2_{between-group}\\) and \\(\\sigma^2_{within-group}\\)? We will unpack this more formally below. 24.4 Formal treatment Treatment 1 Treatment 2 Treatment 3 \\(x_1\\) \\(y_1\\) \\(z_1\\) \\(x_2\\) \\(y_2\\) \\(z_2\\) \\(\\ldots\\) \\(\\ldots\\) \\(\\ldots\\) \\(x_l\\) \\(y_m\\) \\(z_n\\) If \\(l=m=n\\) then we have an equal number of observations for each group. When this is the case, we say that we have a balanced design. Grand mean: \\[ G = \\frac{\\bar{x} + \\bar{y} + \\bar{z}}{3} \\] Between-group variation: \\[ \\text{ss}_{between} = l (\\bar{x} - G)^2 + m (\\bar{y} - G)^2 + n (\\bar{z} - G)^2 \\] Within-group variation: \\[ \\text{ss}_{within} = \\sum_{i=1}^l (x_i-\\bar{x})^2 + \\sum_{i=1}^m (y_i-\\bar{y})^2 + \\sum_{i=1}^n (z_i-\\bar{z})^2 \\] Given this, a reasonable guess for how to estimate the population parameters in \\(H_0\\) might be: \\[ \\widehat{\\frac{\\sigma^2_{between-group}}{\\sigma^2_{within-group}}} = \\frac{\\text{ss}_{between}}{\\text{ss}_{within}} \\] This is pretty much correct, except that in practise we use mean squared deviations, which are just sum of squared deviations divided by their corresponding degrees of freedom. We need to do this because we don’t want the within variance to dominate simply because it usually has many more observations that can contribute to the sum. This leads to the following treatment: \\(\\begin{align} &amp; H_0: \\frac{\\sigma^2_{between-group}}{\\sigma^2_{within-group}} = 1 \\\\ &amp; H_1: \\frac{\\sigma^2_{between-group}}{\\sigma^2_{within-group}} &gt; 1 \\\\ \\end{align}\\) \\(\\begin{align} &amp; \\widehat{\\frac{\\sigma^2_{between-group}}{\\sigma^2_{within-group}}} = \\frac{\\text{ms}_{between}}{\\text{ms}_{within}} \\\\\\\\ &amp; \\text{ms}_{between} = \\frac{\\text{ss}_{between}}{df_{between}} \\\\ &amp; \\text{ms}_{within} = \\frac{\\text{ss}_{within}}{df_{within}} \\\\\\\\ &amp; df_{between} = n_{groups} - 1 \\\\ &amp; df_{within} = n_{total} - n_{groups} \\\\\\\\ &amp; n_{total} = l + m + n \\\\\\\\ &amp; \\widehat{\\frac{\\sigma^2_{between-group}}{\\sigma^2_{within-group}}} = \\frac{\\text{ms}_{between}}{\\text{ms}_{within}} \\sim F(df_{between}, df_{within})\\\\ &amp; F_{obs} = \\frac{\\text{ms}_{between}}{\\text{ms}_{within}} \\sim F(df_{between}, df_{within})\\\\ \\end{align}\\) I have not shown how we know that the ratio of variances has an F distribution as doing so is beyond the scope of the unit. In general, we leave the process of finding good test statistics and characterising their distributions to the statisticians. 24.4.1 What is an F distribution? 24.4.2 Assumptions The observations are obtained independently and randomly from the population defined by the factor levels. The data of each factor level are normally distributed. These normal populations have a common variance (homogeneity of variance). 24.5 Example 1: Made up data Consider the following data: ## x y z ## 1: 6.867731 10.89766 27.558906 ## 2: 10.918217 17.43715 21.949216 ## 3: 5.821857 18.69162 16.893797 ## 4: 17.976404 17.87891 8.926501 ## 5: 11.647539 13.47306 25.624655 Test the hypothesis that the population means \\(\\mu_X\\), \\(\\mu_Y\\), \\(\\mu_Z\\) are different. ## step 1 ## H0: sig_between / sig_within = 1 ## H1: sig_between / sig_within &gt; 1 ## step 2 alph &lt;- 0.05 ## step 3 ## ms_ratio_hat = ms_between / ms_within ## step 4 x &lt;- c(6.867731, 10.918217, 5.821857, 17.976404, 11.647539) y &lt;- c(10.89766, 17.43715, 18.69162, 17.87891, 13.47306) z &lt;- c(27.558906, 21.949216, 16.893797, 8.926501, 25.624655) nx &lt;- length(x) ny &lt;- length(y) nz &lt;- length(z) n_total &lt;- nx + ny + nz n_groups &lt;- 3 ## mean of each group mean_x &lt;- mean(x) mean_y &lt;- mean(y) mean_z &lt;- mean(z) ## grand mean grand_mean &lt;- mean(c(x, y, z)) ## ss-between ss_between &lt;- nx*(mean_x - grand_mean)^2 + ny*(mean_y - grand_mean)^2 + nz*(mean_z - grand_mean)^2 ## ss-within ss_within_x &lt;- 0 for(i in 1:nx) { ss_within_x &lt;- ss_within_x + (x[i] - mean_x)^2 } ss_within_y &lt;- 0 for(i in 1:ny) { ss_within_y &lt;- ss_within_y + (y[i] - mean_y)^2 } ss_within_z &lt;- 0 for(i in 1:nz) { ss_within_z &lt;- ss_within_z + (z[i] - mean_z)^2 } ss_within &lt;- ss_within_x + ss_within_y + ss_within_z ## ss-within --- a better way ss_within &lt;- sum((x - mean_x)^2) + sum((y - mean_y)^2) + sum((z - mean_z)^2) ## dfs df_between &lt;- n_groups-1 df_within &lt;- n_total - n_groups ## mean squares ms_between &lt;- ss_between / df_between ms_within &lt;- ss_within / df_within ## observed F-value fobs &lt;- ms_between / ms_within ## compute pval pval &lt;- pf(fobs, df_between, df_within, lower.tail=F) ## report results print(c(ss_between, ss_within, df_between, df_within, fobs, pval)) ## [1] 227.95300725 361.75603363 2.00000000 12.00000000 3.78077466 ## [6] 0.05329273 ## Check our work using a builtin R function d &lt;- data.table(x,y,z) d &lt;- melt(d, measure.var=c(&#39;x&#39;, &#39;y&#39;, &#39;z&#39;)) d[, variable := factor(variable)] fm &lt;- lm(value ~ variable, data = d) anova(fm) ## Analysis of Variance Table ## ## Response: value ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## variable 2 227.95 113.977 3.7808 0.05329 . ## Residuals 12 361.76 30.146 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # We will ultimately be using the ez library for ANOVAs, so # might as well get started now. library(ez) d[, subject := factor(1:.N)] ezANOVA( data=d, dv=value, wid=subject, between=.(variable), type=3 ) ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 variable 2 12 3.780775 0.05329273 0.3865517 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 2 12 24.07389 156.2317 0.9245458 0.4232137 24.6 Example 2: Criterion learning data fp &lt;- &#39;https://crossley.github.io/book_stats/data/criterion_learning/crit_learn.csv&#39; d &lt;- fread(fp) ## redefine d for simplification d &lt;- d[cnd %in% c(&#39;Delay&#39;, &#39;Long ITI&#39;, &#39;Short ITI&#39;)][, mean(unique(t2c)), .(cnd, sub)] setnames(d, &#39;V1&#39;, &#39;t2c&#39;) ggplot(d, aes(x=cnd, y=t2c)) + geom_boxplot() + theme(aspect.ratio = 1) # Calculate the number of groups and observations per group d[, n_cnds := length(unique(cnd))] d[, n_obs := .N, .(cnd)] ## Calculate the mean within each cnd: d[, t2c_mean_cnd := mean(t2c), .(cnd)] ## Calculate the overall mean: d[, t2c_mean_grand := mean(t2c)] ## Calculate the between-cnd sum of squared differences: d[, ss_between := sum((t2c_mean_cnd - t2c_mean_grand)^2)] ## Calculate the &quot;within-cnd&quot; sum of squared differences. d[, ss_within := sum((t2c - t2c_mean_cnd)^2)] ## Compute degrees of freedom d[, df_between := n_cnds-1] d[, df_within := .N - n_cnds] ## Calculate MSE terms d[, mse_between := ss_between / df_between] d[, mse_within := ss_within / df_within] ## Calculate the F-ratio d[, fobs := mse_between / mse_within] ## Calculate p-val d[, pval := pf(fobs, df_between, df_within, lower.tail=FALSE)] print(round( c(d[, unique(ss_between)], d[, unique(ss_within)], d[, unique(df_between)], d[, unique(df_within)], d[, unique(fobs)], d[, unique(pval)] ), 4)) ## [1] 25758.2646 630055.8679 2.0000 55.0000 1.1243 0.3322 ## Do it with built-in R functions fm &lt;- lm(t2c ~ cnd, data = d) anova(fm) ## Analysis of Variance Table ## ## Response: t2c ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cnd 2 25758 12879 1.1243 0.3322 ## Residuals 55 630056 11456 # We will ultimately be using the ez library for ANOVAs, so # might as well get started now. library(ez) d[, sub := factor(sub)] d[, cnd := factor(cnd)] ezANOVA( data=d, dv=t2c, wid=sub, between=.(cnd), type=3 ) ## Warning: Data is unbalanced (unequal N per group). Make sure you specified a ## well-considered value for the type argument to ezANOVA(). ## Coefficient covariances computed by hccm() ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 cnd 2 55 1.124269 0.3322408 0.03927678 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 2 55 27898.51 583609.9 1.314592 0.2768892 Notice the warning message about unequal n per group. The gist of this warning is as follows: There are different methods of computing \\(ss\\) terms (not shown in this lecture). If we have an unbalanced design (i.e., unequal \\(n\\) per group), then these different methods produce different results. In psychology and neuroscience, the standard approach is to use Type III sums of squares. The reason for this is beyond the scope of this unit. "],["two-way-anova.html", "25 Two-way ANOVA 25.1 Two-way ANOVA Intuition 25.2 Visualising the three null hypotheses 25.3 Two-way ANOVA more formally 25.4 Two-way ANOVA in R 25.5 Two-way ANOVA in R using real data 25.6 Unbalanced designs", " 25 Two-way ANOVA Consider an experiment in which one of two different treatments are administered at one of two different doses. A two-way ANOVA attempts to answer the following three questions: Do different treatments lead to different outcomes? Do different doses lead to different outcomes? Does treatment and dose interact? 25.1 Two-way ANOVA Intuition The logic of a two-way ANOVA is essentially the same as for a one-way ANOVA. That is, if between-group variation is significantly larger than within-group variation, then we have evidence that different groups have different means. However, with a two-way ANOVA, between-group and withing-group aren’t quite as easily defined, because there are many groups for observations to fall between or within. The basic approach to dealing with this is to see that a two-way ANOVA is actually testing three null hypotheses simultaneously (i.e., the three statements above). 25.2 Visualising the three null hypotheses Consider the simple example of two treatments \\(A\\) and \\(B\\), both with two treatment doses. 25.2.1 Example 1 - No main effects and no interaction 25.2.2 Example 2 - No main effects but with an interaction 25.2.3 Example 3 - Two main effects but no interaction 25.2.4 Example 4 - Two main effects and an interaction 25.3 Two-way ANOVA more formally To talk about a two-way ANOVA clearly, it is helpful to introduce the following new nomenclature: Let \\(y_{ijk}\\) be the \\(k^{th}\\) observation obtained for treatment \\(i\\) and dose \\(j\\). In this example, \\(i \\in \\{A, B\\}\\), \\(j \\in \\{1, 2\\}\\), and \\(k \\in \\{1,2,...,n_{i,j}\\}\\), where \\(n_{i,j}\\) is the number of observations in the corresponding condition. Let \\(\\bar{y}_{ij \\bullet}\\) indicate the mean effect observed for treatment \\(i\\) and dose \\(j\\). Let \\(\\bar{y}_{\\bullet j \\bullet}\\) indicate the mean effect observed for dose \\(j\\), averaged over all possible treatments. Let \\(\\bar{y}_{i \\bullet \\bullet}\\) indicate the mean effect observed for treatment \\(i\\), averaged over all possible doses. Let \\(\\bar{y}_{\\bullet \\bullet \\bullet}\\) indicate the mean averaged over all possible treatments and doses. For this simple case in which we have only two treatments \\(A\\) and \\(B\\), only two doses, and we have exactly \\(n\\) observations for each treatment at each dose, our data looks as follows: Treatment A Treatment B Dose 1 \\(\\bar{y}_{11\\bullet} = \\frac{y_{111}+y_{112}+\\ldots+y_{11n}}{n}\\) \\(\\bar{y}_{21\\bullet} = \\frac{y_{121}+y_{122}+\\ldots+y_{12n}}{n}\\) \\(\\bar{y}_{\\bullet 1 \\bullet}\\) Dose 2 \\(\\bar{y}_{12\\bullet} = \\frac{y_{211}+y_{212}+\\ldots+y_{11n}}{n}\\) \\(\\bar{y}_{22\\bullet} = \\frac{y_{221}+y_{222}+\\ldots+y_{22n}}{n}\\) \\(\\bar{y}_{\\bullet 2 \\bullet}\\) \\(\\bar{y}_{1 \\bullet \\bullet}\\) \\(\\bar{y}_{2 \\bullet \\bullet}\\) \\(\\bar{y}_{\\bullet \\bullet \\bullet}\\) Please note that this is called a full factorial experiment. With this convention, we can write the H’s as follows: \\(\\begin{align} &amp; H_{0_1}: \\mu_{Treatment_A} = \\mu_{Treatment_B} \\\\ &amp; H_{0_2}: \\mu_{Dose_1} = \\mu_{Dose_2} \\\\ &amp; H_{0_3}: \\mu_{Treatment_i | Dose_j} = \\mu_{Treatment_i | Dose_j} \\\\\\\\ &amp; H_1: \\lnot H_0 \\\\ \\end{align}\\) 25.3.1 Using variance to test the null hypotheses \\(SS_{total}\\) is the total variability between all observations. \\(SS_{treatment}\\) is the variability between different treatments. \\(SS_{dose}\\) is the variability between different doses. \\(SS_{treatment \\times dose}\\) is the variability between different treatments at different dose levels. \\(SS_{error}\\) is the variability not accounted for by \\(SS_{treatment}\\), \\(SS_{dose}\\), and \\(SS_{treatment \\times dose}\\) 25.3.2 Variability terms akin to between-factor variability \\(n_{treatment} =\\) number of unique treatment levels. \\(n_{dose} =\\) number of unique dose levels. \\(n_{total} =\\) number of observations at level of dose and treatment, which in the following we assume is the same for all combinations of treatment and dose (i.e., we assume a balanced design). \\[\\begin{align} SS_{treatment} &amp;= n_{dose} n_{total} \\sum_i^{ n_{treatment} } (\\bar{y}_{i \\bullet \\bullet} - \\bar{y}_{\\bullet \\bullet \\bullet})^2 \\\\ SS_{dose} &amp;= n_{treatment} n_{total} \\sum_j^{ n_{dose} } (\\bar{y}_{\\bullet j \\bullet} - \\bar{y}_{\\bullet \\bullet \\bullet})^2 \\\\ SS_{treatment \\times dose} &amp;= n_{total} \\sum_i^{ n_{treatment} } \\sum_j^{ n_{dose} } \\left( \\bar{y}_{i j \\bullet} - (\\bar{y}_{i \\bullet \\bullet} + \\bar{y}_{\\bullet j \\bullet} - \\bar{y}_{\\bullet \\bullet \\bullet} ) \\right)^2 \\\\ \\end{align}\\] 25.3.3 Variability terms akin to within-factor variability \\[\\begin{align} SS_{error} &amp;= \\sum_i^{ n_{treatment} } \\sum_j^{ n_{dose} } \\sum_k^{ n_{total} } \\left( \\bar{y}_{i j k} - \\bar{y}_{i j \\bullet} \\right)^2 \\\\ \\end{align}\\] 25.3.4 Total variability \\[\\begin{align} SS_{Total} &amp;= \\sum_i^{ n_{treatment} } \\sum_j^{ n_{dose} } \\sum_k^{ n_{total} } \\left( \\bar{y}_{i j k} - \\bar{y}_{\\bullet \\bullet \\bullet} \\right)^2 \\\\ \\end{align}\\] Note that you can think of the \\(n_{treatment}\\), \\(n_{dose}\\), and \\(n_{total}\\) multipliers above serve to cast each \\(SS\\) term into the same units so that they can be meaningfully compared. You can also think of them as ensuring that \\(SS_{total}\\) is the sum of all the other sources of variability. \\[SS_{total} = SS_{treatment} + SS_{dose} + SS_{treatment \\times dose} + SS_{error}\\] 25.3.5 Degrees of freedom and building F-ratios Effect \\(df\\) \\(SS\\) \\(MS\\) \\(F\\) \\(Treatment\\) \\(n_{treatment} - 1\\) see above \\(\\frac{SS_{treatment}}{df_{treatment}}\\) \\(\\frac{MS_{treatment}}{MS_{error}}\\) \\(Dose\\) \\(n_{dose} - 1\\) see above \\(\\frac{SS_{dose}}{df_{dose}}\\) \\(\\frac{MS_{dose}}{MS_{error}}\\) \\(Treatment \\times Dose\\) \\((n_{treatment} - 1)(n_{dose} - 1)\\) see above \\(\\frac{SS_{treatment \\times dose}}{df_{treatment \\times dose}}\\) \\(\\frac{MS_{treatment \\times dose}}{MS_{error}}\\) \\(Error\\) \\(N - n_{treatment}n_{dose}\\) see above \\(\\frac{SS_{error}}{df_{error}}\\) From here, we compute p-values from the observed \\(F\\) scores in the table above, and we are off to the hypothesis testing races. 25.3.6 More general experiment and data structure More generally, two-way ANOVA deals with any two factors of an experiment that contain an arbitrary number of levels. Consider a full factorial experiment with two factors: Factor 1 containing \\(n_1\\) levels and Factor 2 containing \\(n_2\\) levels. Factor 2 level 1 Factor 2 level 2 \\(\\ldots\\) Factor 2 level \\(n_1\\) Factor 1 level 1 \\(\\bar{y}_{11\\bullet}\\) \\(\\bar{y}_{12\\bullet}\\) \\(\\ldots\\) \\(\\bar{y}_{1 n_2 \\bullet}\\) \\(\\bar{y}_{1 \\bullet \\bullet}\\) Factor 1 level 2 \\(\\bar{y}_{21\\bullet}\\) \\(\\bar{y}_{22\\bullet}\\) \\(\\ldots\\) \\(\\bar{y}_{2 n_2 \\bullet}\\) \\(\\bar{y}_{2 \\bullet \\bullet}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) Factor 1 level \\(n_1\\) \\(\\bar{y}_{n_1 1 \\bullet}\\) \\(\\bar{y}_{n_1 2 \\bullet}\\) \\(\\ldots\\) \\(\\bar{y}_{n_1 n_2 \\bullet}\\) \\(\\bar{y}_{n_1 \\bullet \\bullet}\\) \\(\\bar{y}_{\\bullet 1 \\bullet}\\) \\(\\bar{y}_{\\bullet 2 \\bullet}\\) \\(\\ldots\\) \\(\\bar{y}_{\\bullet n_2 \\bullet}\\) \\(\\bar{y}_{\\bullet \\bullet \\bullet}\\) This more general situation doesn’t change our method at all. We still proceed by generating the ANOVA table: Effect \\(df\\) \\(SS\\) \\(MS\\) \\(F\\) \\(Factor 1\\) \\(n_{Factor 1} - 1\\) see above \\(\\frac{SS_{Factor 1}}{df_{Factor 1}}\\) \\(\\frac{MS_{Factor 1}}{MS_{error}}\\) \\(Factor 2\\) \\(n_{Factor 2} - 1\\) see above \\(\\frac{SS_{Factor 2}}{df_{Factor 2}}\\) \\(\\frac{MS_{Factor 2}}{MS_{error}}\\) \\(Factor 1 \\times Factor 2\\) \\((n_{Factor 1}-1) (n_{Factor 2}-1)\\) see above \\(\\frac{SS_{Factor 1 \\times Factor 2}}{df_{Factor 1 \\times Factor 2}}\\) \\(\\frac{MS_{Factor 1 \\times Factor 2}}{MS_{error}}\\) \\(Error\\) \\(N - n_{Factor 1}n_{Factor 2}\\) see above \\(\\frac{SS_{error}}{df_{error}}\\) 25.4 Two-way ANOVA in R Consider the following data: ## y treatment dose ## 1: 16.314771 A 1 ## 2: 8.368833 A 1 ## 3: 16.648996 A 1 ## 4: 16.362147 A 1 ## 5: 32.073207 A 2 ## 6: 22.300250 A 2 ## 7: 25.357165 A 2 ## 8: 28.526398 A 2 ## 9: 19.971164 B 1 ## 10: 32.023267 B 1 ## 11: 23.817967 B 1 ## 12: 16.004954 B 1 ## 13: 34.261715 B 2 ## 14: 38.552692 B 2 ## 15: 38.503924 B 2 ## 16: 37.942446 B 2 y &lt;- c(16.314771, 8.368833, 16.648996, 16.362147, 32.073207, 22.300250, 25.357165, 28.526398, 19.971164, 32.023267, 23.817967, 16.004954, 34.261715, 38.552692, 38.503924, 37.942446) treatment &lt;- c( &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;) dose &lt;- c( 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2) d &lt;- data.table(y=y, treatment=as.factor(treatment), dose=as.factor(dose)) # generate intuition about results before doing the analysis # (looks like two main effects but no interaction) dd &lt;- d[, .(mean(y), sd(y)/sqrt(.N)), .(treatment, dose)] ggplot(dd, aes(treatment, V1, colour=dose)) + geom_pointrange(aes(ymin=V1-V2, ymax=V1+V2)) ## define number of levels in each factor n_treatment &lt;- d[, length(unique(treatment))] # number of treatment levels n_dose &lt;- d[, length(unique(dose))] # number of dose levels n &lt;- d[, .N, .(treatment, dose)][, unique(N)] # number of observations at each level ## define Df terms df_treatment &lt;- n_treatment - 1 df_dose &lt;- n_dose - 1 df_interaction &lt;- df_treatment * df_dose df_error &lt;- d[, .N] - n_treatment*n_dose ## Define SS terms ss_treatment &lt;- 0 for(i in d[, unique(treatment)]) { ss_treatment &lt;- ss_treatment + (d[treatment==i, mean(y)] - d[, mean(y)])^2 } ss_treatment &lt;- n_dose * n * ss_treatment ss_dose &lt;- 0 for(i in d[, unique(dose)]) { ss_dose &lt;- ss_dose + (d[dose==i, mean(y)] - d[, mean(y)])^2 } ss_dose &lt;- n_treatment * n * ss_dose ss_interaction &lt;- 0 for(i in d[, unique(treatment)]) { for(j in d[, unique(dose)]) { ss_interaction &lt;- ss_interaction + (d[treatment==i &amp; dose==j, mean(y)] - (d[treatment==i, mean(y)] + d[dose==j, mean(y)] - d[, mean(y)]))^2 } } ss_interaction &lt;- n * ss_interaction ss_error &lt;- 0 for(i in d[, unique(treatment)]) { for(j in d[, unique(dose)]) { for(k in 1:n) { ss_error &lt;- ss_error + (d[treatment==i &amp; dose==j][k, y] - d[treatment==i &amp; dose==j, mean(y)])^2 } } } ss_error &lt;- ss_error ## Define MS terms ms_treatment &lt;- ss_treatment / df_treatment ms_dose &lt;- ss_dose / df_dose ms_interaction &lt;- ss_interaction / df_interaction ms_error &lt;- ss_error / df_error ## Define F terms f_treatment &lt;- ms_treatment / ms_error f_dose &lt;- ms_dose / ms_error f_interaction &lt;- ms_interaction / ms_error ## Define Pr(&gt;F) p_treatment &lt;- pf(f_treatment, df_treatment, df_error, lower.tail=F) p_dose &lt;- pf(f_dose, df_dose, df_error, lower.tail=F) p_interaction &lt;- pf(f_interaction, df_interaction, df_error, lower.tail=F) ## NOTE: Lots of things to print out here, so I&#39;ll leave ## this to a live demo and your own personal tinkering ## around to see the actual values and confirm that they ## match the anova() method below. ## Verify results fm &lt;- lm(y ~ treatment*dose, data=d) anova(fm) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 1 352.75 352.75 16.6240 0.001534 ** ## dose 1 729.08 729.08 34.3593 7.697e-05 *** ## treatment:dose 1 2.96 2.96 0.1395 0.715325 ## Residuals 12 254.63 21.22 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Using ezANOVA d[, subject := 1:.N] ezANOVA(data=d, dv=y, wid=subject, between=.(treatment, dose), type=3) ## Warning: Converting &quot;subject&quot; to factor for ANOVA. ## Coefficient covariances computed by hccm() ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 treatment 1 12 16.6239881 1.53355e-03 * 0.58077121 ## 3 dose 1 12 34.3592629 7.69696e-05 * 0.74115205 ## 4 treatment:dose 1 12 0.1394674 7.15325e-01 0.01148876 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 3 12 31.57735 113.392 1.113918 0.3818176 25.5 Two-way ANOVA in R using real data d &lt;- fread(&#39;https://crossley.github.io/book_stats/data/mis/mis_data.csv&#39;) d &lt;- d[phase==&#39;Base&#39;, .(subject, group, target, error)] d[, group := factor(group)] d[, target := factor(target)] ## First, get one observation per subject per group per target dd &lt;- d[, mean(error), .(subject, group, target)] ## Do different groups have different mean errors? ## Do different targets have different mean errors? ## Does target and group interact? ## begin by making diagnostic plots ddd &lt;- dd[, .(mean(V1), sd(V1)/sqrt(length(unique(target)))), .(group)] ggplot(ddd, aes(group, V1)) + geom_pointrange(aes(ymin=V1-V2, ymax=V1+V2)) ddd &lt;- dd[, .(mean(V1), sd(V1)/sqrt(length(unique(group)))), .(target)] ggplot(ddd, aes(target, V1)) + geom_pointrange(aes(ymin=V1-V2, ymax=V1+V2)) ddd &lt;- dd[, .(mean(V1), sd(V1)/sqrt(length(unique(target))*length(unique(group)))), .(group, target)] ggplot(ddd, aes(target, V1, colour=group)) + geom_pointrange(aes(ymin=V1-V2, ymax=V1+V2), position=position_dodge(width=.1)) ## To my eye, it looks like the effect of group ought to be ## significant, or pretty close to it, but that&#39;s probably ## about it. ## Verify results options(contrasts = c(&quot;contr.sum&quot;,&quot;contr.poly&quot;)) # type 3 ss fm &lt;- lm(V1 ~ group * target, data=dd) anova(fm) ## Analysis of Variance Table ## ## Response: V1 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 1 461.40 461.40 431.8110 &lt;2e-16 *** ## target 10 13.45 1.35 1.2589 0.2561 ## group:target 10 10.80 1.08 1.0106 0.4358 ## Residuals 198 211.57 1.07 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ezANOVA(data=dd, dv=V1, wid=subject, between=.(group, target), type=3) ## Warning: Converting &quot;subject&quot; to factor for ANOVA. ## Warning: The column supplied as the wid variable contains non-unique values ## across levels of the supplied between-Ss variables. Automatically fixing this ## by generating unique wid labels. ## Coefficient covariances computed by hccm() ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 group 1 198 431.811012 1.207998e-51 * 0.68561998 ## 3 target 10 198 1.258945 2.560987e-01 0.05978197 ## 4 group:target 10 198 1.010574 4.358029e-01 0.04856061 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 21 198 37.66427 72.21667 4.917428 5.462064e-10 * 25.6 Unbalanced designs A balanced design means that there exactly the same amount of observations (denoted by \\(n\\) in the previous examples of this lecture) in every possible combination of factors explored by an experiment. Well, it turns out that it is fairly common to have an unbalanced design, which just means that there are different numbers of observations per factor level combination. Furthermore, it turns that for balanced designs, the sums of squares calculations I showed are unambiguous and everybody agrees how to go about computing them. For unbalanced designs, however, it turns out that there are more than one way of going about calculating sums of squares. This is often the reason why various statistical software packages (e.g., R, SAS, SPSS, etc) produce slightly different results. A good discussion can be found [here](https://www.r-bloggers.com/anova-%E2%80%93-type-iiiiii-ss-explained/, but we won’t go into it here. The important thing to remember is that in psychology and neuroscience, the standard is to apply Type III sums of squares**, and to do that, you just need to execute the line options(contrasts = c(\"contr.sum\",\"contr.poly\")) anywhere before you fit your linear model and generate your ANOVA summary. Also note that options(contrasts = c(\"contr.sum\",\"contr.poly\")) doesn’t do anything at all to balanced designs, so there is no harm in just doing it all the time if you care for that approach. Finally, note that whether or not a design is balanced or not only matters for designs with at least two factors. "],["repeated-measures-anova.html", "26 Repeated measures ANOVA 26.1 Intuition 26.2 Formal treatment 26.3 Repeated measures ANOVA in R", " 26 Repeated measures ANOVA A repeated measures design is one in which at least one of the factors consists of repeated measurements on the same experiment unit – this usually corresponds to multiple measurements from the same subjects. It is fair to view this as an extension of the paired-samples t-test, just as it is fair to view factorial ANOVA as an extension of the independent samples t-test. Advantage: individual differences possibly reduced as a source of between-group differences. Advantage: sample size is not divided between conditions so can require fewer subjects. Disadvantage: fewer subjects means smaller degrees of freedom (we will see below the relevant \\(df\\) term shrinks from \\(n_{observations} - k\\) to \\((k - 1)(n_{subject} - 1)\\). The more degrees of freedom we have, in general, the less extreme observed outcome we need to reject the null (because of its effect on the shape of the sampling distribution of our test statistic). 26.1 Intuition The intuition for a repeated measures ANOVA is the same as that for a factorial ANOVA. E.g., if the population means in the levels of some factor (e.g., the mean effect of different doses of a medicine) are different, then between-level variability should be greater than within-level variability. However, the repeated measures aspect introduces one important difference. Between-level variability will inherently be smaller in a repeated measures design than in an independent samples design (e.g., because the same subjects give measurements for each level, and subjects tend to be similar to themselves). This means that, to decide that there are true differences, we should require less between-level differences in variability for repeated measures designs than for independent samples designs. Recall that for a factorial ANOVA, the \\(F\\)-test that we use is a ratio of between-level variability to within-level variability. \\[F = \\frac{MS_{between-levels}}{MS_{within-levels}}\\] In a repeated measures ANOVA, the \\(F\\)-test that we use is the ratio \\[F = \\frac{MS_{between-levels}}{MS_{within-levels} - MS_{between-subjects}}\\] 26.2 Formal treatment \\(k\\) is the number of factor levels \\(n\\) is the number of subjects \\(x_{ij}\\) is observation from factor level \\(i\\) and subject \\(j\\) \\[\\begin{align} SS_{between-levels} &amp;= n \\sum_{i=1}^k (\\bar{x_{i \\bullet}} - \\bar{x_{\\bullet \\bullet}})^2 \\\\ SS_{within-levels} &amp;= \\sum_{i=1}^k \\sum_{j=1}^n (x_{ij} - \\bar{x_{i \\bullet}})^2 \\\\ SS_{between-subject} &amp;= k \\sum_{j=1}^n (\\bar{x_{\\bullet j}} - \\bar{x_{\\bullet \\bullet}})^2 \\\\ SS_{error} &amp;= SS_{within-levels} - SS_{between-subject} \\\\ \\end{align}\\] The nomenclature \\(SS_{error}\\) will make more sense in the coming lectures. This leads to the ANOVA table: \\(Df\\) \\(SS\\) \\(MS\\) \\(F\\) \\(P(&gt;F)\\) k-1 see above \\(SS_{between-levels}\\) \\(\\frac{MS_{between-levels}}{MS_{error}}\\) (k-1)(n-1) see above \\(SS_{error}\\) 26.3 Repeated measures ANOVA in R 26.3.1 toy example ## level subject score ## 1: 1 1 11.262954 ## 2: 1 2 9.673767 ## 3: 1 3 11.329799 ## 4: 1 4 11.272429 ## 5: 1 5 10.414641 ## 6: 2 1 18.460050 ## 7: 2 2 19.071433 ## 8: 2 3 19.705280 ## 9: 2 4 19.994233 ## 10: 2 5 22.404653 ## 11: 3 1 30.763593 ## 12: 3 2 29.200991 ## 13: 3 3 28.852343 ## 14: 3 4 29.710538 ## 15: 3 5 29.700785 Notice in the above data that each subject gives multiple measurements (one per factor level). level &lt;- rep(1:3, each=5) subject &lt;- rep(1:5, 3) score &lt;- c(11.262954, 9.673767, 11.329799, 11.272429, 10.414641, 18.460050, 19.071433, 19.705280, 19.994233, 22.404653, 30.763593, 29.200991, 28.852343, 29.710538, 29.700785 ) d &lt;- data.table(level, subject, score) k &lt;- d[, length(unique(level))] # n factor levels n &lt;- d[, length(unique(subject))] # n subs ## do it by hand ss_between_levels &lt;- 0 for(i in 1:k) { ss_between_levels &lt;- ss_between_levels + (d[level==i, mean(score)] - d[, mean(score)])^2 } ss_between_levels &lt;- n * ss_between_levels ss_between_subject &lt;- 0 for(j in 1:n) { ss_between_subject &lt;- ss_between_subject + (d[subject==j, mean(score)] - d[, mean(score)])^2 } ss_between_subject &lt;- k * ss_between_subject ss_within_levels &lt;- 0 for(i in 1:k) { for(j in 1:n) { ss_within_levels &lt;- ss_within_levels + (d[level==i &amp; subject==j, score] - d[level==i, mean(score)])^2 } } ss_error &lt;- ss_within_levels - ss_between_subject df_between_levels &lt;- k - 1 df_error &lt;- (k-1)*(n-1) ms_between_levels &lt;- ss_between_levels / df_between_levels ms_error &lt;- ss_error / df_error fobs &lt;- ms_between_levels / ms_error p_val &lt;- pf(fobs, df_between_levels, df_error, lower.tail=F) ## Use the function `ezANOVA()` from the `ez` package to ## perform a repeated measures ANOVA d[, subject := factor(subject)] d[, level := factor(level)] ezANOVA( data=d, ## where the data is located dv=score, ## the dependent variable wid=subject, ## the repeated measure indicator column within = .(level), ## a list of repeated measures factors type = 3 ## type of sums of squares desired ) ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 level 2 8 370.7887 1.29746e-08 * 0.9852661 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 2 level 0.5279246 0.3835817 ## ## $`Sphericity Corrections` ## Effect GGe p[GG] p[GG]&lt;.05 HFe p[HF] p[HF]&lt;.05 ## 2 level 0.679313 2.30526e-06 * 0.9073176 5.770776e-08 * 26.3.2 Real data ## Consider the MIS data d &lt;- fread(&#39;https://crossley.github.io/book_stats/data/mis/mis_data.csv&#39;) ## We will answer this question: ## Are there significant differences in the mean error per ## subject across phases? Note that this question ignores ## differences between conditions ## First, fix the annoying bug that different subjects in ## different groups have the same number. d[group==1, subject := subject+10] ## compute mean error per subject dd &lt;- d[order(subject, phase), mean(error, na.rm=TRUE), .(subject, phase)] ## It&#39;s important to code factors as factors dd[, subject := factor(subject)] dd[, phase := factor(phase)] ## do it by hand n &lt;- d[, length(unique(subject))] k &lt;- d[, length(unique(phase))] ss_between_phases &lt;- 0 for(i in d[, unique(phase)]) { ss_between_phases &lt;- ss_between_phases + (dd[phase==i, mean(V1)] - dd[, mean(V1)])^2 } ss_between_phases &lt;- n * ss_between_phases ss_between_subject &lt;- 0 for(j in d[, unique(subject)]) { ss_between_subject &lt;- ss_between_subject + (dd[subject==j, mean(V1)] - dd[, mean(V1)])^2 } ss_between_subject &lt;- k * ss_between_subject ss_within_phases &lt;- 0 for(i in d[, unique(phase)]) { for(j in d[, unique(subject)]) { ss_within_phases &lt;- ss_within_phases + (dd[phase==i &amp; subject==j, V1] - dd[phase==i, mean(V1)])^2 } } ss_error &lt;- ss_within_phases - ss_between_subject df_between_phases &lt;- k - 1 df_error &lt;- (k-1)*(n-1) ms_between_phases &lt;- ss_between_phases / df_between_phases ms_error &lt;- ss_error / df_error fobs &lt;- ms_between_phases / ms_error p_val &lt;- pf(fobs, df_between_levels, df_error, lower.tail=F) ## Do it with ezANOVA() ezANOVA( data=dd, dv=V1, wid=subject, within=.(phase), type=3 ) ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 phase 2 38 123.1449 2.479838e-17 * 0.7786623 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 2 phase 0.4135505 0.0003537993 * ## ## $`Sphericity Corrections` ## Effect GGe p[GG] p[GG]&lt;.05 HFe p[HF] p[HF]&lt;.05 ## 2 phase 0.6303384 9.950264e-12 * 0.654296 4.302482e-12 * 26.3.3 Making sense of ezANOVA output What is Mauchly's Test for Sphericity and Sphericity Corrections? Both have to do with the underlying assumptions being made by a repeated measures ANOVA. Time permitting, we will return to this as we review the course material in preparation for the final exam. 26.3.4 Quick note on balanced versus unbalanced data The formulas I wrote in previous sections for computing the various sums of squares all assumed that we had a perfectly balanced design. Just as with factorial ANOVA, everything gets a little wonky with an unbalanced design. The details of this aren’t really suitable for this class. The important thing to know is that ezANOVA() will handle it all for you. "],["mixed-design-anova.html", "27 Mixed-design ANOVA 27.1 Toy example 27.2 Real data", " 27 Mixed-design ANOVA We have covered factorial ANOVA (one-way, two-way, etc.) and repeated measures ANOVA. A mixed-design ANOVA is what you get if some of your experimental factors are derived from independent samples (e.g., different subjects) and some are derived from repeated measures (i.e., the same subjects at different times). Mixed situations are very common, and are actually characteristic of nearly every set of real data we have used in this class to date. The good news is that there isn’t really a lot of work to do for us to learn to perform a mixed-design ANOVA. The maths involved are understandably more complex than what we have seen in the previous section but the basic logic and intuitions we have hopefully developed still holds. We are still asking an omnibus question about differences in means Differences in means still comes down to between-level variation being greater than within-level variation. Repeated measures introduces the same familiar problem of non-independent observations between-levels, and is dealt with in a similar way to what we saw in the last lecture (i.e., “correcting” the denominator of the F-ratio). With that, lets just dive into some practice. Important notice about mixed-design ANOVA: I will not ask you to compute the relevant sums of squares by hand for a mixed design ANOVA, but you will need to know how to use ezANOVA() to perform one. 27.1 Toy example ## subject level1 level2 score ## 1: 1 1 1 11.262954 ## 2: 2 1 1 9.673767 ## 3: 3 1 1 11.329799 ## 4: 4 1 0 11.272429 ## 5: 5 1 0 10.414641 ## 6: 1 2 1 18.460050 ## 7: 2 2 1 19.071433 ## 8: 3 2 1 19.705280 ## 9: 4 2 0 19.994233 ## 10: 5 2 0 22.404653 ## 11: 1 3 1 30.763593 ## 12: 2 3 1 29.200991 ## 13: 3 3 1 28.852343 ## 14: 4 3 0 29.710538 ## 15: 5 3 0 29.700785 subject &lt;- rep(1:5, 3) level1 &lt;- rep(1:3, each=5) level2 &lt;- rep(c(1, 1, 1, 0, 0), 3) score &lt;- c(11.262954, 9.673767, 11.329799, 11.272429, 10.414641, 18.460050, 19.071433, 19.705280, 19.994233, 22.404653, 30.763593, 29.200991, 28.852343, 29.710538, 29.700785 ) d &lt;- data.table(subject, level1, level2, score) # Be sure to diagnose what factors are repeated # (within-subject) and which are between-subject. # same subjects present at each level of level1 # level 1 is a within-subjects factor d[, unique(subject), .(level1)] ## level1 V1 ## 1: 1 1 ## 2: 1 2 ## 3: 1 3 ## 4: 1 4 ## 5: 1 5 ## 6: 2 1 ## 7: 2 2 ## 8: 2 3 ## 9: 2 4 ## 10: 2 5 ## 11: 3 1 ## 12: 3 2 ## 13: 3 3 ## 14: 3 4 ## 15: 3 5 # different subjects present in different levels of level 2 # level 2 is a between-subjects factor d[, unique(subject), .(level2)] ## level2 V1 ## 1: 1 1 ## 2: 1 2 ## 3: 1 3 ## 4: 0 4 ## 5: 0 5 ## Differences between means of level1 and level2 main ## effects or interaction? d[, subject := factor(subject)] d[, level1 := factor(level1)] d[, level2 := factor(level2)] ezANOVA( data=d, dv=score, wid=subject, within=.(level1), between=.(level2), type=3 ) ## Warning: Data is unbalanced (unequal N per group). Make sure you specified a ## well-considered value for the type argument to ezANOVA(). ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 level2 1 3 4.067233 1.370875e-01 0.2129728 ## 3 level1 2 6 406.004382 3.946207e-07 * 0.9908527 ## 4 level2:level1 2 6 1.563131 2.841678e-01 0.2943058 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 3 level1 0.7245834 0.7245834 ## 4 level2:level1 0.7245834 0.7245834 ## ## $`Sphericity Corrections` ## Effect GGe p[GG] p[GG]&lt;.05 HFe p[HF] ## 3 level1 0.7840576 6.479112e-06 * 1.491901 3.946207e-07 ## 4 level2:level1 0.7840576 2.920523e-01 1.491901 2.841678e-01 ## p[HF]&lt;.05 ## 3 * ## 4 27.2 Real data ## important libraries library(data.table) library(ggplot2) library(ez) ## clean slate rm(list=ls()) d &lt;- fread(&#39;https://crossley.github.io/book_stats/data/mis/mis_data.csv&#39;) ## We will answer this question: ## Are there significant differences in the mean error per ## subject across phases and between groups? ## First, fix the annoying bug that different subjects in ## different groups have the same number. d[group==1, subject := subject+10] ## compute mean error per subject per group dd &lt;- d[order(subject, phase), mean(error, na.rm=TRUE), .(subject, phase, group)] ## It&#39;s important to code factors as factors dd[, subject := factor(subject)] dd[, phase := factor(phase)] dd[, group := factor(group)] ggplot(dd, aes(group, V1, colour=phase)) + geom_boxplot() ## Do it with ezANOVA() ezANOVA( data=dd, dv=V1, wid=subject, within=.(phase), between=.(group), type=3 ) ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 group 1 18 45.30549 2.612929e-06 * 0.4729081 ## 3 phase 2 36 269.80842 2.143099e-22 * 0.9060701 ## 4 group:phase 2 36 23.62868 2.791502e-07 * 0.4579289 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 3 phase 0.01665791 7.651975e-16 * ## 4 group:phase 0.01665791 7.651975e-16 * ## ## $`Sphericity Corrections` ## Effect GGe p[GG] p[GG]&lt;.05 HFe p[HF] p[HF]&lt;.05 ## 3 phase 0.5041995 2.296727e-12 * 0.504943 2.218260e-12 * ## 4 group:phase 0.5041995 1.192202e-04 * 0.504943 1.181316e-04 * "],["simple-linear-regression.html", "28 Simple linear regression", " 28 Simple linear regression Simple linear regression models attempt to predict the value of some observed outcome random variable \\(\\boldsymbol{Y}\\) as a linear function of a predictor random variable \\(\\boldsymbol{X}\\). For the \\(i^{th}\\) observation, we can write: \\[ y_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon_{i} \\] \\(y_{i}\\) is the \\(i^{th}\\) observed outcome \\(x_{i}\\) is the \\(i^{th}\\) value of the predictor variable \\(\\epsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{\\epsilon})\\) is called the residual \\(\\beta_{0_{i}}\\) and \\(\\beta_{1_{i}}\\) are parameters of the linear regression model Now imagine having many observations such that \\(i \\in [1, n]\\): \\[\\begin{align} \\boldsymbol{y} &amp;= \\beta_{0} + \\beta_{1} \\boldsymbol{x} + \\boldsymbol{\\epsilon} \\\\\\\\ \\boldsymbol{y} &amp;= \\beta_{0} \\begin{bmatrix} 1\\\\ 1\\\\ \\vdots\\\\ 1 \\end{bmatrix} + \\beta_{1} \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n\\\\ \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_n\\\\ \\end{bmatrix} \\\\\\\\ \\boldsymbol{y} &amp;= \\begin{bmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_n\\\\ \\end{bmatrix} \\\\\\\\ \\boldsymbol{y} &amp;= \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\end{align}\\] 28.0.0.1 How can we pick \\(\\boldsymbol{\\beta}\\) values that best fit our data? let \\(y_i\\) denote observed values let \\(\\hat{y_{i}}\\) denote predicted values \\[\\begin{align} \\hat{y_{i}} &amp;= E[y_{i}]\\\\ &amp;= E[\\beta_{0} + \\beta_{1} x_{i} + \\epsilon_{i}]\\\\ &amp;= \\beta_{0} + \\beta_{1} x_{i} \\end{align}\\] The best fitting \\(\\boldsymbol{\\beta}\\) values are those that minimise the discrepancy between \\(y_{i}\\) and \\(\\hat{y_{i}}\\). \\[ \\DeclareMathOperator*{\\argmin}{\\arg\\!\\min} \\argmin_{\\boldsymbol{\\beta}} \\sum_{i=1}^{n} (y_{i} - \\hat{y_{i}})^2 \\] The \\(\\boldsymbol{\\beta}\\) values that do this can be found by using a computer to try out a bunch of different values and recording which ones give the smallest error. However, in this case, the \\(\\boldsymbol{\\beta}\\) values that minimise error can be solved for analytically. I won’t go through the derivation here, even though it is fairly simple to go through it. If you want to have a go on your own, the method is to take the derivative with respect to \\(\\boldsymbol{\\beta}\\), and then find the \\(\\boldsymbol{\\beta}\\) values that make the resulting expression equal to zero. 28.0.0.2 Regression model significance Notice that \\(\\sum_{i=1}^{n} (y_{i} - \\hat{y_{i}})^2\\) is a sum of squares very much in the style of the sums of squares we have seen thus far in ANOVAs. \\(SS_{error} = \\sum_{i=1}^{n} (y_{i} - \\hat{y_{i}})^2\\) \\(SS_{error}\\) is sometimes called \\(SS_{residual}\\) \\(SS_{error}\\) is what you get when you compare raw observations against the full model predictions. \\(SS_{total}\\) is what you get when you compare raw observations against the grand mean. \\(SS_{total} = \\sum_{i=1}^{n} (y_{i} - \\bar{y_{i}})^2\\) Just as \\(SS_{error}\\) comes from \\(\\sum_{i=1}^{n} (y_{i} - \\hat{y_{i}})^2\\) with \\(\\hat{y} = \\beta_{0} + \\beta_{1} x + \\epsilon\\), you can think of \\(SS_{total}\\) as coming from \\(\\sum_{i=1}^{n} (y_{i} - \\hat{y_{i}})^2\\) with \\(\\hat{y} = \\bar{y} + \\epsilon\\). From this perspective, \\(SS_{error}\\) is the variability of the data around the prediction from the full model, and \\(SS_{total}\\) is the variability of the data around the mean (which is pretty much the simplest possible model and thus serves as a good baseline). Finally \\(SS_{model} = \\sum_{i=1}^{n} (\\bar{y} - \\hat{y_i})^2\\) essentially tells you how much the added complexity of the full model reduces the overall variability (i.e., makes better predictions). The percent of variability accounted for above the simple model is given by: \\[ R^2 = \\frac{SS_{model}}{SS_{total}} \\] Does the more complex model provide a significantly better fit to the data than the simplest model? This is what the \\(F\\) ratio tells us. \\[ F = \\frac{MS_{model}}{MS_{error}} \\] That is, the regression \\(F\\)-ratio tells us how much the regression model has improved the prediction over the simple mean model, relative to the overall inaccuracy in the regression model. The \\(F\\) ratio tells us if the overall regression model provides a better fit to the data than the simple model, but we can also ask questions about the best fitting \\(\\beta\\) values (i.e., is either \\(\\beta\\) different from zero?). We won’t prove this here, but it turns out the best fitting \\(\\beta\\) values (i.e., \\(\\hat{\\beta}\\)) can be tested with a \\(t\\)-test. 28.0.0.3 Example: Predict one MEG channel from another library(data.table) library(ggplot2) rm(list=ls()) d &lt;- fread(&#39;https://crossley.github.io/book_stats/data/eeg/epochs.txt&#39;) d[, V1 := NULL] ## convert from wide to long format dd &lt;- melt(d, id.vars=c(&#39;time&#39;, &#39;condition&#39;, &#39;epoch&#39;)) ## pick out some columns randomly y &lt;- dd[variable==&#39;MEG 001&#39;, mean(value), .(epoch)][, V1] x &lt;- dd[variable==&#39;MEG 010&#39;, mean(value), .(epoch)][, V1] ## visualise possible linear relationship ddd &lt;- data.table(x, y) ggplot(ddd, aes(x, y)) + geom_point() + geom_smooth(method=&#39;lm&#39;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## fit a simple linear regression model fm &lt;- lm(y~x, data=ddd) summary(fm) ## ## Call: ## lm(formula = y ~ x, data = ddd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -527.21 -133.24 -4.46 118.84 417.85 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -23.6812 18.1518 -1.305 0.195 ## x 2.5436 0.3759 6.766 1.01e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 180.5 on 97 degrees of freedom ## Multiple R-squared: 0.3206, Adjusted R-squared: 0.3136 ## F-statistic: 45.78 on 1 and 97 DF, p-value: 1.006e-09 cor(x, y)^2 ## [1] 0.3206392 A correlation analysis provides information on the strength and direction of the linear relationship between two variables, while a simple linear regression analysis estimates parameters in a linear equation that can be used to predict values of one variable based on the other. Note that Multiple R-squared is equal to cor(x,y)^2 \\(R^2 = \\rho_{x,y}\\) 28.0.0.4 Example: Speed-Accuracy trade-off in MIS data library(data.table) library(ggplot2) rm(list=ls()) d &lt;- fread(&#39;https://crossley.github.io/book_stats/data/mis/mis_data.csv&#39;) ## give subjects in different groups unique numbers d[group==1, subject := subject + 10] ## define x and y x &lt;- d[, mean(error, na.rm=T), .(group, subject)][, V1] y &lt;- d[, mean(movement_time, na.rm=T), .(group, subject)][, V1] ## visualise possible linear relationship ddd &lt;- data.table(x, y) ggplot(ddd, aes(x, y)) + geom_point() + geom_smooth(method=&#39;lm&#39;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## fit a simple linear regression model fm &lt;- lm(y~x, data=ddd) summary(fm) ## ## Call: ## lm(formula = y ~ x, data = ddd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.820 -11.013 -6.315 12.287 34.197 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 323.951 4.597 70.476 &lt; 2e-16 *** ## x -12.703 2.154 -5.898 1.39e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.62 on 18 degrees of freedom ## Multiple R-squared: 0.659, Adjusted R-squared: 0.6401 ## F-statistic: 34.79 on 1 and 18 DF, p-value: 1.388e-05 cor(x, y)^2 ## [1] 0.659039 cor(fm$fitted.values, y)^2 ## [1] 0.659039 "],["multiple-regression.html", "29 Multiple Regression", " 29 Multiple Regression Multiple regression works just like the simple linear regression we just covered, but with multiple predictor variables. \\[\\begin{align} y_{i} &amp;= \\beta_{0} + \\beta_{1} x_{1_i} + \\beta_{2} x_{2_i} + \\dots + \\beta_{k} x_{k_i} + \\epsilon_{i} \\\\\\\\ \\boldsymbol{y} &amp;= \\beta_{0} \\begin{bmatrix} 1\\\\ 1\\\\ \\vdots\\\\ 1 \\end{bmatrix} + \\beta_{1} \\begin{bmatrix} x_{1_1}\\\\ x_{1_2}\\\\ \\vdots\\\\ x_{1_n}\\\\ \\end{bmatrix} + \\beta_{2} \\begin{bmatrix} x_{2_1}\\\\ x_{2_2}\\\\ \\vdots\\\\ x_{2_n}\\\\ \\end{bmatrix} + \\ldots + \\beta_{k} \\begin{bmatrix} x_{k_1}\\\\ x_{k_2}\\\\ \\vdots\\\\ x_{k_n}\\\\ \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_n\\\\ \\end{bmatrix} \\\\\\\\ \\boldsymbol{y} &amp;= \\begin{bmatrix} 1 &amp; x_{1_1} &amp; x_{2_1} &amp; \\ldots &amp; x_{k_1} \\\\ 1 &amp; x_{1_2} &amp; x_{2_2} &amp; \\ldots &amp; x_{k_2} \\\\ \\vdots &amp; \\vdots &amp; \\dots &amp; \\dots \\\\ 1 &amp; x_{1_n} &amp; x_{2_n} &amp; \\ldots &amp; x_{k_n} \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{k} \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_n\\\\ \\end{bmatrix} \\\\\\\\ \\boldsymbol{y} &amp;= \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\end{align}\\] It’s powerful to write these equations in matrix form (the last line above) because it highlights how the two situations are essentially the same.. at least mathematically. 29.0.0.1 Example: library(data.table) library(ggplot2) ## install.packages(&#39;scatterplot3d&#39;) ## If you need to library(scatterplot3d) rm(list=ls()) d &lt;- fread(&#39;https://crossley.github.io/book_stats/data/eeg/epochs.txt&#39;) d[, V1 := NULL] ## convert from wide to long format dd &lt;- melt(d, id.vars=c(&#39;time&#39;, &#39;condition&#39;, &#39;epoch&#39;)) ## pick out some columns randomly y &lt;- dd[variable==&#39;MEG 001&#39;, mean(value), .(epoch)][, V1] x1 &lt;- dd[variable==&#39;MEG 010&#39;, mean(value), .(epoch)][, V1] x2 &lt;- dd[variable==&#39;MEG 015&#39;, mean(value), .(epoch)][, V1] ddd &lt;- data.table(y, x1, x2) ## visualise possible linear relationship plot3d &lt;- scatterplot3d(x1, x2, y, angle=55, scale.y=0.7, pch=16, color=&quot;red&quot;, main=&quot;Regression Plane&quot;) ## fit a simple linear regression model fm &lt;- lm(y ~ x1 + x2, data=ddd) plot3d$plane3d(fm, lty.box = &quot;solid&quot;) summary(fm) ## ## Call: ## lm(formula = y ~ x1 + x2, data = ddd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -371.16 -105.98 8.39 105.13 413.27 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -26.7827 15.1606 -1.767 0.0805 . ## x1 4.0858 0.3919 10.426 &lt; 2e-16 *** ## x2 -1.9990 0.3042 -6.572 2.58e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 150.7 on 96 degrees of freedom ## Multiple R-squared: 0.5314, Adjusted R-squared: 0.5217 ## F-statistic: 54.44 on 2 and 96 DF, p-value: &lt; 2.2e-16 cor(fm$fitted.values, y)^2 ## [1] 0.5314302 "],["general-linear-model-glm.html", "30 General linear model (GLM) 30.1 General linear model: ANOVA and regression have a common base", " 30 General linear model (GLM) 30.1 General linear model: ANOVA and regression have a common base The multiple regression model we just considered is an example of the general linear model. \\[\\begin{align} y_{i} &amp;= \\beta_{0} + \\beta_{1} x_{1_i} + \\beta_{2} x_{2_i} + \\dots + \\beta_{k} x_{k_i} + \\epsilon_{i} \\\\\\\\ \\boldsymbol{y} &amp;= \\beta_{0} \\begin{bmatrix} 1\\\\ 1\\\\ \\vdots\\\\ 1 \\end{bmatrix} + \\beta_{1} \\begin{bmatrix} x_{1_1}\\\\ x_{1_2}\\\\ \\vdots\\\\ x_{1_n}\\\\ \\end{bmatrix} + \\beta_{2} \\begin{bmatrix} x_{2_1}\\\\ x_{2_2}\\\\ \\vdots\\\\ x_{2_n}\\\\ \\end{bmatrix} + \\ldots + \\beta_{k} \\begin{bmatrix} x_{k_1}\\\\ x_{k_2}\\\\ \\vdots\\\\ x_{k_n}\\\\ \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_n\\\\ \\end{bmatrix} \\\\\\\\ \\boldsymbol{y} &amp;= \\begin{bmatrix} 1 &amp; x_{1_1} &amp; x_{2_1} &amp; \\ldots &amp; x_{k_1} \\\\ 1 &amp; x_{1_2} &amp; x_{2_2} &amp; \\ldots &amp; x_{k_2} \\\\ \\vdots &amp; \\vdots &amp; \\dots &amp; \\dots \\\\ 1 &amp; x_{1_n} &amp; x_{2_n} &amp; \\ldots &amp; x_{k_n} \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{k} \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_n\\\\ \\end{bmatrix} \\\\\\\\ \\boldsymbol{y} &amp;= \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\end{align}\\] In regression, the \\(x_{i,j}\\) values that make up the design matrix \\(\\boldsymbol{X}\\) – i.e., the regressors – are continuously valued. ANOVA can be framed by exactly the same linear model, given that the ANOVA factors are input as “dummy coded” regressors. What is dummy coding? Consider a factor with three levels. We can dummy code this as follows: Factor level Dummy variable 1 Dummy variable 2 1 0 0 2 1 0 3 0 1 This coding leads to the following linear model: \\[\\begin{align} \\boldsymbol{y} &amp;= \\beta_{0} \\begin{bmatrix} 1\\\\ 1\\\\ \\vdots\\\\ 1\\\\ 1\\\\ \\vdots\\\\ 1\\\\ 1\\\\ \\end{bmatrix} + \\beta_{1} \\begin{bmatrix} 0\\\\ 0\\\\ \\vdots\\\\ 1\\\\ 1\\\\ \\vdots\\\\ 0\\\\ 0 \\end{bmatrix} + \\beta_{2} \\begin{bmatrix} 0\\\\ 0\\\\ \\vdots\\\\ 0\\\\ 0\\\\ \\vdots\\\\ 1\\\\ 1 \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_{m-1}\\\\ \\epsilon_m\\\\ \\vdots\\\\ \\epsilon_{n-1}\\\\ \\epsilon_n \\end{bmatrix} \\\\\\\\ \\boldsymbol{y} &amp;= \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots\\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots\\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\beta_{2} \\end{bmatrix} + \\begin{bmatrix} \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_{m-1}\\\\ \\epsilon_m\\\\ \\vdots\\\\ \\epsilon_{n-1}\\\\ \\epsilon_n\\\\ \\end{bmatrix} \\\\\\\\ \\boldsymbol{y} &amp;= \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\end{align}\\] With this coding, it’s fairly straight forward to show the following: \\[ \\beta_0 = \\bar{x_1} \\\\ \\beta_1 = \\bar{x_2} - \\bar{x_1} \\\\ \\beta_2 = \\bar{x_3} - \\bar{x_1} \\\\ \\] That is, the \\(\\beta\\) estimates corresponds to differences in means, which is exactly what ANOVA is after. 30.1.0.1 Example: Criterion learning data d &lt;- fread(&#39;https://crossley.github.io/book_stats/data/criterion_learning/crit_learn.csv&#39;) ## We will answer this question: Are there significant differences between the ## mean `t2c` per subject betweem conditions? For simplicity, only consider the ## `Delay`, `Long ITI` and `Short ITI` conditions. ## clean up the data (just trsut me this makes sense to do here) dd &lt;- unique(d[prob_num &lt;= 3 &amp; nps &gt;= 3 &amp; cnd %in% c(&#39;Delay&#39;, &#39;Long ITI&#39;, &#39;Short ITI&#39;), .(cnd, sub, prob_num, t2c)]) ddd &lt;- dd[, .(t2c=mean(t2c)), .(cnd, sub)] ddd[, sub := factor(sub)] ddd[, cnd := factor(cnd)] ggplot(ddd, aes(x=cnd, y=t2c)) + geom_boxplot() + theme(legend.position=&#39;none&#39;) fm &lt;- lm(t2c ~ cnd, data=ddd) summary(fm) ## ## Call: ## lm(formula = t2c ~ cnd, data = ddd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.593 -21.259 -12.481 7.519 106.949 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.708 5.109 9.142 6.52e-12 *** ## cnd1 5.884 7.017 0.839 0.406 ## cnd2 -2.894 7.017 -0.412 0.682 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 35.35 on 46 degrees of freedom ## Multiple R-squared: 0.01518, Adjusted R-squared: -0.02764 ## F-statistic: 0.3544 on 2 and 46 DF, p-value: 0.7035 anova(fm) ## Analysis of Variance Table ## ## Response: t2c ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cnd 2 886 442.82 0.3544 0.7035 ## Residuals 46 57475 1249.45 ddd[cnd==&#39;Delay&#39;, mean(t2c)] ## [1] 52.59259 ddd[cnd==&#39;Long ITI&#39;, mean(t2c)] - ddd[cnd==&#39;Delay&#39;, mean(t2c)] ## [1] -8.777778 ddd[cnd==&#39;Short ITI&#39;, mean(t2c)] - ddd[cnd==&#39;Delay&#39;, mean(t2c)] ## [1] -8.874644 Cool that we can see that R does the dummy coding for us, and automatically sets the baseline value to the Delay condition. What if you want to assign a different condition to baseline? Not for this lecture but you should keep taking classes and learning! In summary, an regression with dummy coded factors as regressors is equivalent to an ANOVA, and both are just instances of the general linear model. library(data.table) library(ggplot2) library(ez) library(EnvStats) "],["summary-of-data-sets.html", "31 SUmmary of Data Sets 31.1 Rat maze data 31.2 Criterion learning data 31.3 NHP data 31.4 Minimally invasive surgery (MIS) data 31.5 MEG data", " 31 SUmmary of Data Sets 31.1 Rat maze data Consider an experiment in which a set of rats run through a set of mazes some number of times each. The researchers running this experiment are interested in how quickly rats can run these mazes, and so they record the time in seconds each rat takes to complete each run of each maze. rm(list=ls()) d &lt;- fread(&#39;https://crossley.github.io/book_stats/data/maze/maze.csv&#39;) The resulting data.table contains the following columns: rat: Rat identifier (i.e., different numbers correspond to different rats). maze: Maze identifier (i.e., different numbers correspond to different mazes). run: Run identifier (i.e., different numbers correspond to different runs). time: The time in seconds taken to complete the maze. 31.2 Criterion learning data Does feedback-delay impair criterion learning? What is criterion learning? Here is an example trial. FigName In this case, thin bars belong to category A, and thick bars belong to category B. Bar thickness is continuous… when exactly does thick become thin? Where is the category boundary (i.e., criterion) that separates thick from thin? Criterion learning is the process that allows our brains to figure this out. rm(list=ls()) d &lt;- fread(&#39;https://crossley.github.io/book_stats/data/criterion_learning/crit_learn.csv&#39;) The resulting data.table contains the following columns: t: Trial number across the entire experiment. t_prob: Trial number per problem. sub: Subject identifier (i.e., different numbers correspond to different subjects). cnd: Condition identifier. prob_num: Problem number (gets increased by 1 every time a participant solves a problem). t2c: Trials to criterion (i.e., the number of trials it took a participant to solve a particular problem.) exp: Experiment indicator. Overall, this study was broken down into two experiments – one using sine-wave grating stimuli and the other using a different type of stimuli. nps: Number of problems solved. This is the same as max(prob_num) 31.3 NHP data The data file can be found here: https://crossley.github.io/book_stats/data//nhp_cat_learn/ii_gabor.csv This data file contains the results from a monkey performing a category learning experiment similar to those that we have seen a handful of times already in this class. On each trial of the experiment the monkey sees a sine-wave grating and must learn through trial and error whether that grating is a member of category A or category B. rm(list=ls()) d &lt;- fread(&#39;https://crossley.github.io/book_stats/data//nhp_cat_learn/ii_gabor.csv&#39;) col_names &lt;- c( &#39;cat&#39;, &#39;x&#39;, &#39;y&#39;, &#39;resp&#39;, &#39;rt&#39;, &#39;phase&#39; ) setnames(d, col_names) d[, trial := 1:.N] trial: Trial number. cat: Category label. x: The spatial frequency of the sine-wave grating. y: The orientation of the sine-wave grating. resp: The response made by the monkey (i.e., category A or B). rt: Reaction time (time from stimulus onset to button press). phase: Indicates the phase of the experiment. In phase==2, the category labels are swapped relative to phase==1. 31.4 Minimally invasive surgery (MIS) data This data is from a motor learning and motor control experiment. On each trial, participants simply rested their hand in the middle of a desk, and then tried to move their hand quickly and accurately to a visual target somewhere on a circle centred at their hand position and with a radius of about 8 cm. We tested two different groups of people. One group were college students, and the other group were professional surgeons. The data file can be found here: https://crossley.github.io/book_stats/data/mis/mis_data.csv rm(list=ls()) d &lt;- fread(&#39;https://crossley.github.io/book_stats/data/mis/mis_data.csv&#39;) subject: Anonymous subject ID group: Indicates college student or surgeon phase: Indicates the phase of the experiment trial: Trial number target: The visual target was reached to for this trial error: The mismatch between the centre of the target and the final hand position movement_time: How long the movement lasted reaction_time: How long until the movement began peak_velocity: The maximum velocity reached during the reach 31.5 MEG data We also have magnetoencephalography (MEG) data collected from a single participant while they performed a category learning experiment. On each trial of the category learning experiment, the participant viewed a circular sine wave grating, and had to push a button to indicate whether they believed the stimulus belonged to category A or category B. We have seen and worked with this type of category learning experiment many times throughout this course, and it is further described by the following figure. FigName MEG is used to record the time-series of magnetic and electric potentials at the scalp, which are generated by the activity of neurons. There are many sensors, each configured to pick up signal from a different position on the scalp. This is shown in the following figure (the text labels indicate the channel name and are placed approximately where the MEG sensor is located on a real head). FigName The data file that we will be working with is arranged into epochs aligned to stimulus presentation. This means that every time a stimulus is presented we say that an epoch has occurred. We then assign a time of \\(t=0\\) to the exact moment the stimulus appeared. We then typically look at the neural time series from just before the stimulus appeared to a little while after the stimulus has appeared. For this data, each epoch starts 0.1 seconds before stimulus onset, and concludes 0.3 seconds after stimulus onset. The following figure shows the MEG signal at every sensory location across the entire scalp for 5 time points within this \\([-0.1s, 0.3s]\\) interval. FigName The data can be located here: https://crossley.github.io/book_stats/data/eeg/epochs.txt rm(list=ls()) ## d &lt;- fread(&#39;https://crossley.github.io/book_stats/data/eeg/epochs.txt&#39;) The time column contains times in seconds relative to stimulus onset. Stimulus onset always occurs at \\(0\\) seconds. The condition column indicates which category the stimulus belonged to for the given epoch. We won’t make use of this column here, and we will remove it below. The epoch column is the epoch number. You can think of this like we have usually thought of trial columns in examples throughout the course. The many different MEG xyz columns contain the actual neural time series signals for each sensor. See the above figure for how these column names map onto scalp positions. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
