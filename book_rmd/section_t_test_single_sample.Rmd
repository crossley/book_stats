```{r echo=F, message=F, warning=F}
rm(list=ls())
```

# t-test

So far, everything that we have done we have been lucky
enough to know both the mean and the variance of the
sampling distribution in our hypothesis tests. The mean is
specified in $H_0$ and $H_1$ and the variance has either
fallen out luckily (e.g., as with the Binomial test), or I
have just given you a number and told you pretend that we
just know it to be true (e.g., previous cheese maze
example). Of course, in most real world scenarios, we will
not know the variance of the sampling distribution, and this
means that the approaches we have developed so far aren't
quite appropriate. Here is what we do instead:

Let $X_1, X_2, \ldots, X_n$ be independent and identically
distributed as

$$
X_i \sim N(\mu_X, \sigma_X)
$$

and define two random variables $\bar{X}$ and $S^2$ as

$$
\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i
$$

$$
S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2
$$

then the random variable

$$
\frac{\bar{X} - \mu_X}{\frac{\sigma_X}{\sqrt{n}}} \sim N(0, 1) = Z
$$

and

$$
\frac{\bar{X} - \mu_X}{\frac{S}{\sqrt{n}}} \sim t(n-1)
$$

where $t$ is a t-distribution, which is completely defined
by one parameter called the *degrees of freedom* given by
$n-1$. This all means that the mathematical formulation for
how our sampling distribution is defined is different
depending on whether or not we know $\sigma_X$.
Lets examine how this pans out using our previous cheese
example, but without assuming known variance.

<h4>1\. Specify the null and alternative hypotheses ($H_0$ and
   $H_1$) in terms of a distribution and population
   parameter.</h4>

$$
H_0: \mu = 90 \\
H_1: \mu < 90
$$

<h4>2\. Specify the type I error rate -- denoted by the symbol
   $\alpha$ -- you are willing to tolerate.</h4>

$$
\alpha = 0.05
$$

<h4>3\. Specify the sample statistic that you will use to
   estimate the population parameter in step 1 and state how
   it is distributed under the assumption that $H_0$ is
   true.</h4>

In this example we do not know $\sigma_{x}$, and so we
must estimate it. This means that we do not want to reason using
the observed $\bar{x}$ value and corresponding sampling distribution,
but instead want to reason using an observed $t$ value and corresponding
t-distribution.

$$
t_{obs} = \frac{\bar{x} - \mu_x}{\frac{s_x}{\sqrt{n}}} \sim t(n-1)
$$

```{r, echo=F, fig.width=10}
mu_x <- 90
mu_x_bar <- mu_x
sig_x <- 10

n <- 100
x_obs <- rnorm(n, mu_x, sig_x)

t_obs <- (mean(x_obs)-90)/(sd(x_obs)/sqrt(n))
t_crit <- qt(0.05, n-1, lower.tail=TRUE)
mu <- 0
t <- seq(-4, 4, 0.01)
ft <- dt(t, n-1)

x_bar_obs <- mean(x_obs)
muxbar <- 90
sig_x_bar <- sig_x / sqrt(n)
z_obs <- (x_bar_obs - 90) / sig_x_bar
z <- seq(-4, 4, 0.01)
fz <- dnorm(z, 0, 1)

d <- data.table(t,
                ft,
                z, 
                fz,
                region=factor(t<=t_obs, labels=c('t > t_obs', 't <= t_obs')),
                cregion=factor(t<=t_crit, labels=c('t > t_crit', 't <= t_crit')))

g1 <- ggplot(d, aes(t, ft, colour=region)) +
  geom_line() +
  geom_vline(xintercept=t_obs, colour='black', linetype=2) +
  geom_ribbon(aes(ymin=0, ymax=dnorm(t, 0, 1), fill=region), 
              colour='black', alpha=0.5) +
  annotate('text', x=t_obs-0.1, y=dnorm(t_obs, 0, 1),
           label='t_obs or \n more extreme',
           colour='#00AFBB',
           hjust=1) +
  ylab('f(t)') +
  xlab('t') +
  ggtitle('p-value approach') +
  theme(legend.position = "none")

g2 <- ggplot(d, aes(t, ft, colour=cregion)) +
  geom_line() +
  geom_vline(xintercept=t_obs, colour='black', linetype=2) +
  geom_vline(xintercept=t_crit, colour='black', linetype=2) +
  geom_ribbon(aes(ymin=0, ymax=dnorm(t, 0, 1), fill=cregion), 
              colour='black', alpha=0.5) +
  annotate('text', x=t_crit-0.1, y=dnorm(t_crit, 0, 1),
           label='critical value\n(rejection region)',
           colour='#00AFBB',
           hjust=1) +
  annotate('text', x=t_obs+0.1, y=dnorm(t_obs, 0, 1),
           label='t observed',
           colour='black',
           hjust=0) +
  ylab('f(t)') +
  xlab('t') +
  ggtitle('Critical value approach') +
  theme(legend.position = "none")

ggarrange(g1, g2, ncol=2)
```

<h4>4\. Obtain a random sample and use it to compute the sample
   statistic from step 3. Call this value
   $\widehat{\theta}_{\text{obs}}$</h4>

For our data, the following is true:

* $n=15$,
* $\bar{x} =$ `r mean(x_obs)`,
* $s_x =$ `r sd(x_obs)`
* $t_{obs} =$ `r (mean(x_obs)-90)/(sd(x_obs)/sqrt(n))`.

The above plot shows the $t(n-1)$ sampling distribution in
colour, and the $Z\sim\mathcal{N}(0,1)$ in black. The $t$
has higher tails than the $Z$. This is because the t-value
is the result of two random variables (sample mean and
sample variance), while the z-value is only a product of
only one random variable (the sample mean). However, it is
easy to see that the difference between $t$ and $Z$ is
reduces as $n$ increases.

```{r, echo=F}
z <- seq(-5, 5, 0.001)
fz <- dnorm(z, 0, 1)
ft1 <- dt(z, 1)
ft3 <- dt(z, 3)
ft10 <- dt(z, 10)

d <- data.table(z, fz, ft1, ft3, ft10)

dd <- melt(d, measure.vars=c('fz', 'ft1', 'ft3', 'ft10'))

ggplot(dd, aes(z, value, colour=variable)) +
  geom_line() +
  xlab('x') +
  ylab('f(x)')
```

<h4>5\. If $\widehat{\theta}_{\text{obs}}$ is very unlikely to
   occur under the assumption that $H_0$ is true, then
   reject $H_0$. Otherwise, do not reject $H_0$.</h4>

When computing the p-value, we will turn to `pt()`. From the
plot above, and from reasoning about the alternative hypothesis,
we see that we need `lower.tail=TRUE`.

```{r}
## p-value
p_val <- pt(t_obs, n-1, lower.tail=TRUE)
p_val

## critical value
t_crit <- qt(0.05, n-1, lower.tail=TRUE)
t_crit
```

Finally, there is a built in function called `t.test()` that
will do all of this for you.
```{r}
t.test(x_obs, mu=90, alternative='less')
```

## Two-tailed t-test

* t-test arises from the Normal test scenarios in which the
sample variance $\sigma_X$ is unknown.

<b>1\. Specify the null and alternative hypotheses ($H_0$ and
   $H_1$) in terms of a distribution and population
   parameter.</b>
   
$$
H_0: \mu = 90 \\
H_1: \mu \neq 90
$$

<b>2\. Specify the type I error rate -- denoted by the symbol
   $\alpha$ -- you are willing to tolerate.</b>
   
$$
\alpha = 0.05
$$
   
<b>3\. Specify the sample statistic that you will use to
   estimate the population parameter in step 1 and state how
   it is distributed under the assumption that $H_0$ is
   true.</b>
   
$$
\widehat{\mu} = \bar{x} \\
\bar{x} \sim \mathcal{N}(\mu_{\bar{x}}, \sigma_{\bar{x}}) \\
\mu_{\bar{x}} = \mu_{x} \\
\sigma_{\bar{x}} = \frac{\sigma_{x}}{\sqrt{n}} \rightarrow
\widehat{\sigma}_{\bar{x}} = \frac{s_{x}}{\sqrt{n}} \\
t = \frac{\bar{x} - \mu_x}{\frac{s_x}{\sqrt{n}}} \sim t(n-1)
$$

<b>4\. Obtain a random sample and use it to compute the
   sample statistic from step 3. Call this value
   $\widehat{\theta}_{\text{obs}}$</b>
   
The researchers perform 15 trials and measure the time to
cheese on each trial. The data are as follows:

```{r}
xobs <- c(105.25909, 73.47533, 106.59599, 105.44859, 88.29283,
          49.20100, 61.42866, 74.10559, 79.88466, 128.09307,
          95.27187 ,64.01982 ,57.04686 ,74.21077, 74.01570)

n <- length(xobs)
xbarobs <- mean(xobs)
sigxbarobs <- sd(xobs) / sqrt(n)

mux <-90
muxbar <- 90

tobs <- (xbarobs - muxbar) / sigxbarobs
```
   
<b>5\. If $\widehat{\theta}_{\text{obs}}$ is very unlikely to
   occur under the assumption that $H_0$ is true, then
   reject $H_0$. Otherwise, do not reject $H_0$.</b>
   
```{r}
tobs_upper <- -tobs
tobs_lower <- tobs

t_crit_upper <- qt(0.05/2, n-1, lower.tail=FALSE)
t_crit_lower <- qt(0.05/2, n-1, lower.tail=TRUE)
```

```{r, echo=F, fig.width=10}
x <- seq(-4, 4, 0.01)
fx <- dt(x, n-1)
d <- data.table(x,
                fx,
                region=factor(
                  (x>t_crit_lower) & (x < t_crit_upper), 
                  labels=c('rejection region', 'failed rejection region'))
                )

ggplot(d, aes(x, fx)) +
  geom_line() +
  geom_segment(x=tobs_lower, xend=tobs_lower, y=0, yend=d[, max(fx)], colour='black', linetype=2) +
  geom_segment(x=tobs_upper, xend=tobs_upper, y=0, yend=d[, max(fx)], colour='black', linetype=2) +
  geom_segment(x=t_crit_lower, xend=t_crit_lower, y=0, yend=d[, max(fx)], colour='red', linetype=2) +
  geom_segment(x=t_crit_upper, xend=t_crit_upper, y=0, yend=d[, max(fx)], colour='red', linetype=2) +
  geom_ribbon(data=d[x<t_crit_lower], 
              aes(ymin=0, ymax=dt(x, n-1), fill=region), alpha=0.5) +
  geom_ribbon(data=d[x>t_crit_lower & x<t_crit_upper], 
              aes(ymin=0, ymax=dt(x, n-1), fill=region), alpha=0.5) +
  geom_ribbon(data=d[x>t_crit_upper], 
              aes(ymin=0, ymax=dt(x, n-1), fill=region), alpha=0.5) +
  annotate('text', 
           x=tobs_lower, 
           y=dt(tobs_lower, n-1),
           label='tobs lower',
           colour='black',
           hjust=0) +
  annotate('text', 
           x=tobs_upper, 
           y=dt(tobs_upper, n-1),
           label='tobs upper',
           colour='black',
           hjust=1) +
  annotate('text', x=t_crit_lower, y=-0.005,
           label='t_crit_lower',
           colour='black',
           hjust=1) +
  annotate('text', x=t_crit_upper, y=-0.005,
           label='t_crit_upper',
           colour='black',
           hjust=0) +
  ylab('f(t)') +
  xlab('t')
```

```{r}
# compute p-value by hand
pval_upper <- pt(tobs_upper, n-1, lower.tail=FALSE)
pval_lower <- pt(tobs_lower, n-1, lower.tail=TRUE)
pval <- pval_upper + pval_lower
pval

# check with t.test
t.test(xobs, mu=mux, alternative='two.sided')
```
